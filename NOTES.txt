https://medium.com/helidon/helidon-logging-and-mdc-5de272cf085d

explains, imperfectly, how to use JUL logging with Helidon.
Turns out the configuration file that Helidon expects is logging.properties at the root of the context.
The format is a little weird.

NAMING SCHEMES:

We're going to use the terminology from our SMS/SMPP days.

for queues

    <platform>.<region>.<direction>

    e.g.

    whatsapp.us.mo  - the platform is whatsapp, the region is the United States, the direction is Mobile Originated.

    test.local.mo   - the platform is a developer/test, the region is the scope of the test resources (likely a dev machine), the direction is Mobile Originated.

Current design uses Topics with routingKeys.
I'm thinking that we'd likely have separate endpoints running for each platform gateway but using topics & routingKeys gives us flexibility.

Test Setup:

Run the container image with the RabbitMQ server:

    docker run -it --rm --name rabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:4.0-management

FakePlatformMO (client)
    Reads messages from file, for each
    HTTP POST to
     --> Rcvr (port: 4242)
            --> RabbitMQ client enqueue to "test.mo"

     --> "Operator"
         RabbitMQ client dequeues from "test.mo"
         Processes the message (side effects galore)
         Creates response
         Enqueues response to "test.mt"

     --> Sndr
        RabbitMQ client dequeues from "test.mt"
        HTTP POST to

     --> FakePlatformMT (port: 2424)
        Writes each received message to file.

 Validator compares input file with output file.

Test Program:
FakePlatformMO generates messages with known values.
FakePlatformMT receives messages and validates that it corresponds to a previously sent message.


Helidon has libraries that support tracing (https://helidon.io/docs/v4/se/tracing) for both the web server and web client. Note: OpenTracing (https://opentracing.io/) has been retired in favor of OpenTelemetry (https://opentelemetry.io/docs/languages/js/instrumentation/)

OpenAPI is also supported in case parts of the software needs to be used with AWS Lambda.

Scheduling support is available (https://helidon.io/docs/v4/se/scheduling) which should allow us to avoid shit like EventBridge. Under the covers it uses cron-utils (https://github.com/jmrozanec/cron-utils)


=========== Building an executable jar ===========
Added the following components to the <build/> section of the pom.xml file:
    exec-maven-plugin
    maven-compiler-plugin
    maven-jar-plugin
    maven-assembly-plugin
Now we can run:
    mvn clean compile assembly:single

to yield an artifact that can be run with 'java -jar'
The artifact ends up being around 23MB as of 12/12/2024.

Java Microbenchmark Harness:
    https://www.baeldung.com/java-microbenchmark-harness


Trying to compile the project with native-image yielded errors related to logback.
The GraalVM output was pretty helpful. It suggested adding to the file that tracks usages of reflection.
While researching the problem someone suggested switching to the new configuration system available for logback: logback-tyler
This provides a class generator that provides a fairly easy to edit Configurator implementation that doesn't use XML or reflection.
In addition to making Graal happy the initialization will be faster and more efficient for the lack of XML.

Of course, it couldn't be that simple ;-) There seems to be a bug that causes the generated class to not be marked public. The service-provider mechanism added JDK9 in support of the new package system complained of not having access. Simply editing the class to be public appears to solve the problem. I posted a question to the GitHub discussion board:

    https://github.com/qos-ch/logback-tyler/discussions/5

Needed to add a reachability-metadata.json file to META-INF/native-image to have the .properties files included in the binary.

Some notable/interesting bits from the compiler output:

Top 10 origins of code area:                                Top 10 object types in image heap:
  11.60MB java.base                                            8.48MB byte[] for embedded resources
   2.59MB java.xml                                             5.24MB byte[] for code metadata
   1.22MB svm.jar (Native Image)                               3.31MB byte[] for java.lang.String
 747.48kB logback-core-1.5.12.jar                              2.31MB java.lang.String
 468.30kB amqp-client-5.23.0.jar                               2.20MB java.lang.Class
 326.55kB java.rmi                                           843.80kB byte[] for general heap data
 244.26kB logback-classic-1.5.12.jar                         785.30kB com.oracle.svm.core.hub.DynamicHubCompanion
 207.22kB helidon-http-http2-4.1.4.jar                       554.23kB heap alignment
 203.91kB java.naming                                        513.00kB int[][]
 203.76kB helidon-webclient-api-4.1.4.jar                    491.98kB byte[] for reflection metadata
   1.46MB for 37 more packages                                 5.34MB for 2375 more object types

Recommendations:
 HEAP: Set max heap for improved and more predictable memory usage.
 CPU:  Enable more CPU features with '-march=native' for improved performance.

 Questions: Why are we getting java.xml and java.rmi included?
 The binary size is significant: 50MB.

 We need a way to produce containers that will run locally on my M1
 Can we run a Linux VM locally to run the build? Or even a container?

 NOTE: For a moment I thought the --target option of native-image would let us produce an executable for different platforms.
 This is Java (Write-Once Run Everywhere™) after. But it turns out it's bullshit. After six years of development this is still on the to do list.
 Given the prevalence of containers which are _only_ supported for Linux its amazing that this hasn't been prioritized.
 As it stands, we'll need to create a bunch of extra container rigging to produce something that we then copy over into another image.

    https://github.com/spotify/dockerfile-maven is a project for building container images with Docker. It's not still in development, however.

=========== STUFF WE WILL WANT ===========

- Software BOM with security information for our dependencies.
- Qodana or something free to perform structured code analysis (we need to take some time to configure this to remove bogus or just unhelpful problems.)
- Container images for arm64 and amd64.

=========== STUFF WE SHOULD LEARN MORE ABOUT ===========
The modules system introduced in JDK 9.
The inner workings of Docker so I can evaluate images, how to extend, etc.

Docker progress:

Finally figured out how to specify/pass arguments when running Burble inside a docker container.
Changed the

docker run -it --rm --name burble -p 2424:2424 -p 4242:4242 burble:0.1 FakeOperator


native-image -jar target/sndrRcvr-1.0-SNAPSHOT-jar-with-dependencies.jar -o burble
    produces a file, burble, that is still fucking massive at 44MB. :-(

The container that builds on eclipse-temurin:23 and includes the binary weighs in at 719MB.

Switching to eclipse-temurin:23-alpine reduces the image to a "mere" 581MB.
Most of the container comes from the installation of
    apk add --no-cache fontconfig ttf-dejavu gnupg ca-certificates p11-kit-trust musl-locales musl-locales-lang binutils tzdata coreutils openssl (63MB)
    OpenJDK23U-jdk_aarch64_alpine-linux_hotspot_23.0.1_11.tar.gz (310MB uncompressed)
The Burble executable is the third-largest item.

We need to add config to the native-image build to specify the musl C lib. This is only applicable to the Linux version, however.
Musl is not a thing for macOS.

Most people apparently use a container to build their linux image. There's an "official" Maven image repo

docker run -it --rm --name mvnc -v "$HOME/.m2":/root/.m2 -v "$(pwd)":/Users/mark/Development/sndrRcvr/src -w /Users/mark/Development/sndrRcvr/src maven:latest mvn package

We want a container
- with graalvm 23
- 3.x Maven
Initially this doesn't need to be alpine based but the musl thing might require it

This repo looks promising: https://github.com/vegardit/docker-graalvm-maven/blob/main/README.md

https://github.com/vegardit/docker-graalvm-maven/blob/main/README.md

=== Example use of the vegardit image to build a native binary for linux ===
docker run --rm -it \
  -v $PWD:/mnt/myproject:rw \       # what's the effect of the :rw part of this?
  -w /mnt/myproject \
  vegardit/graalvm-maven:latest-java23 \
  mvn clean package

Here's how we're using it to

> docker run --rm -it -v "$HOME/.m2/repository":/root/.m2/repository -v "$(pwd)":/mnt/sndrRcvr -w /mnt/sndrRcvr vegardit/graalvm-maven:latest-java23 mvn clean package

Works! Produces a jar file in the target folder of our host OS.

> docker run --rm -it -v "$HOME/.m2/repository":/root/.m2/repository:rw -v "$(pwd)":/mnt/sndrRcvr -w /mnt/sndrRcvr vegardit/graalvm-maven:latest-java23 mvn -Pnative clean package

Works! Produces an ELF executable using the aarch64 instruction set.
NOTE: the unix `file` command points out it's dynamically linked, however:
    target/burble: ELF 64-bit LSB pie executable, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-aarch64.so.1, BuildID[sha1]=99c5d1bf7a89955c2461187540350e00678a5fa1, for GNU/Linux 3.7.0, not stripped

Once the artifact is produced, we can build the docker image that will execute it...

Made a copy of Dockerfile then renamed both.
    Dockerfile.bin - builds an image that uses a native-image generated executable.
    Dockerfile.jvm - builds an image that runs a normal jar file using eclipse-temurin:23-alpine.

Use them like so to build the image:
    docker build -t burble-bin:0.1.0 -f Dockerfile.bin .
    docker build -t burble-jvm:0.1.0 -f Dockerfile.jvm .

Then to run them:
    The binary executable version:
    docker run -it --rm -p 4242:4242 --name burble-bin-rcvr burble-bin:0.1.0 Rcvr
    docker run -it --rm --name burble-bin-fkop burble-bin:0.1.0 FakeOperator
    docker run -it --rm --name burble-bin-sndr burble-bin:0.1.0 Sndr

    The JVM version:
    docker run -it --rm -p 4242:4242 --name burble-jvm-rcvr burble-jvm:0.1.0 Rcvr
    docker run -it --rm --name burble-jvm-fkop burble-jvm:0.1.0 FakeOperator
    docker run -it --rm --name burble-jvm-sndr burble-jvm:0.1.0 Sndr

Where the general form is:
    docker run -it --rm [-p 2424:2424] | [-p 4242:4242] --name burble-jvm burble-jvm:0.1.0 <programName: Rcvr | FakeOperator | Sndr>

The Dockerfiles both default to running FakeOperator.

To run the image without executing the entrypoint/cmd:
    docker run -it --entrypoint /bin/sh burble-bin:0.1.0

Actually it appears that the binary version running on the alpine container fails because, as noted above, the executable is dynamically linked.
It fails with a very terse and somewhat misleading error:
    "exec /opt/app/burble: no such file or directory"
I think this means that it can't find the expected libc rather than the program itself.

I added a block to the configuration section of the native-maven-plugin setup:
<buildArgs>
    <buildArg>--static --libc=musl --enable-sbom</buildArg>
</buildArgs>

Which seems to be signalling the correct switches in the native-image program but the container that produces the linux binary lacks the referenced library.

docker run --rm -it -v "$HOME/.m2/repository":/root/.m2/repository:rw -v "$(pwd)":/mnt/sndrRcvr -w /mnt/sndrRcvr mstewart/graalvm-maven-musl mvn -Pnative clean package

Notable output from native-image:

"HEAP: Set max heap for improved and more predictable memory usage."

Would be nice if they mentioned the switch/param/flag that controls this. Is the normal JVM flag? Where does it get set?

 "CPU:  Enable more CPU features with '-march=native' for improved performance."

 Added <buildArg>-march=native</buildArg> to the native-maven-plugin config which eliminates the message, so I assume it was the right way to do it.

 1 experimental option(s) unlocked:
 - '-H:IncludeResources': Use a resource-config.json in your META-INF/native-image/<groupID>/<artifactID> directory instead. (origin(s): 'META-INF/native-image/com.rabbitmq/amqp-client/native-image.properties' in 'file:///root/.m2/repository/com/rabbitmq/amqp-client/5.23.0/amqp-client-5.23.0.jar')

This is annoying. The lib is providing the needed data so it can be used to produce the native executable, but they've (I guess) changed their mind how this is to be implemented? Worse, they leave it to me to figure out how this is to be represented in the new file.
I think, for now, we will leave this unaddressed. At least until we're certain of the lib versions we want to use.

::IMPORTANT NOTE::
As of 12/26/2024, building a fully statically linked graalvm native image is *only* supported on linux for x86_64:
    https://github.com/oracle/graal/issues/9490
This is sad. The project seems old enough that it shouldn't still be a problem. There's no cross-compilation either though that is a little more understandable.
We should try building the musl/statically linked binary on one of our x86 machines just to be sure it will work.

NEXT STEPS:
X create a working docker-compose.yml to coordinate testing (leave rabbitmq out initially)
_ create some end-to-end tests
_ extend the rabbitmq image to add the admin user and any other customized bits we need then add it to the docker-compose.yaml.
_ create a management CLI using JLine/Jansi, etc.


I swear, where Docker and GraalVM is concerned, every step is a fucking struggle.

Running just a single container via docker-compose.yaml I seem unable to get an HTTP call to the Rcvr web server.

mark@YA-T7RX9LX2PL sndrRcvr % docker compose up -d

mark@YA-T7RX9LX2PL sndrRcvr % curl --verbose http://localhost:4242/health
* Host localhost:4242 was resolved.
* IPv6: ::1
* IPv4: 127.0.0.1
*   Trying [::1]:4242...
* Connected to localhost (::1) port 4242
> GET /health HTTP/1.1
> Host: localhost:4242
> User-Agent: curl/8.7.1
> Accept: */*
>
* Request completely sent off
* Empty reply from server
* Closing connection
curl: (52) Empty reply from server

===== 2024-12-28 ======
Ran the jvm  version of the containers and no longer get the weird HTTP problem. Were the binaries not working?

I re-ran the build_linux.sh script to make sure the executable is what we want.

mark@YA-T7RX9LX2PL sndrRcvr % file target/burble
target/burble: ELF 64-bit LSB pie executable, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-aarch64.so.1, BuildID[sha1]=0c26671ef34dfacfcaaea9595bc5e32e0417980a, for GNU/Linux 3.7.0, not stripped

OK, check.

The build uses a Debian-based container while the execution container is "eclipse-temurin:23" (I think this uses ubuntu as its base.)
Let's try it with "eclipse-temurin:23-jre" as the base layer (this is definitely an Ubuntu image and might be a bit smaller than the JDK version.)

...and, confirmed, that the problem is only with the binary version. Possibly because of the Debian-Ubuntu schism?

No, even when I change the Dockerfile of the image that runs the program to use the same Debian-based image that built the program AND when running curl from within that container.

$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ cat /proc/version
Linux version 6.10.11-linuxkit (root@buildkitsandbox) (gcc (Alpine 13.2.1_git20240309) 13.2.1 20240309, GNU ld (GNU Binutils) 2.42) #1 SMP Thu Oct  3 10:17:28 UTC 2024

Why is the gcc showing Alpine?

Hmm, the RabbitMQ supplied image that we're using--running Noble Numbat, version 24.04.1 LTS--shows the same gcc info. Maybe this is normal?

$ ldd /opt/app/burble
        linux-vdso.so.1 (0x0000ffffa22a2000)
        libz.so.1 => /lib/aarch64-linux-gnu/libz.so.1 (0x0000ffffa2230000)
        libc.so.6 => /lib/aarch64-linux-gnu/libc.so.6 (0x0000ffff9ea50000)
        /lib/ld-linux-aarch64.so.1 (0x0000ffffa2265000)

===== 2024-12-30 ======

Curious if we get a smaller image if we build the jre instead of the jdk...
With JDK...
burble-jvm                               0.1.0            912143180f41   41 hours ago   801MB
With JRE...
burble-jvm                               0.1.0            7171dc515c27   3 minutes ago   507MB

So, yay. The first actual reduction we've seen. GraalVM binaries are still fucking huge.
Baseline manual test (sending messages through the pipeline) appears to work fine.

What could we gain by putting a JRE on top of something like debian:stable-slim (which is used as the base by vegardit/graalvm-maven, which we use for building linux binaries.)

debian                                   stable-slim      5f21ebd35844   7 days ago       136MB
eclipse-temurin                          23               c2a3aba09776   2 months ago     712MB

How does the Ubuntu base for eclipse-temurin compare to slim?

While investigating this I checked out https://hub.docker.com/_/eclipse-temurin.
Two things of note:
They show how to set up the JDK using a reference to one of their images:

    FROM <base image>
    ENV JAVA_HOME=/opt/java/openjdk
    COPY --from=eclipse-temurin:23 $JAVA_HOME $JAVA_HOME
    ENV PATH="${JAVA_HOME}/bin:${PATH}"

They recommend building a custom JRE using jlink. Is this worth the bother?
There's also an eclipse-temurin:23.0.1_11-jre-alpine image.
See https://github.com/docker-library/repo-info/blob/master/repos/eclipse-temurin/local/23-jre-alpine.md

eclipse-temurin                          23.0.1_11-jre-alpine   623a424ca41d   2 months ago     291MB
Inspecting the layers it appears that the JRE comprises ~161MB of this image.
NOTE: busybox, which combines ~400 commands into a single program, doesn't include curl but *does* include wget so we won't need the former to do basic, intra-container testing.

Adding our code only adds 8MB:
burble-jvm                               0.1.0                  52f06679217c   2 minutes ago   298MB

This is a decent tradeoff.

Another good tip here to avoid having to rebuild the image every time the Maven produced jar changes is to mount the host path onto the container. So given,

    FROM eclipse-temurin:21.0.2_13-jdk
    CMD ["java", "-jar", "/opt/app/japp.jar"]

We can run the container like this:
    docker build -t <tag> .
    docker run -it -v /path/on/host/system/jars:/opt/app <tag>

What syntax do we use for a docker-compose file to do this?

TODO
_ Since building a fully statically linked graalvm native image is *only* supported on linux for x86_64 we should bust out our large memory Macbook.
X Create a merged PlatformGatewayMT and MO generator for testing purposes.

Digging into the testcontainers project and found an image for RabbitMQ that's built on an Alpine base.
As expected it's smaller than the non -alpine version:
rabbitmq                                 4.0-management          14c30a03410f   3 months ago   425MB
rabbitmq                                 4.0-management-alpine   74bf73c53b96   3 months ago   295MB

Testcontainers supports running docker-compose files though the logging output is dramatically different from what we see running those files via docker. See https://codeal.medium.com/how-to-run-docker-compose-with-testcontainers-7d1ba73afeeb (it mentions the use of the volumes section of the compose file that, for things like postgres, can be used to run ddl scripts.


We want to be able to do this for integration testing.
That said, testing things we've noted to be possibly problematic--having different components come up out of order or restarting--would seem to require a per-container level of control.

Side-note: It appears possible to use Testcontainers with Docker alternatives, OrbStack and Colima. See http://rockyourcode.com/testcontainers-with-orbstack/ and http://rockyourcode.com/testcontainers-with-colima. Also https://github.com/testcontainers/testcontainers-java/issues/5034#issuecomment-1036433226

Probably not worth the extra effort unless we have problems with Docker.

===== 2025-01-02 ======

Working on EndToEndMessaging:

The docker setup code informs me:
    'container_name' property set for service 'brkr' but this property is not supported by Testcontainers, consider removing it
But clearly it's more than just a suggestion because it throws an ExceptionInInitializerError that kills the container startup.

This is tracked in https://github.com/testcontainers/testcontainers-java/issues/2472, but they don't seem to be interested in fixing it; a perfectly reasonable PR adding support for it--https://github.com/testcontainers/testcontainers-java/pull/2741--was closed.
So I've commented out the container_name in the docker-compose-jvm.yaml file.

Created an integration test, EndToEndMessaging that uses this file. The @Container annotation is supposed to start the docker process, but it doesn't seem to be doing so. Adding a static block that calls the .start() method after the annotation is a simple workaround even if it's a bit disappointing.

Reworked PlatformGatewayMT to act as both a MO source and the MT destination.

Next up: using specific containers to test ordering dependencies.

Scenarios:
- what should Rcvr do if the broker disappears?
    Currently, when messages are received we actually return the following to each calling request:
        connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)Complete.

    Obviously we shouldn't do this.
    Does it recover when the broker comes back?

    Restarted the broker container and got the following in the Rcvr log:

        2025-01-05 10:38:44,542 ERROR [] [AMQP Connection 192.168.1.155:5672] c.r.c.impl.ForgivingExceptionHandler - Caught an exception during connection recovery!
        java.io.IOException: null
        	at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:140)
        	at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:136)
        	at com.rabbitmq.client.impl.AMQConnection.start(AMQConnection.java:406)
        	at com.rabbitmq.client.impl.recovery.RecoveryAwareAMQConnectionFactory.newConnection(RecoveryAwareAMQConnectionFactory.java:71)
        	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.recoverConnection(AutorecoveringConnection.java:628)
        	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.beginAutomaticRecovery(AutorecoveringConnection.java:589)
        	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.lambda$addAutomaticRecoveryListener$3(AutorecoveringConnection.java:524)
        	at com.rabbitmq.client.impl.AMQConnection.notifyRecoveryCanBeginListeners(AMQConnection.java:839)
        	at com.rabbitmq.client.impl.AMQConnection.doFinalShutdown(AMQConnection.java:816)
        	at com.rabbitmq.client.impl.AMQConnection$MainLoop.run(AMQConnection.java:700)
        	at java.base/java.lang.Thread.run(Thread.java:1575)
        Caused by: com.rabbitmq.client.ShutdownSignalException: connection error
        	at com.rabbitmq.utility.ValueOrException.getValue(ValueOrException.java:66)
        	at com.rabbitmq.utility.BlockingValueOrException.uninterruptibleGetValue(BlockingValueOrException.java:36)
        	at com.rabbitmq.client.impl.AMQChannel$BlockingRpcContinuation.getReply(AMQChannel.java:552)
        	at com.rabbitmq.client.impl.AMQConnection.start(AMQConnection.java:336)
        	... 8 common frames omitted
        Caused by: java.io.EOFException: null
        	at java.base/java.io.DataInputStream.readUnsignedByte(DataInputStream.java:297)
        	at com.rabbitmq.client.impl.Frame.readFrom(Frame.java:91)
        	at com.rabbitmq.client.impl.SocketFrameHandler.readFrame(SocketFrameHandler.java:199)
        	at com.rabbitmq.client.impl.AMQConnection$MainLoop.run(AMQConnection.java:687)
        	... 1 common frames omitted

However, when we sent more traffic, the Rcvr handled it and was able to send it to the broker. Then I started the FakeOperator and the messages again passed through.

So is there a callback available to know when the broker is available?

===== 2025-01-07 ======
Having yet more fun with Testcontainers. >:-[

To explore the availability scenarios above, I'm programmatically creating containers for each application.

If I try starting a Rcvr container without a RabbitMQ broker running I get a complaint from InternalCommandPortListeningCheck that seems to stem from the call to withExposedPorts() that's used to connect with the Rcvr's web server.

It's clear, however, that to really have our components be testable in this way we need to provide a direct means of configuring ports.
Added an override mechanism to ConfigLoader that reads the properties file and the environment vars, overriding the former with the latter where they overlap.

Need a similar mechanism for PlatformGateway to provide it the effective http port for the Rcvr...
Hacked a fix for this but still running into the InternalCommandPortListeningCheck:
    https://github.com/testcontainers/testcontainers-java/issues/6730

Fucking hell, maybe it's just easier to add bash.
Did that but also needed to rebuild the jar file (which I had been neglecting.) Time to set up a shared mount for the containers so we don't need to rebuild the container everytime we make a change to the Java code (assuming we even remember to do that!)

===== 2025-01-09 ======
ServiceAvailabilityTest.testRcvrReconnect:
I have most of the containers running and passing messages along, up to where Sndr is supposed to call the PlatformGateway's web server to deliver the generated MTs.
Having trouble communicating with the PlatformGateway program that's running directly on the host (not in a container like the rest.)

Let's try...
Run PlatformGateway (its web server using the same sndr.properties configuration, just listening for calls)
Run the Sndr container with bash:

    docker run -it --entrypoint /bin/bash burble-jvm:0.1.0

then use wget to post some data:

    wget --post-data 'wtf dude 2'  http://192.168.1.155:2424/mtReceive
    Connecting to 192.168.1.155:2424 (192.168.1.155:2424)
    saving to 'mtReceive'
    mtReceive            100% |********************************************************************************************|     2  0:00:00 ETA
    'mtReceive' saved

So as long as we got the url right, it should work.
Someone on stackoverflow suggested using 'host.docker.internal' as the host name for the container host. That actually works from
a bash shell inside a running container:
    wget --post-data 'wtf dude 2'  http://host.docker.internal:2424/mtReceive
    Connecting to host.docker.internal:2424 (192.168.65.254:2424)
    saving to 'mtReceive'
    mtReceive            100% |********************************************************************************************|     2  0:00:00 ETA
    'mtReceive' saved

Confirmed that the running program handled it:
    2025-01-09 15:58:12,929 INFO  [] [[0x67cf6f99 0x537fdca4] WebServer socket] c.e.PlatformGateway - Received content: wtf dude 2

When running PlatformGateway, it tells us
    2025-01-09 15:57:27,162 INFO  [] [start @default (/0.0.0.0:2424)] io.helidon.webserver.ServerListener - [0x67cf6f99] http://0.0.0.0:2424 bound for socket '@default'


RANDOM NOTE: https://eclipse.dev/openj9/ OpenJ9 JVM touts faster startup, less memory.

===== 2025-01-18 ======

ServiceAvailabilityTest is still broken due to some innocuous refactoring...

Sndr.init is called (and throws an exception) _after_ the test message are sent and received by the Rcvr:

    2025-01-14 15:54:43,187 INFO  [] [docker-java-stream-788704569] c.e.i.ServiceAvailabilityTest - STDERR: Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method<channel.close>(reply-code=406, reply-text=PRECONDITION_FAILED - inequivalent arg 'durable' for exchange 'test.mt' in vhost '/': received 'true' but current is 'false', class-id=40, method-id=10)

Both the Sndr and FakeOperator can set up the test.mt exchange. Both use the sndr.properties to set the durable property.

That said, even before this the FakeOperator throws almost the same exception for the test.mo queue:

    2025-01-18 13:43:10,542 INFO  [] [docker-java-stream-1817243090] c.e.i.ServiceAvailabilityTest - STDERR: Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method<channel.close>(reply-code=406, reply-text=PRECONDITION_FAILED - inequivalent arg 'durable' for exchange 'test.mo' in vhost '/': received 'true' but current is 'false', class-id=40, method-id=10)

The ordering may simply be an artifact of the logging setup that uses Slf4jLogConsumer to view the container output.

Pared it back to just the rabbit broker and rcvr.
Verified that the messages can be sent to rcvr and  with rabbitmqctl list_queues (from a shell inside the broker container) that they are received.

Next, turned off the Rcvr and turned on the FakeOperator:
    2025-01-19 10:17:04,408 INFO  [] [docker-java-stream-1832978947] c.e.i.ServiceAvailabilityTest - STDOUT: 2025-01-19 15:17:04.406636+00:00 [error] <0.774.0> operation basic.consume caused a channel exception not_found: no queue 'test.mo' in vhost '/'

 Looks like some configuration file errors were causing the trouble.
 Working now.

===== 2025-01-20 ======

The Maven test goal wasn't picking up my integration tests. This is because IntelliJ had auto-imported the older JUnit Test class/annotation instead of the newer Jupiter JUnit 5 version.

Now they run as expected though the Surefire output is lacking the nice formatted .html file. (The regular output are a bunch of text files, one for each test. Really not useful.)

Figured out how to make the surefire plugins (which are so freaking old and pull in so many additional dependencies) generate the readable HTML test report whenever the test goal is run.

Oh, also it appears that running the two container-based tests individually works but running them sequentially via Maven's test goal does not. The PlatformGateway instance isn't shutdown (and port released) before the next instance tries to start.

    2025-01-20 13:50:29,449 ERROR [ff0af9f3-e9de-446f-b242-e4506119c47a] [main] io.helidon.webserver.LoomServer - Failed to start listener: /0.0.0.0:2424
    java.util.concurrent.ExecutionException: java.io.UncheckedIOException: Failed to start server
    ...
    Caused by: java.net.BindException: Address already in use

Hmm, added a stop() method to PlatformGateway that calls through to the contained WebServer's stop() method.
Seems to have resolved the problem.

===== 2025-01-23 ======

On to the next test...testGatewayUnavailable

Added code to support the "restart" of the simulated platform gateway; requires creating a new instance of the web server.

Remembered that the current Sndr really doesn't support retrying, it always acks the message so they won't be delivered ever if the endpoint was missing when the message was received.

While researching the topic I came across:
    https://github.com/joshdevins/rabbitmq-ha-client (NOTE: this is a fairly old project)

 https://www.rabbitmq.com/docs/consumers#acknowledgement-modes covers the basics here with additional documents for the details.

 If a message to a given user is rejected/nacked for gateway availability reasons we want all subsequent messages to be held (not sent) until the first message has been delivered. This consideration is only for individual sessions. Other messages behave according to their sessions.

Side-note: tried integrating spotify's dockerfile-maven-plugin (https://github.com/spotify/dockerfile-maven/blob/master/docs/usage.md) into our pom.xml. Didn't work. Will need to circle around later. It's too simple to write a shell script that does it to spend the time now.


===== 2025-01-24 ======
Wrote rbc.sh to handle rebuilding container after making changes to the non-test code.
TODO
_ restructure Dockerfile-jvm to move the application code to the end; it's not caching layers effectively.
_ Clean up the design of PlatformGateway and HttpMTHandler.

Continuing work on testGatewayUnavailable...

When I added the methods to PlatformGateway to simulate throttling or availability I found that the broker would only requeue/retry the first (failed) message in very rapid succession. This is because the queue consumer declares:
    channel.basicQos(1);
The parameter is named "prefetchCount" which seems like an internally appropriate name but a bit confusing to the developer

===== 2025-01-24 ======

Setting channel.basicQos(3) will change the behavior such that the first three messages are retried/re-queued.
At least anecdotally, they are getting re-processed in the same order as they were put onto the queue.

The scenario we want to handle is, for a given user session with an ordered sequence of events, where the first message is rejected and re-queued the second message is attempted and succeeds. A subsequent delivery attempt for the first message may succeed but the damage is done.

In addition to a some logic to detect send errors due to the structure of a message (rather than a service outage or throttling situation) we need some session logic to prevent out-of-order messaging.

Added a GatewaySimStrategy that simply rejects the first message it sees to approximate the problem.
Calling the com.rabbitmq.client.Channel impl's basicQos() method with a value of 3 we see a final ordering in the current version testGatewayUnavailable:

    Messages received: [27 goodbye, 28 goodbye, 26 goodbye, 29 goodbye, 30 goodbye]

 Here it tries to send a batch of 3 messages. The first is re-queued but the second and third can be sent immediately.

RANDOM NOTE: When we start measuring perf we should make sure to only use System::nanoTime. Apparently the getMillis() is subject to adjustment such that it's possible for it to go backwards!
Also, check out the JMH Java microbenchmark harness: https://github.com/openjdk/jmh and

======== Initial Thoughts about the internals of a real Operator implementation ========
I've been considering whether it would make sense to use Records for this application. Most of the articles I've read don't really explain what domains Records might improve. They wave their hands about Data Transfer Objects which, almost always, feel like a terrible idea. Then I found the following https://blogs.oracle.com/javamagazine/post/records-come-to-java where the author notes:
"...the 'records are nominal tuples' design choice means you should expect that records will work best where you might use tuples in other languages. This includes use cases such as compound map keys or to simulate multi-return from a method. An example compound map key might look like this: record OrderPartition(CurrencyPair pair, Side side) {}"

This has some appeal. Immutable tuples pulled from APIs or similar data sources that can be composed easily and without loads of boilerplate. The compact constructor form also provides a nice place for fail-fast validation.

The Operator, unlike the Editor tools (that will follow), should not need to mutate any of the data it uses. Also Record Patterns (see https://docs.oracle.com/en/java/javase/22/language/record-patterns.html) could prove useful for the processing logic we need to write.

Along these lines and making the Java parts of the system fit well with the functional design of RabbitMQ and (possibly) Phoenix LiveView, consider reading https://www.oreilly.com/library/view/a-functional-approach/9781098109912/ which discusses ways to make Java more functional for the benefits that brings. The author has a blog at https://belief-driven-design.com/looking-at-java-21-switch-pattern-matching-14648/.


===== 2025-01-27 ======
Continuing to think about the session based ordering logic...

We need to add a notion of session groups and sequence order state to our (probably overly simplistic) message model.
Then, I'm thinking, we would reject/re-queue any message from a given session that is not the next expected message.

Start by assuming all the messages are from the same session?

Okay, implemented logic that seems to enforce ordered message delivery by a hardcoded session.
It's hideous looking--we should take a crack at revamping it, possibly with pattern matching--but appears to do what we need.
Let's create some additional variations of the problem to make sure it covers everything we can think of.

===== 2025-01-28 ======
To further test, error handling I need an actual session construct. The tests currently assume only a single session.

Where should this be constructed? In the Rcvr or the Operator?


===== 2025-01-29 ======
RANDOM NOTE: Auth0 has a pretty nice free tier with 25K users, 5 orgs, unlimited Okta & Social connections (not sure what that means), a custom domain with branded domains, and some DoS protection. No credit card. Seems cool.


Initial Domain Modelling for Operator:

Customer---hasMany--->User
|
+---hasMany---> Script

Both Customer and User have the same properties: countryCode, languageCode, brblId, platformId*
A Script is an interface with properties: id, next[] (returns a list of Script ids).
A Session is an interface with properties: id, currentScript, user (User)

An MT is an output from the execution of a Script in the context of a Session.
An MO is an input to the execution of a Script in the context of a Session.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Side-notes:
To address both the "insufficient resolution" and the back-in-time problem with System.currentTimeMillis() I found a class--NanoClock--the provides the wall-clock utility of currentTimeMillis and the better resolution of System.nanoTime() avoiding the impact host adjustments of the clock:
    https://github.com/jenetics/jenetics/blob/master/jenetics/src/main/java/io/jenetics/util/NanoClock.java

For testing purposes if/when we have ML support we could initiate a chat from the MO side and have the system respond. Validation would be weird though.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

===== 2025-01-30 ======
https://www.bandwidth.com/pricing/ shows some reasonable figures e.g. $0.004/message on a U.S. 10DLC.

Random Note of the Day: turso.tech created a fork of SQLite that includes accommodation for vector search--commonly used with RAG-based ML applications. This might be an interesting thing to use instead of spinning up a shared Postgres instance. The folks behind this also founded ScyllaDB. In an interview, Glauber Costa commented that SQLite's creators reserved implementation

===== 2025-02-01 ===
From https://openjdk.org/jeps/499: "Structured concurrency is an approach to concurrent programming that preserves the natural relationship between tasks and subtasks, which leads to more readable, maintainable, and reliable concurrent code. The term 'structured concurrency' was coined by Martin Sústrik and popularized by Nathaniel J. Smith. Ideas from other languages, such as Erlang's hierarchical supervisors, inform the design of error handling in structured concurrency."

Neat. Let's try it out. **Still** in preview for the upcoming Java 24 which is annoying but if the API is close to complete...


===== 2025-02-04 ===

https://www.allareacodes.com/canadian_area_codes.htm

Once I started using StructuredConcurrency/virtual thread preview features I had to add

        <configuration>
            <argLine>--enable-preview</argLine>
        </configuration>

to the maven-surefire-plugin. You can almost hear that old code creaking despite the fact that it continues to make new releases...

The built-in test runners were less pedantic. The IDE added the feature flag to the compiler automatically but not surefire.

One of the tests failed with an ExceptionInInitializer. This was enough to cause surefire (ironically) to fail to produce the surefire.html file.

com.enoughisasgoodasafeast.integration.EndToEndMessagingTest complained "Caused by: org.testcontainers.containers.ContainerLaunchException: Local Docker Compose exited abnormally with code 1 whilst running command: compose up -d"

It didn't fail a second time so we'll move on...

===== 2025-02-05 ===

https://www.rabbitmq.com/docs/shovel is interesting for operational purposes. It can move messages between queues.

https://www.rabbitmq.com/docs/publishers#concurrency drops a bomb on multiple threads writing to the same channel:
    "In general, publishing on a shared 'publishing context' (channel in AMQP 0-9-1, connection in STOMP, session in AMQP 1.0 and so on) should be avoided and considered unsafe."
    " Doing so can result in incorrect framing of data frames on the wire. That leads to connection closure."

So channel per thread?

Looking at https://www.rabbitmq.com/docs/publishers#connection-recovery it appears that the Java client supports automatic recovery of connections and topology (queues, exchanges, bindings, and consumers) so maybe the heartbeat setup isn't something we need to explicitly handle. Perhaps it is enough to handle the cases where we receive a message and the queue is incommunicado.

Confirmed that if a queue comes back after being gone. The existing client/code will send the message quite happily.

Use a local temporary store to continue accepting messages? sqlite?

===== 2025-02-06 =====

Should we change Script to Page and create a new Script that represents a collection of Pages?
Or go the opposite direction and create a Book construct that represents a graph of Scripts?

Got a simple, state machine working with a test that attaches a chain of Scripts.

===== 2025-02-07 =====

Typically, we deal with directed, acyclic graphs but a Customer might want to engage in conversation that might loop around.
We can and probably should drop the acyclic part.
Our reporting metrics will want to be able to show numbers not just for how many users reached a particular node but also how they got there (the sequence of edges.)

There are libraries that provide more general graph functionality. For example, https://jgrapht.org/guide/UserOverview.
Question: do we need more functionality? I don't think so. At least not for the purposes of walking a User through a graph of Scripts.


===== 2025-02-08 =====

Thinking about the more complex logic processing needs, I think the Script record will need some additional data (e.g. the list of choices displayed in the previous script) as well as specialized logic. This could be done as a single object or the data and logic could be a separated into a structured string (as a new field to Script) and a static function that mapped to the Script's type.

If a single object the data would still need to be parsed at initialization time.

===== 2025-02-13 =====

Steve Carey kindly offered to provide a login to his sftp server (scarey.net) where I could put backups. Also, an Ubuntu container that I could use for load test or remote access, etc. What a mensch!
He needs a public key for this purpose.

~> ssh-keygen -t ed25519 -C "Key for scarey's sftp server/other services"
Generating public/private ed25519 key pair.
Enter file in which to save the key (/Users/mark/.ssh/id_ed25519):
Created directory '/Users/mark/.ssh'.
Enter passphrase (empty for no passphrase):
I left the passphrase empty because I've not been the best about remembering such things.

I emailed him the public key.

When I told him about this project and asked him about his thoughts on hosting it from home he sent a link to a thread on reddit that pointed to https://www.servethehome.com/introducing-project-tinyminimicro-home-lab-revolution/

This led me to looking at some other stuff including gitea.com which offers a free-for-open-source self-hosted version of their GitHub-like software.  GitLab, which Steve uses on his home lab, also offers a free version for "personal projects."

We're not quite at this point yet, I think, but useful for when we have a baseline ready.



===== 2025-02-16 =====

Sitting in Logan, waiting to get on a plane to Florida.

Next: replace the List<Script> field, "next" in the Script class with a List<ResponseLogic>

Discovered an interesting wrinkle of sorts about Records with fields that are collections.
I was passing List.of() in one of the constructors which resulted in the next field being an immutable collection.
In my canonical constructor I had logic that initializes the field with an empty, mutable ArrayList.
For the kind of objects graphs we're building we **have to be able** to add elements to the next list.
Even then the order of object creation is difficult.

Created a OperatorTest.