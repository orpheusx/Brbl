https://medium.com/helidon/helidon-logging-and-mdc-5de272cf085d

explains, imperfectly, how to use JUL logging with Helidon.
Turns out the configuration file that Helidon expects is logging.properties at the root of the context.
The format is a little weird.

NAMING SCHEMES:

We're going to use the terminology from our SMS/SMPP days.

for queues

    <platform>.<region>.<direction>

    e.g.

    whatsapp.us.mo  - the platform is whatsapp, the regions is the United States, the direction is Mobile Originated.

    test.local.mo   - the platform is a developer/test, the region is the scope of the test resources (likely a dev machine), the direction is Mobile Originated.

Current design uses Topics with routingKeys.
I'm thinking that we'd likely have separate endpoints running for each platform gateway but using topics & routingKeys gives us flexibility.

Test Setup:

Run the container image with the RabbitMQ server:

    docker run -it --rm --name rabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:4.0-management

FakePlatformMO (client)
    Reads messages from file, for each
    HTTP POST to
     --> Rcvr (port: 4242)
            --> RabbitMQ client enqueue to "test.mo"

     --> "Operator"
         RabbitMQ client dequeues from "test.mo"
         Processes the message (side effects galore)
         Creates response
         Enqueues response to "test.mt"

     --> Sndr
        RabbitMQ client dequeues from "test.mt"
        HTTP POST to

     --> FakePlatformMT (port: 2424)
        Writes each received message to file.

 Validator compares input file with output file.

Test Program:
FakePlatformMO generates messages with known values.
FakePlatformMT receives messages and validates that it corresponds to a previously sent message.


Helidon has libraries that support tracing (https://helidon.io/docs/v4/se/tracing) for both the web server and web client. Note: OpenTracing (https://opentracing.io/) has been retired in favor of OpenTelemetry (https://opentelemetry.io/docs/languages/js/instrumentation/)

OpenAPI is also supported in case parts of the software needs to be used with AWS Lambda.

Scheduling support is available (https://helidon.io/docs/v4/se/scheduling) which should allow us to avoid shit like EventBridge. Under the covers it uses cron-utils (https://github.com/jmrozanec/cron-utils)


=========== Building an executable jar ===========
Added the following components to the <build/> section of the pom.xml file:
    exec-maven-plugin
    maven-compiler-plugin
    maven-jar-plugin
    maven-assembly-plugin
Now we can run:
    mvn clean compile assembly:single

to yield an artifact that can be run with 'java -jar'
The artifact ends up being around 23MB as of 12/12/2024.

Java Microbenchmark Harness:
    https://www.baeldung.com/java-microbenchmark-harness


Trying to compile the project with native-image yielded errors related to logback.
The GraalVM output was pretty helpful. It suggested adding to the file that tracks usages of reflection.
While researching the problem someone suggested switching to the new configuration system available for logback: logback-tyler
This provides a class generator that provides a fairly easy to edit Configurator implementation that doesn't use XML or reflection.

Of course, it couldn't be that simple ;-) There seems to be a bug that causes the generated class to not be marked public. The service-provider mechanism added JDK9 in support of the new package system complained of not having access. Simply editing the class to be public appears to solve the problem. I posted a question to the GitHub discussion board:

    https://github.com/qos-ch/logback-tyler/discussions/5

Needed to add a reachability-metadata.json file to META-INF/native-image to have the .properties files included in the binary.

Some notable/interesting bits from the compiler output:

Top 10 origins of code area:                                Top 10 object types in image heap:
  11.60MB java.base                                            8.48MB byte[] for embedded resources
   2.59MB java.xml                                             5.24MB byte[] for code metadata
   1.22MB svm.jar (Native Image)                               3.31MB byte[] for java.lang.String
 747.48kB logback-core-1.5.12.jar                              2.31MB java.lang.String
 468.30kB amqp-client-5.23.0.jar                               2.20MB java.lang.Class
 326.55kB java.rmi                                           843.80kB byte[] for general heap data
 244.26kB logback-classic-1.5.12.jar                         785.30kB com.oracle.svm.core.hub.DynamicHubCompanion
 207.22kB helidon-http-http2-4.1.4.jar                       554.23kB heap alignment
 203.91kB java.naming                                        513.00kB int[][]
 203.76kB helidon-webclient-api-4.1.4.jar                    491.98kB byte[] for reflection metadata
   1.46MB for 37 more packages                                 5.34MB for 2375 more object types

Recommendations:
 HEAP: Set max heap for improved and more predictable memory usage.
 CPU:  Enable more CPU features with '-march=native' for improved performance.

 Questions: Why are we getting java.xml and java.rmi included?
 The binary size is significant: 50MB.

 We need a way to produce containers that will run locally on my M1
 Can we run a Linux VM locally to run the build? Or even a container?

 NOTE: For a moment I thought the --target option of native-image would let us produce an executable for different platforms.
 This is Java (Write-Once Run Everywhereâ„¢) after. But it turns out it's bullshit. After six years of development this is still on the to do list.
 Given the prevalence of containers which are _only_ supported for Linux its amazing that this hasn't been prioritized.
 As it stands, we'll need to create a bunch of extra container rigging to produce something that we then copy over into another image.

    https://github.com/spotify/dockerfile-maven is a project for building container images with Docker. It's not still in development, however.

=========== STUFF WE WILL WANT ===========

- Software BOM with security information for our dependencies.
- Qodana or something free to perform structured code analysis (we need to take some time to configure this to remove bogus or just unhelpful problems.)
- Container images for arm64 and amd64.

=========== STUFF WE SHOULD LEARN MORE ABOUT ===========
The modules system introduced in JDK 9.
The inner workings of Docker so I can evaluate images, how to extend, etc.

Docker progress:

Finally figured out how to specify/pass arguments when running Burble inside a docker container.
Changed the

docker run -it --rm --name burble -p 2424:2424 -p 4242:4242 burble:0.1 FakeOperator


native-image -jar target/sndrRcvr-1.0-SNAPSHOT-jar-with-dependencies.jar -o burble
    produces a file, burble, that is still fucking massive at 44MB. :-(

The container that builds on eclipse-temurin:23 and includes the binary weighs in at 719MB.

Switching to eclipse-temurin:23-alpine reduces the image to a "mere" 581MB.
Most of the container comes from the installation of
    apk add --no-cache fontconfig ttf-dejavu gnupg ca-certificates p11-kit-trust musl-locales musl-locales-lang binutils tzdata coreutils openssl (63MB)
    OpenJDK23U-jdk_aarch64_alpine-linux_hotspot_23.0.1_11.tar.gz (310MB uncompressed)
The Burble executable is the third-largest item.

We need to add config to the native-image build to specify the musl C lib. This is only applicable to the Linux version, however.
Musl is not a thing for macOS.

Most people apparently use a container to build their linux image. There's an "official" Maven image repo

docker run -it --rm --name mvnc -v "$HOME/.m2":/root/.m2 -v "$(pwd)":/Users/mark/Development/sndrRcvr/src -w /Users/mark/Development/sndrRcvr/src maven:latest mvn package

We want a container
- with graalvm 23
- 3.x Maven
Initially this doesn't need to be alpine based but the musl thing might require it

This repo looks promising: https://github.com/vegardit/docker-graalvm-maven/blob/main/README.md

https://github.com/vegardit/docker-graalvm-maven/blob/main/README.md

=== Example use of the vegardit image to build a native binary for linux ===
docker run --rm -it \
  -v $PWD:/mnt/myproject:rw \       # what's the effect of the :rw part of this?
  -w /mnt/myproject \
  vegardit/graalvm-maven:latest-java23 \
  mvn clean package

Here's how we're using it to

> docker run --rm -it -v "$HOME/.m2/repository":/root/.m2/repository -v "$(pwd)":/mnt/sndrRcvr -w /mnt/sndrRcvr vegardit/graalvm-maven:latest-java23 mvn clean package

Works! Produces a jar file in the target folder of our host OS.

> docker run --rm -it -v "$HOME/.m2/repository":/root/.m2/repository:rw -v "$(pwd)":/mnt/sndrRcvr -w /mnt/sndrRcvr vegardit/graalvm-maven:latest-java23 mvn -Pnative clean package

Works! Produces an ELF executable using the aarch64 instruction set.
NOTE: the unix `file` command points out it's dynamically linked, however:
    target/burble: ELF 64-bit LSB pie executable, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-aarch64.so.1, BuildID[sha1]=99c5d1bf7a89955c2461187540350e00678a5fa1, for GNU/Linux 3.7.0, not stripped

Once the artifact is produced, we can build the docker image that will execute it...

Made a copy of Dockerfile then renamed both.
    Dockerfile.bin - builds an image that uses a native-image generated executable.
    Dockerfile.jvm - builds an image that runs a normal jar file using eclipse-temurin:23-alpine.

Use them like so to build the image:
    docker build -t burble-bin:0.1.0 -f Dockerfile.bin .
    docker build -t burble-jvm:0.1.0 -f Dockerfile.jvm .

Then to run them:
    The binary executable version:
    docker run -it --rm -p 4242:4242 --name burble-bin-rcvr burble-bin:0.1.0 Rcvr
    docker run -it --rm --name burble-bin-fkop burble-bin:0.1.0 FakeOperator
    docker run -it --rm --name burble-bin-sndr burble-bin:0.1.0 Sndr

    The JVM version:
    docker run -it --rm -p 4242:4242 --name burble-jvm-rcvr burble-jvm:0.1.0 Rcvr
    docker run -it --rm --name burble-jvm-fkop burble-jvm:0.1.0 FakeOperator
    docker run -it --rm --name burble-jvm-sndr burble-jvm:0.1.0 Sndr

Where the general form is:
    docker run -it --rm [-p 2424:2424] | [-p 4242:4242] --name burble-jvm burble-jvm:0.1.0 <programName: Rcvr | FakeOperator | Sndr>

The Dockerfiles both default to running FakeOperator.

To run the image without executing the entrypoint/cmd:
    docker run -it --entrypoint /bin/sh burble-bin:0.1.0

Actually it appears that the binary version running on the alpine container fails because, as noted above, the executable is dynamically linked.
It fails with a very terse and somewhat misleading error:
    "exec /opt/app/burble: no such file or directory"
I think this means that it can't find the expected libc rather than the program itself.

I added a block to the configuration section of the native-maven-plugin setup:
<buildArgs>
    <buildArg>--static --libc=musl --enable-sbom</buildArg>
</buildArgs>

Which seems to be signalling the correct switches in the native-image program but the container that produces the linux binary lacks the referenced library.

docker run --rm -it -v "$HOME/.m2/repository":/root/.m2/repository:rw -v "$(pwd)":/mnt/sndrRcvr -w /mnt/sndrRcvr mstewart/graalvm-maven-musl mvn -Pnative clean package

Notable output from native-image:

"HEAP: Set max heap for improved and more predictable memory usage."

Would be nice if they mentioned the switch/param/flag that controls this. Is the normal JVM flag? Where does it get set?

 "CPU:  Enable more CPU features with '-march=native' for improved performance."

 Added <buildArg>-march=native</buildArg> to the native-maven-plugin config which eliminates the message, so I assume it was the right way to do it.

 1 experimental option(s) unlocked:
 - '-H:IncludeResources': Use a resource-config.json in your META-INF/native-image/<groupID>/<artifactID> directory instead. (origin(s): 'META-INF/native-image/com.rabbitmq/amqp-client/native-image.properties' in 'file:///root/.m2/repository/com/rabbitmq/amqp-client/5.23.0/amqp-client-5.23.0.jar')

This is annoying. The lib is providing the needed data so it can be used to produce the native executable, but they've (I guess) changed their mind how this is to be implemented? Worse, they leave it to me to figure out how this is to be represented in the new file.
I think, for now, we will leave this unaddressed. At least until we're certain of the lib versions we want to use.

::IMPORTANT NOTE::
As of 12/26/2024, building a fully statically linked graalvm native image is *only* supported on linux for x86_64:
    https://github.com/oracle/graal/issues/9490
This is sad. The project seems old enough that it shouldn't still be a problem. There's no cross-compilation either though that is a little more understandable.
We should try building the musl/statically linked binary on one of our x86 machines just to be sure it will work.

NEXT STEPS:
X create a working docker-compose.yml to coordinate testing (leave rabbitmq out initially)
_ create some end-to-end tests
_ extend the rabbitmq image to add the admin user and any other customized bits we need then add it to the docker-compose.yaml.
_ create a management CLI using JLine/Jansi, etc.


I swear, where Docker and GraalVM is concerned, every step is a fucking struggle.

Running just a single container via docker-compose.yaml I seem unable to get an HTTP call to the Rcvr web server.

mark@YA-T7RX9LX2PL sndrRcvr % docker compose up -d

mark@YA-T7RX9LX2PL sndrRcvr % curl --verbose http://localhost:4242/health
* Host localhost:4242 was resolved.
* IPv6: ::1
* IPv4: 127.0.0.1
*   Trying [::1]:4242...
* Connected to localhost (::1) port 4242
> GET /health HTTP/1.1
> Host: localhost:4242
> User-Agent: curl/8.7.1
> Accept: */*
>
* Request completely sent off
* Empty reply from server
* Closing connection
curl: (52) Empty reply from server

===== 2024-12-28 ======
Ran the jvm  version of the containers and no longer get the weird HTTP problem. Were the binaries not working?

I re-ran the build_linux.sh script to make sure the executable is what we want.

mark@YA-T7RX9LX2PL sndrRcvr % file target/burble
target/burble: ELF 64-bit LSB pie executable, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-aarch64.so.1, BuildID[sha1]=0c26671ef34dfacfcaaea9595bc5e32e0417980a, for GNU/Linux 3.7.0, not stripped

OK, check.

The build uses a Debian-based container while the execution container is "eclipse-temurin:23" (I think this uses ubuntu as its base.)
Let's try it with "eclipse-temurin:23-jre" as the base layer (this is definitely an Ubuntu image and might be a bit smaller than the JDK version.)

...and, confirmed, that the problem is only with the binary version. Possibly because of the Debian-Ubuntu schism?

No, even when I change the Dockerfile of the image that runs the program to use the same Debian-based image that built the program AND when running curl from within that container.

$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ cat /proc/version
Linux version 6.10.11-linuxkit (root@buildkitsandbox) (gcc (Alpine 13.2.1_git20240309) 13.2.1 20240309, GNU ld (GNU Binutils) 2.42) #1 SMP Thu Oct  3 10:17:28 UTC 2024

Why is the gcc showing Alpine?

Hmm, the RabbitMQ supplied image that we're using--running Noble Numbat, version 24.04.1 LTS--shows the same gcc info. Maybe this is normal?

$ ldd /opt/app/burble
        linux-vdso.so.1 (0x0000ffffa22a2000)
        libz.so.1 => /lib/aarch64-linux-gnu/libz.so.1 (0x0000ffffa2230000)
        libc.so.6 => /lib/aarch64-linux-gnu/libc.so.6 (0x0000ffff9ea50000)
        /lib/ld-linux-aarch64.so.1 (0x0000ffffa2265000)

===== 2024-12-30 ======

Curious if we get a smaller image if we build the jre instead of the jdk...
With JDK...
burble-jvm                               0.1.0            912143180f41   41 hours ago   801MB
With JRE...
burble-jvm                               0.1.0            7171dc515c27   3 minutes ago   507MB

So, yay. The first actual reduction we've seen. GraalVM binaries are still fucking huge.
Baseline manual test (sending messages through the pipeline) appears to work fine.

What could we gain by putting a JRE on top of something like debian:stable-slim (which is used as the base by vegardit/graalvm-maven, which we use for building linux binaries.)

debian                                   stable-slim      5f21ebd35844   7 days ago       136MB
eclipse-temurin                          23               c2a3aba09776   2 months ago     712MB

How does the Ubuntu base for eclipse-temurin compare to slim?

While investigating this I checked out https://hub.docker.com/_/eclipse-temurin.
Two things of note:
They show how to set up the JDK using a reference to one of their images:

    FROM <base image>
    ENV JAVA_HOME=/opt/java/openjdk
    COPY --from=eclipse-temurin:23 $JAVA_HOME $JAVA_HOME
    ENV PATH="${JAVA_HOME}/bin:${PATH}"

They recommend building a custom JRE using jlink. Is this worth the bother?
There's also an eclipse-temurin:23.0.1_11-jre-alpine image.
See https://github.com/docker-library/repo-info/blob/master/repos/eclipse-temurin/local/23-jre-alpine.md

eclipse-temurin                          23.0.1_11-jre-alpine   623a424ca41d   2 months ago     291MB
Inspecting the layers it appears that the JRE comprises ~161MB of this image.
NOTE: busybox, which combines ~400 commands into a single program, doesn't include curl but *does* include wget so we won't need the former to do basic, intra-container testing.

Adding our code only adds 8MB:
burble-jvm                               0.1.0                  52f06679217c   2 minutes ago   298MB

This is a decent tradeoff.

Another good tip here to avoid having to rebuild the image every time the Maven produced jar changes is to mount the host path onto the container. So given,

    FROM eclipse-temurin:21.0.2_13-jdk
    CMD ["java", "-jar", "/opt/app/japp.jar"]

We can run the container like this:
    docker build -t <tag> .
    docker run -it -v /path/on/host/system/jars:/opt/app <tag>

What syntax do we use for a docker-compose file to do this?

TODO
_ Since building a fully statically linked graalvm native image is *only* supported on linux for x86_64 we should bust out our large memory Macbook.
X Create a merged PlatformGatewayMT and MO generator for testing purposes.

Digging into the testcontainers project and found an image for RabbitMQ that's built on an Alpine base.
As expected it's smaller than the non -alpine version:
rabbitmq                                 4.0-management          14c30a03410f   3 months ago   425MB
rabbitmq                                 4.0-management-alpine   74bf73c53b96   3 months ago   295MB

Testcontainers supports running docker-compose files though the logging output is dramatically different from what we see running those files via docker. See https://codeal.medium.com/how-to-run-docker-compose-with-testcontainers-7d1ba73afeeb (it mentions the use of the volumes section of the compose file that, for things like postgres, can be used to run ddl scripts.


We want to be able to do this for integration testing.
That said, testing things we've noted to be possibly problematic--having different components come up out of order or restarting--would seem to require a per-container level of control.

Side-note: It appears possible to use Testcontainers with Docker alternatives, OrbStack and Colima. See http://rockyourcode.com/testcontainers-with-orbstack/ and http://rockyourcode.com/testcontainers-with-colima. Also https://github.com/testcontainers/testcontainers-java/issues/5034#issuecomment-1036433226

Probably not worth the extra effort unless we have problems with Docker.

===== 2025-01-02 ======

Working on EndToEndMessaging:

The docker setup code informs me:
    'container_name' property set for service 'brkr' but this property is not supported by Testcontainers, consider removing it
But clearly it's more than just a suggestion because it throws an ExceptionInInitializerError that kills the container startup.

This is tracked in https://github.com/testcontainers/testcontainers-java/issues/2472, but they don't seem to be interested in fixing it; a perfectly reasonable PR adding support for it--https://github.com/testcontainers/testcontainers-java/pull/2741--was closed.
So I've commented out the container_name in the docker-compose-jvm.yaml file.

Created an integration test, EndToEndMessaging that uses this file. The @Container annotation is supposed to start the docker process, but it doesn't seem to be doing so. Adding a static block that calls the .start() method after the annotation is a simple workaround even if it's a bit disappointing.

Reworked PlatformGatewayMT to act as both a MO source and the MT destination.

Next up: using specific containers to test ordering dependencies.

Scenarios:
- what should Rcvr do if the broker disappears?
    Currently, when messages are received we actually return the following to each calling request:
        connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)Complete.

    Obviously we shouldn't do this.
    Does it recover when the broker comes back?

    Restarted the broker container and got the following in the Rcvr log:

        2025-01-05 10:38:44,542 ERROR [] [AMQP Connection 192.168.1.155:5672] c.r.c.impl.ForgivingExceptionHandler - Caught an exception during connection recovery!
        java.io.IOException: null
        	at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:140)
        	at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:136)
        	at com.rabbitmq.client.impl.AMQConnection.start(AMQConnection.java:406)
        	at com.rabbitmq.client.impl.recovery.RecoveryAwareAMQConnectionFactory.newConnection(RecoveryAwareAMQConnectionFactory.java:71)
        	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.recoverConnection(AutorecoveringConnection.java:628)
        	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.beginAutomaticRecovery(AutorecoveringConnection.java:589)
        	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.lambda$addAutomaticRecoveryListener$3(AutorecoveringConnection.java:524)
        	at com.rabbitmq.client.impl.AMQConnection.notifyRecoveryCanBeginListeners(AMQConnection.java:839)
        	at com.rabbitmq.client.impl.AMQConnection.doFinalShutdown(AMQConnection.java:816)
        	at com.rabbitmq.client.impl.AMQConnection$MainLoop.run(AMQConnection.java:700)
        	at java.base/java.lang.Thread.run(Thread.java:1575)
        Caused by: com.rabbitmq.client.ShutdownSignalException: connection error
        	at com.rabbitmq.utility.ValueOrException.getValue(ValueOrException.java:66)
        	at com.rabbitmq.utility.BlockingValueOrException.uninterruptibleGetValue(BlockingValueOrException.java:36)
        	at com.rabbitmq.client.impl.AMQChannel$BlockingRpcContinuation.getReply(AMQChannel.java:552)
        	at com.rabbitmq.client.impl.AMQConnection.start(AMQConnection.java:336)
        	... 8 common frames omitted
        Caused by: java.io.EOFException: null
        	at java.base/java.io.DataInputStream.readUnsignedByte(DataInputStream.java:297)
        	at com.rabbitmq.client.impl.Frame.readFrom(Frame.java:91)
        	at com.rabbitmq.client.impl.SocketFrameHandler.readFrame(SocketFrameHandler.java:199)
        	at com.rabbitmq.client.impl.AMQConnection$MainLoop.run(AMQConnection.java:687)
        	... 1 common frames omitted

However, when we sent more traffic, the Rcvr handled it and was able to send it to the broker. Then I started the FakeOperator and the messages again passed through.

So is there a callback available to know when the broker is available?

===== 2025-01-07 ======
Having yet more fun with Testcontainers. >:-[

To explore the availability scenarios above, I'm programmatically creating containers for each application.

If I try starting a Rcvr container without a RabbitMQ broker running I get a complaint from InternalCommandPortListeningCheck that seems to stem from the call to withExposedPorts() that's used to connect with the Rcvr's web server.

It's clear, however, that to really have our components be testable in this way we need to provide a direct means of configuring ports.
Added an override mechanism to ConfigLoader that reads the properties file and the environment vars, overriding the former with the latter where they overlap.

Need a similar mechanism for PlatformGateway to provide it the effective http port for the Rcvr...
Hacked a fix for this but still running into the InternalCommandPortListeningCheck:
    https://github.com/testcontainers/testcontainers-java/issues/6730

Fucking hell, maybe it's just easier to add bash.
Did that but also needed to rebuild the jar file (which I had been neglecting.) Time to set up a shared mount for the containers so we don't need to rebuild the container everytime we make a change to the Java code (assuming we even remember to do that!)

===== 2025-01-09 ======
ServiceAvailabilityTest.testRcvrReconnect:
I have most of the containers running and passing messages along, up to where Sndr is supposed to call the PlatformGateway's web server to deliver the generated MTs.
Having trouble communicating with the PlatformGateway program that's running directly on the host (not in a container like the rest.)

Let's try...
Run PlatformGateway (its web server using the same sndr.properties configuration, just listening for calls)
Run the Sndr container with bash:

    docker run -it --entrypoint /bin/bash burble-jvm:0.1.0

then use wget to post some data:

    wget --post-data 'wtf dude 2'  http://192.168.1.155:2424/mtReceive
    Connecting to 192.168.1.155:2424 (192.168.1.155:2424)
    saving to 'mtReceive'
    mtReceive            100% |********************************************************************************************|     2  0:00:00 ETA
    'mtReceive' saved

So as long as we got the url right, it should work.
Someone on stackoverflow suggested using 'host.docker.internal' as the host name for the container host. That actually works from
a bash shell inside a running container:
    wget --post-data 'wtf dude 2'  http://host.docker.internal:2424/mtReceive
    Connecting to host.docker.internal:2424 (192.168.65.254:2424)
    saving to 'mtReceive'
    mtReceive            100% |********************************************************************************************|     2  0:00:00 ETA
    'mtReceive' saved

Confirmed that the running program handled it:
    2025-01-09 15:58:12,929 INFO  [] [[0x67cf6f99 0x537fdca4] WebServer socket] c.e.PlatformGateway - Received content: wtf dude 2

When running PlatformGateway, it tells us
    2025-01-09 15:57:27,162 INFO  [] [start @default (/0.0.0.0:2424)] io.helidon.webserver.ServerListener - [0x67cf6f99] http://0.0.0.0:2424 bound for socket '@default'

Okay, got it sussed. Test is passing.

