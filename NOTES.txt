https://medium.com/helidon/helidon-logging-and-mdc-5de272cf085d

explains, imperfectly, how to use JUL logging with Helidon.
Turns out the configuration file that Helidon expects is logging.properties at the root of the context.
The format is a little weird.

NAMING SCHEMES:

We're going to use the terminology from our SMS/SMPP days.

for queues

    <platform>.<region>.<direction>

    e.g.

    whatsapp.us.mo  - the platform is whatsapp, the region is the United States, the direction is Mobile Originated.

    test.local.mo   - the platform is a developer/test, the region is the scope of the test resources (likely a dev machine), the direction is Mobile Originated.

Current design uses Topics with routingKeys.
I'm thinking that we'd likely have separate endpoints running for each platform gateway but using topics & routingKeys gives us flexibility.

Test Setup:

Run the container image with the RabbitMQ server:

    docker run -it --rm --name rabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:4.0-management

FakePlatformMO (client)
    Reads messages from file, for each
    HTTP POST to
     --> Rcvr (port: 4242)
            --> RabbitMQ client enqueue to "test.mo"

     --> "Operator"
         RabbitMQ client dequeues from "test.mo"
         Processes the message (side effects galore)
         Creates response
         Enqueues response to "test.mt"

     --> Sndr
        RabbitMQ client dequeues from "test.mt"
        HTTP POST to

     --> FakePlatformMT (port: 2424)
        Writes each received message to file.

 Validator compares input file with output file.

Test Program:
FakePlatformMO generates messages with known values.
FakePlatformMT receives messages and validates that it corresponds to a previously sent message.


Helidon has libraries that support tracing (https://helidon.io/docs/v4/se/tracing) for both the web server and web client. Note: OpenTracing (https://opentracing.io/) has been retired in favor of OpenTelemetry (https://opentelemetry.io/docs/languages/js/instrumentation/)

OpenAPI is also supported in case parts of the software needs to be used with AWS Lambda.

Scheduling support is available (https://helidon.io/docs/v4/se/scheduling) which should allow us to avoid shit like EventBridge. Under the covers it uses cron-utils (https://github.com/jmrozanec/cron-utils)


=========== Building an executable jar ===========
Added the following components to the <build/> section of the pom.xml file:
    exec-maven-plugin
    maven-compiler-plugin
    maven-jar-plugin
    maven-assembly-plugin
Now we can run:
    mvn clean compile assembly:single

to yield an artifact that can be run with 'java -jar'
The artifact ends up being around 23MB as of 12/12/2024.

Java Microbenchmark Harness:
    https://www.baeldung.com/java-microbenchmark-harness


Trying to compile the project with native-image yielded errors related to logback.
The GraalVM output was pretty helpful. It suggested adding to the file that tracks usages of reflection.
While researching the problem someone suggested switching to the new configuration system available for logback: logback-tyler
This provides a class generator that provides a fairly easy to edit Configurator implementation that doesn't use XML or reflection.
In addition to making Graal happy the initialization will be faster and more efficient for the lack of XML.

Of course, it couldn't be that simple ;-) There seems to be a bug that causes the generated class to not be marked public. The service-provider mechanism added JDK9 in support of the new package system complained of not having access. Simply editing the class to be public appears to solve the problem. I posted a question to the GitHub discussion board:

    https://github.com/qos-ch/logback-tyler/discussions/5

Needed to add a reachability-metadata.json file to META-INF/native-image to have the .properties files included in the binary.

Some notable/interesting bits from the compiler output:

Top 10 origins of code area:                                Top 10 object types in image heap:
  11.60MB java.base                                            8.48MB byte[] for embedded resources
   2.59MB java.xml                                             5.24MB byte[] for code metadata
   1.22MB svm.jar (Native Image)                               3.31MB byte[] for java.lang.String
 747.48kB logback-core-1.5.12.jar                              2.31MB java.lang.String
 468.30kB amqp-client-5.23.0.jar                               2.20MB java.lang.Class
 326.55kB java.rmi                                           843.80kB byte[] for general heap data
 244.26kB logback-classic-1.5.12.jar                         785.30kB com.oracle.svm.core.hub.DynamicHubCompanion
 207.22kB helidon-http-http2-4.1.4.jar                       554.23kB heap alignment
 203.91kB java.naming                                        513.00kB int[][]
 203.76kB helidon-webclient-api-4.1.4.jar                    491.98kB byte[] for reflection metadata
   1.46MB for 37 more packages                                 5.34MB for 2375 more object types

Recommendations:
 HEAP: Set max heap for improved and more predictable memory usage.
 CPU:  Enable more CPU features with '-march=native' for improved performance.

 Questions: Why are we getting java.xml and java.rmi included?
 The binary size is significant: 50MB.

 We need a way to produce containers that will run locally on my M1
 Can we run a Linux VM locally to run the build? Or even a container?

 NOTE: For a moment I thought the --target option of native-image would let us produce an executable for different platforms.
 This is Java (Write-Once Run Everywhere™) after. But it turns out it's bullshit. After six years of development this is still on the to do list.
 Given the prevalence of containers which are _only_ supported for Linux its amazing that this hasn't been prioritized.
 As it stands, we'll need to create a bunch of extra container rigging to produce something that we then copy over into another image.

    https://github.com/spotify/dockerfile-maven is a project for building container images with Docker. It's not still in development, however.

=========== STUFF WE WILL WANT ===========

- Software BOM with security information for our dependencies.
- Qodana or something free to perform structured code analysis (we need to take some time to configure this to remove bogus or just unhelpful problems.)
- Container images for arm64 and amd64.

=========== STUFF WE SHOULD LEARN MORE ABOUT ===========
The modules system introduced in JDK 9.
The inner workings of Docker so I can evaluate images, how to extend, etc.

Docker progress:

Finally figured out how to specify/pass arguments when running Burble inside a docker container.
Changed the

docker run -it --rm --name burble -p 2424:2424 -p 4242:4242 burble:0.1 FakeOperator


native-image -jar target/sndrRcvr-1.0-SNAPSHOT-jar-with-dependencies.jar -o burble
    produces a file, burble, that is still fucking massive at 44MB. :-(

The container that builds on eclipse-temurin:23 and includes the binary weighs in at 719MB.

Switching to eclipse-temurin:23-alpine reduces the image to a "mere" 581MB.
Most of the container comes from the installation of
    apk add --no-cache fontconfig ttf-dejavu gnupg ca-certificates p11-kit-trust musl-locales musl-locales-lang binutils tzdata coreutils openssl (63MB)
    OpenJDK23U-jdk_aarch64_alpine-linux_hotspot_23.0.1_11.tar.gz (310MB uncompressed)
The Burble executable is the third-largest item.

We need to add config to the native-image build to specify the musl C lib. This is only applicable to the Linux version, however.
Musl is not a thing for macOS.

Most people apparently use a container to build their linux image. There's an "official" Maven image repo

docker run -it --rm --name mvnc -v "$HOME/.m2":/root/.m2 -v "$(pwd)":/Users/mark/Development/sndrRcvr/src -w /Users/mark/Development/sndrRcvr/src maven:latest mvn package

We want a container
- with graalvm 23
- 3.x Maven
Initially this doesn't need to be alpine based but the musl thing might require it

This repo looks promising: https://github.com/vegardit/docker-graalvm-maven/blob/main/README.md

https://github.com/vegardit/docker-graalvm-maven/blob/main/README.md

=== Example use of the vegardit image to build a native binary for linux ===
docker run --rm -it \
  -v $PWD:/mnt/myproject:rw \       # what's the effect of the :rw part of this?
  -w /mnt/myproject \
  vegardit/graalvm-maven:latest-java23 \
  mvn clean package

Here's how we're using it to

> docker run --rm -it -v "$HOME/.m2/repository":/root/.m2/repository -v "$(pwd)":/mnt/sndrRcvr -w /mnt/sndrRcvr vegardit/graalvm-maven:latest-java23 mvn clean package

Works! Produces a jar file in the target folder of our host OS.

> docker run --rm -it -v "$HOME/.m2/repository":/root/.m2/repository:rw -v "$(pwd)":/mnt/sndrRcvr -w /mnt/sndrRcvr vegardit/graalvm-maven:latest-java23 mvn -Pnative clean package

Works! Produces an ELF executable using the aarch64 instruction set.
NOTE: the unix `file` command points out it's dynamically linked, however:
    target/burble: ELF 64-bit LSB pie executable, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-aarch64.so.1, BuildID[sha1]=99c5d1bf7a89955c2461187540350e00678a5fa1, for GNU/Linux 3.7.0, not stripped

Once the artifact is produced, we can build the docker image that will execute it...

Made a copy of Dockerfile then renamed both.
    Dockerfile.bin - builds an image that uses a native-image generated executable.
    Dockerfile.jvm - builds an image that runs a normal jar file using eclipse-temurin:23-alpine.

Use them like so to build the image:
    docker build -t burble-bin:0.1.0 -f Dockerfile.bin .
    docker build -t burble-jvm:0.1.0 -f Dockerfile.jvm .

Then to run them:
    The binary executable version:
    docker run -it --rm -p 4242:4242 --name burble-bin-rcvr burble-bin:0.1.0 Rcvr
    docker run -it --rm --name burble-bin-fkop burble-bin:0.1.0 FakeOperator
    docker run -it --rm --name burble-bin-sndr burble-bin:0.1.0 Sndr

    The JVM version:
    docker run -it --rm -p 4242:4242 --name burble-jvm-rcvr burble-jvm:0.1.0 Rcvr
    docker run -it --rm --name burble-jvm-fkop burble-jvm:0.1.0 FakeOperator
    docker run -it --rm --name burble-jvm-sndr burble-jvm:0.1.0 Sndr

Where the general form is:
    docker run -it --rm [-p 2424:2424] | [-p 4242:4242] --name burble-jvm burble-jvm:0.1.0 <programName: Rcvr | FakeOperator | Sndr>

The Dockerfiles both default to running FakeOperator.

To run the image without executing the entrypoint/cmd:
    docker run -it --entrypoint /bin/sh burble-bin:0.1.0

Actually it appears that the binary version running on the alpine container fails because, as noted above, the executable is dynamically linked.
It fails with a very terse and somewhat misleading error:
    "exec /opt/app/burble: no such file or directory"
I think this means that it can't find the expected libc rather than the program itself.

I added a block to the configuration section of the native-maven-plugin setup:
<buildArgs>
    <buildArg>--static --libc=musl --enable-sbom</buildArg>
</buildArgs>

Which seems to be signalling the correct switches in the native-image program but the container that produces the linux binary lacks the referenced library.

docker run --rm -it -v "$HOME/.m2/repository":/root/.m2/repository:rw -v "$(pwd)":/mnt/sndrRcvr -w /mnt/sndrRcvr mstewart/graalvm-maven-musl mvn -Pnative clean package

Notable output from native-image:

"HEAP: Set max heap for improved and more predictable memory usage."

Would be nice if they mentioned the switch/param/flag that controls this. Is it the normal JVM flag? Where does it get set?

 "CPU:  Enable more CPU features with '-march=native' for improved performance."

 Added <buildArg>-march=native</buildArg> to the native-maven-plugin config which eliminates the message, so I assume it was the right way to do it.

 1 experimental option(s) unlocked:
 - '-H:IncludeResources': Use a resource-config.json in your META-INF/native-image/<groupID>/<artifactID> directory instead. (origin(s): 'META-INF/native-image/com.rabbitmq/amqp-client/native-image.properties' in 'file:///root/.m2/repository/com/rabbitmq/amqp-client/5.23.0/amqp-client-5.23.0.jar')

This is annoying. The lib is providing the needed data so it can be used to produce the native executable, but they've (I guess) changed their mind how this is to be implemented? Worse, they leave it to me to figure out how this is to be represented in the new file.
I think, for now, we will leave this unaddressed. At least until we're certain of the lib versions we want to use.

::IMPORTANT NOTE::
As of 12/26/2024, building a fully statically linked graalvm native image is *only* supported on linux for x86_64:
    https://github.com/oracle/graal/issues/9490
This is sad. The project seems old enough that it shouldn't still be a problem. There's no cross-compilation either though that is a little more understandable.
We should try building the musl/statically linked binary on one of our x86 machines just to be sure it will work.

NEXT STEPS:
X create a working docker-compose.yml to coordinate testing (leave rabbitmq out initially)
_ create some end-to-end tests
_ extend the rabbitmq image to add the admin user and any other customized bits we need then add it to the docker-compose.yaml.
_ create a management CLI using JLine/Jansi, etc.


I swear, where Docker and GraalVM is concerned, every step is a fucking struggle.

Running just a single container via docker-compose.yaml I seem unable to get an HTTP call to the Rcvr web server.

mark@YA-T7RX9LX2PL sndrRcvr % docker compose up -d

mark@YA-T7RX9LX2PL sndrRcvr % curl --verbose http://localhost:4242/health
* Host localhost:4242 was resolved.
* IPv6: ::1
* IPv4: 127.0.0.1
*   Trying [::1]:4242...
* Connected to localhost (::1) port 4242
> GET /health HTTP/1.1
> Host: localhost:4242
> User-Agent: curl/8.7.1
> Accept: */*
>
* Request completely sent off
* Empty reply from server
* Closing connection
curl: (52) Empty reply from server

===== 2024-12-28 ======
Ran the jvm  version of the containers and no longer get the weird HTTP problem. Were the binaries not working?

I re-ran the build_linux.sh node to make sure the executable is what we want.

mark@YA-T7RX9LX2PL sndrRcvr % file target/burble
target/burble: ELF 64-bit LSB pie executable, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-aarch64.so.1, BuildID[sha1]=0c26671ef34dfacfcaaea9595bc5e32e0417980a, for GNU/Linux 3.7.0, not stripped

OK, check.

The build uses a Debian-based container while the execution container is "eclipse-temurin:23" (I think this uses ubuntu as its base.)
Let's try it with "eclipse-temurin:23-jre" as the base layer (this is definitely an Ubuntu image and might be a bit smaller than the JDK version.)

...and, confirmed, that the problem is only with the binary version. Possibly because of the Debian-Ubuntu schism?

No, even when I change the Dockerfile of the image that runs the program to use the same Debian-based image that built the program AND when running curl from within that container.

$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ cat /proc/version
Linux version 6.10.11-linuxkit (root@buildkitsandbox) (gcc (Alpine 13.2.1_git20240309) 13.2.1 20240309, GNU ld (GNU Binutils) 2.42) #1 SMP Thu Oct  3 10:17:28 UTC 2024

Why is the gcc showing Alpine?

Hmm, the RabbitMQ supplied image that we're using--running Noble Numbat, version 24.04.1 LTS--shows the same gcc info. Maybe this is normal?

$ ldd /opt/app/burble
        linux-vdso.so.1 (0x0000ffffa22a2000)
        libz.so.1 => /lib/aarch64-linux-gnu/libz.so.1 (0x0000ffffa2230000)
        libc.so.6 => /lib/aarch64-linux-gnu/libc.so.6 (0x0000ffff9ea50000)
        /lib/ld-linux-aarch64.so.1 (0x0000ffffa2265000)

===== 2024-12-30 ======

Curious if we get a smaller image if we build the jre instead of the jdk...
With JDK...
burble-jvm                               0.1.0            912143180f41   41 hours ago   801MB
With JRE...
burble-jvm                               0.1.0            7171dc515c27   3 minutes ago   507MB

So, yay. The first actual reduction we've seen. GraalVM binaries are still fucking huge.
Baseline manual test (sending messages through the pipeline) appears to work fine.

What could we gain by putting a JRE on top of something like debian:stable-slim (which is used as the base by vegardit/graalvm-maven, which we use for building linux binaries.)

debian                                   stable-slim      5f21ebd35844   7 days ago       136MB
eclipse-temurin                          23               c2a3aba09776   2 months ago     712MB

How does the Ubuntu base for eclipse-temurin compare to slim?

While investigating this I checked out https://hub.docker.com/_/eclipse-temurin.
Two things of note:
They show how to set up the JDK using a reference to one of their images:

    FROM <base image>
    ENV JAVA_HOME=/opt/java/openjdk
    COPY --from=eclipse-temurin:23 $JAVA_HOME $JAVA_HOME
    ENV PATH="${JAVA_HOME}/bin:${PATH}"

They recommend building a custom JRE using jlink. Is this worth the bother?
There's also an eclipse-temurin:23.0.1_11-jre-alpine image.
See https://github.com/docker-library/repo-info/blob/master/repos/eclipse-temurin/local/23-jre-alpine.md

eclipse-temurin                          23.0.1_11-jre-alpine   623a424ca41d   2 months ago     291MB
Inspecting the layers it appears that the JRE comprises ~161MB of this image.
NOTE: busybox, which combines ~400 commands into a single program, doesn't include curl but *does* include wget so we won't need the former to do basic, intra-container testing.

Adding our code only adds 8MB:
burble-jvm                               0.1.0                  52f06679217c   2 minutes ago   298MB

This is a decent tradeoff.

Another good tip here to avoid having to rebuild the image every time the Maven produced jar changes is to mount the host path onto the container. So given,

    FROM eclipse-temurin:21.0.2_13-jdk
    CMD ["java", "-jar", "/opt/app/japp.jar"]

We can run the container like this:
    docker build -t <tag> .
    docker run -it -v /path/on/host/system/jars:/opt/app <tag>

What syntax do we use for a docker-compose file to do this?

TODO
_ Since building a fully statically linked graalvm native image is *only* supported on linux for x86_64 we should bust out our large memory Macbook.
X Create a merged PlatformGatewayMT and MO generator for testing purposes.

Digging into the testcontainers project and found an image for RabbitMQ that's built on an Alpine base.
As expected it's smaller than the non -alpine version:
rabbitmq                                 4.0-management          14c30a03410f   3 months ago   425MB
rabbitmq                                 4.0-management-alpine   74bf73c53b96   3 months ago   295MB

Testcontainers supports running docker-compose files though the logging output is dramatically different from what we see running those files via docker. See https://codeal.medium.com/how-to-run-docker-compose-with-testcontainers-7d1ba73afeeb (it mentions the use of the volumes section of the compose file that, for things like postgres, can be used to run ddl scripts.


We want to be able to do this for integration testing.
That said, testing things we've noted to be possibly problematic--having different components come up out of order or restarting--would seem to require a per-container level of control.

Side-note: It appears possible to use Testcontainers with Docker alternatives, OrbStack and Colima. See http://rockyourcode.com/testcontainers-with-orbstack/ and http://rockyourcode.com/testcontainers-with-colima. Also https://github.com/testcontainers/testcontainers-java/issues/5034#issuecomment-1036433226

Probably not worth the extra effort unless we have problems with Docker.

===== 2025-01-02 ======

Working on EndToEndMessaging:

The docker setup code informs me:
    'container_name' property set for service 'brkr' but this property is not supported by Testcontainers, consider removing it
But clearly it's more than just a suggestion because it throws an ExceptionInInitializerError that kills the container startup.

This is tracked in https://github.com/testcontainers/testcontainers-java/issues/2472, but they don't seem to be interested in fixing it; a perfectly reasonable PR adding support for it--https://github.com/testcontainers/testcontainers-java/pull/2741--was closed.
So I've commented out the container_name in the docker-compose-jvm.yaml file.

Created an integration test, EndToEndMessaging that uses this file. The @Container annotation is supposed to start the docker process, but it doesn't seem to be doing so. Adding a static block that calls the .start() method after the annotation is a simple workaround even if it's a bit disappointing.

Reworked PlatformGatewayMT to act as both a MO source and the MT destination.

Next up: using specific containers to test ordering dependencies.

Scenarios:
- what should Rcvr do if the broker disappears?
    Currently, when messages are received we actually return the following to each calling request:
        connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)Complete.

    Obviously we shouldn't do this.
    Does it recover when the broker comes back?

    Restarted the broker container and got the following in the Rcvr log:

        2025-01-05 10:38:44,542 ERROR [] [AMQP Connection 192.168.1.155:5672] c.r.c.impl.ForgivingExceptionHandler - Caught an exception during connection recovery!
        java.io.IOException: null
        	at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:140)
        	at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:136)
        	at com.rabbitmq.client.impl.AMQConnection.start(AMQConnection.java:406)
        	at com.rabbitmq.client.impl.recovery.RecoveryAwareAMQConnectionFactory.newConnection(RecoveryAwareAMQConnectionFactory.java:71)
        	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.recoverConnection(AutorecoveringConnection.java:628)
        	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.beginAutomaticRecovery(AutorecoveringConnection.java:589)
        	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.lambda$addAutomaticRecoveryListener$3(AutorecoveringConnection.java:524)
        	at com.rabbitmq.client.impl.AMQConnection.notifyRecoveryCanBeginListeners(AMQConnection.java:839)
        	at com.rabbitmq.client.impl.AMQConnection.doFinalShutdown(AMQConnection.java:816)
        	at com.rabbitmq.client.impl.AMQConnection$MainLoop.run(AMQConnection.java:700)
        	at java.base/java.lang.Thread.run(Thread.java:1575)
        Caused by: com.rabbitmq.client.ShutdownSignalException: connection error
        	at com.rabbitmq.utility.ValueOrException.getValue(ValueOrException.java:66)
        	at com.rabbitmq.utility.BlockingValueOrException.uninterruptibleGetValue(BlockingValueOrException.java:36)
        	at com.rabbitmq.client.impl.AMQChannel$BlockingRpcContinuation.getReply(AMQChannel.java:552)
        	at com.rabbitmq.client.impl.AMQConnection.start(AMQConnection.java:336)
        	... 8 common frames omitted
        Caused by: java.io.EOFException: null
        	at java.base/java.io.DataInputStream.readUnsignedByte(DataInputStream.java:297)
        	at com.rabbitmq.client.impl.Frame.readFrom(Frame.java:91)
        	at com.rabbitmq.client.impl.SocketFrameHandler.readFrame(SocketFrameHandler.java:199)
        	at com.rabbitmq.client.impl.AMQConnection$MainLoop.run(AMQConnection.java:687)
        	... 1 common frames omitted

However, when we sent more traffic, the Rcvr handled it and was able to send it to the broker. Then I started the FakeOperator and the messages again passed through.

So is there a callback available to know when the broker is available?

===== 2025-01-07 ======
Having yet more fun with Testcontainers. >:-[

To explore the availability scenarios above, I'm programmatically creating containers for each application.

If I try starting a Rcvr container without a RabbitMQ broker running I get a complaint from InternalCommandPortListeningCheck that seems to stem from the call to withExposedPorts() that's used to connect with the Rcvr's web server.

It's clear, however, that to really have our components be testable in this way we need to provide a direct means of configuring ports.
Added an override mechanism to ConfigLoader that reads the properties file and the environment vars, overriding the former with the latter where they overlap.

Need a similar mechanism for PlatformGateway to provide it the effective http port for the Rcvr...
Hacked a fix for this but still running into the InternalCommandPortListeningCheck:
    https://github.com/testcontainers/testcontainers-java/issues/6730

Fucking hell, maybe it's just easier to add bash.
Did that but also needed to rebuild the jar file (which I had been neglecting.) Time to set up a shared mount for the containers so we don't need to rebuild the container everytime we make a change to the Java code (assuming we even remember to do that!)

===== 2025-01-09 ======
ServiceAvailabilityTest.testRcvrReconnect:
I have most of the containers running and passing messages along, up to where Sndr is supposed to call the PlatformGateway's web server to deliver the generated MTs.
Having trouble communicating with the PlatformGateway program that's running directly on the host (not in a container like the rest.)

Let's try...
Run PlatformGateway (its web server using the same sndr.properties configuration, just listening for calls)
Run the Sndr container with bash:

    docker run -it --entrypoint /bin/bash burble-jvm:0.1.0

then use wget to post some data:

    wget --post-data 'wtf dude 2'  http://192.168.1.155:2424/mtReceive
    Connecting to 192.168.1.155:2424 (192.168.1.155:2424)
    saving to 'mtReceive'
    mtReceive            100% |********************************************************************************************|     2  0:00:00 ETA
    'mtReceive' saved

So as long as we got the url right, it should work.
Someone on stackoverflow suggested using 'host.docker.internal' as the host name for the container host. That actually works from
a bash shell inside a running container:
    wget --post-data 'wtf dude 2'  http://host.docker.internal:2424/mtReceive
    Connecting to host.docker.internal:2424 (192.168.65.254:2424)
    saving to 'mtReceive'
    mtReceive            100% |********************************************************************************************|     2  0:00:00 ETA
    'mtReceive' saved

Confirmed that the running program handled it:
    2025-01-09 15:58:12,929 INFO  [] [[0x67cf6f99 0x537fdca4] WebServer socket] c.e.PlatformGateway - Received content: wtf dude 2

When running PlatformGateway, it tells us
    2025-01-09 15:57:27,162 INFO  [] [start @default (/0.0.0.0:2424)] io.helidon.webserver.ServerListener - [0x67cf6f99] http://0.0.0.0:2424 bound for socket '@default'


RANDOM NOTE: https://eclipse.dev/openj9/ OpenJ9 JVM touts faster startup, less memory.

===== 2025-01-18 ======

ServiceAvailabilityTest is still broken due to some innocuous refactoring...

Sndr.init is called (and throws an exception) _after_ the test message are sent and received by the Rcvr:

    2025-01-14 15:54:43,187 INFO  [] [docker-java-stream-788704569] c.e.i.ServiceAvailabilityTest - STDERR: Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method<channel.close>(reply-code=406, reply-text=PRECONDITION_FAILED - inequivalent arg 'durable' for exchange 'test.mt' in vhost '/': received 'true' but current is 'false', class-id=40, method-id=10)

Both the Sndr and FakeOperator can set up the test.mt exchange. Both use the sndr.properties to set the durable property.

That said, even before this the FakeOperator throws almost the same exception for the test.mo queue:

    2025-01-18 13:43:10,542 INFO  [] [docker-java-stream-1817243090] c.e.i.ServiceAvailabilityTest - STDERR: Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method<channel.close>(reply-code=406, reply-text=PRECONDITION_FAILED - inequivalent arg 'durable' for exchange 'test.mo' in vhost '/': received 'true' but current is 'false', class-id=40, method-id=10)

The ordering may simply be an artifact of the logging setup that uses Slf4jLogConsumer to view the container output.

Pared it back to just the rabbit broker and rcvr.
Verified that the messages can be sent to rcvr and  with rabbitmqctl list_queues (from a shell inside the broker container) that they are received.

Next, turned off the Rcvr and turned on the FakeOperator:
    2025-01-19 10:17:04,408 INFO  [] [docker-java-stream-1832978947] c.e.i.ServiceAvailabilityTest - STDOUT: 2025-01-19 15:17:04.406636+00:00 [error] <0.774.0> operation basic.consume caused a channel exception not_found: no queue 'test.mo' in vhost '/'

 Looks like some configuration file errors were causing the trouble.
 Working now.

===== 2025-01-20 ======

The Maven test goal wasn't picking up my integration tests. This is because IntelliJ had auto-imported the older JUnit Test class/annotation instead of the newer Jupiter JUnit 5 version.

Now they run as expected though the Surefire output is lacking the nice formatted .html file. (The regular output are a bunch of text files, one for each test. Really not useful.)

Figured out how to make the surefire plugins (which are so freaking old and pull in so many additional dependencies) generate the readable HTML test report whenever the test goal is run.

Oh, also it appears that running the two container-based tests individually works but running them sequentially via Maven's test goal does not. The PlatformGateway instance isn't shutdown (and port released) before the next instance tries to start.

    2025-01-20 13:50:29,449 ERROR [ff0af9f3-e9de-446f-b242-e4506119c47a] [main] io.helidon.webserver.LoomServer - Failed to start listener: /0.0.0.0:2424
    java.util.concurrent.ExecutionException: java.io.UncheckedIOException: Failed to start server
    ...
    Caused by: java.net.BindException: Address already in use

Hmm, added a stop() method to PlatformGateway that calls through to the contained WebServer's stop() method.
Seems to have resolved the problem.

===== 2025-01-23 ======

On to the next test...testGatewayUnavailable

Added code to support the "restart" of the simulated platform gateway; requires creating a new instance of the web server.

Remembered that the current Sndr really doesn't support retrying, it always acks the message so they won't be delivered ever if the endpoint was missing when the message was received.

While researching the topic I came across:
    https://github.com/joshdevins/rabbitmq-ha-client (NOTE: this is a fairly old project)

 https://www.rabbitmq.com/docs/consumers#acknowledgement-modes covers the basics here with additional documents for the details.

 If a message to a given user is rejected/nacked for gateway availability reasons we want all subsequent messages to be held (not sent) until the first message has been delivered. This consideration is only for individual sessions. Other messages behave according to their sessions.

Side-note: tried integrating spotify's dockerfile-maven-plugin (https://github.com/spotify/dockerfile-maven/blob/master/docs/usage.md) into our pom.xml. Didn't work. Will need to circle around later. It's too simple to write a shell node that does it to spend the time now.


===== 2025-01-24 ======
Wrote rbc.sh to handle rebuilding container after making changes to the non-test code.
TODO
_ restructure Dockerfile-jvm to move the application code to the end; it's not caching layers effectively.
_ Clean up the design of PlatformGateway and HttpMTHandler.

Continuing work on testGatewayUnavailable...

When I added the methods to PlatformGateway to simulate throttling or availability I found that the broker would only requeue/retry the first (failed) message in very rapid succession. This is because the queue consumer declares:
    channel.basicQos(1);
The parameter is named "prefetchCount" which seems like an internally appropriate name but a bit confusing to the developer

===== 2025-01-24 ======

Setting channel.basicQos(3) will change the behavior such that the first three messages are retried/re-queued.
At least anecdotally, they are getting re-processed in the same order as they were put onto the queue.

The scenario we want to handle is, for a given user session with an ordered sequence of events, where the first message is rejected and re-queued the second message is attempted and succeeds. A subsequent delivery attempt for the first message may succeed but the damage is done.

In addition to a some logic to detect send errors due to the structure of a message (rather than a service outage or throttling situation) we need some session logic to prevent out-of-order messaging.

Added a GatewaySimStrategy that simply rejects the first message it sees to approximate the problem.
Calling the com.rabbitmq.client.Channel impl's basicQos() method with a value of 3 we see a final ordering in the current version testGatewayUnavailable:

    Messages received: [27 goodbye, 28 goodbye, 26 goodbye, 29 goodbye, 30 goodbye]

 Here it tries to send a batch of 3 messages. The first is re-queued but the second and third can be sent immediately.

RANDOM NOTE: When we start measuring perf we should make sure to only use System::nanoTime. Apparently the getMillis() is subject to adjustment such that it's possible for it to go backwards!
Also, check out the JMH Java microbenchmark harness: https://github.com/openjdk/jmh and

======== Initial Thoughts about the internals of a real Operator implementation ========
I've been considering whether it would make sense to use Records for this application. Most of the articles I've read don't really explain what domains Records might improve. They wave their hands about Data Transfer Objects which, almost always, feel like a terrible idea. Then I found the following https://blogs.oracle.com/javamagazine/post/records-come-to-java where the author notes:
"...the 'records are nominal tuples' design choice means you should expect that records will work best where you might use tuples in other languages. This includes use cases such as compound map keys or to simulate multi-return from a method. An example compound map key might look like this: record OrderPartition(CurrencyPair pair, Side side) {}"

This has some appeal. Immutable tuples pulled from APIs or similar data sources that can be composed easily and without loads of boilerplate. The compact constructor form also provides a nice place for fail-fast validation.

The Operator, unlike the Editor tools (that will follow), should not need to mutate any of the data it uses. Also Record Patterns (see https://docs.oracle.com/en/java/javase/22/language/record-patterns.html) could prove useful for the processing logic we need to write.

Along these lines and making the Java parts of the system fit well with the functional design of RabbitMQ and (possibly) Phoenix LiveView, consider reading https://www.oreilly.com/library/view/a-functional-approach/9781098109912/ which discusses ways to make Java more functional for the benefits that brings. The author has a blog at https://belief-driven-design.com/looking-at-java-21-switch-pattern-matching-14648/.


===== 2025-01-27 ======
Continuing to think about the session based ordering logic...

We need to add a notion of session groups and sequence order state to our (probably overly simplistic) message model.
Then, I'm thinking, we would reject/re-queue any message from a given session that is not the next expected message.

Start by assuming all the messages are from the same session?

Okay, implemented logic that seems to enforce ordered message delivery by a hardcoded session.
It's hideous looking--we should take a crack at revamping it, possibly with pattern matching--but appears to do what we need.
Let's create some additional variations of the problem to make sure it covers everything we can think of.

===== 2025-01-28 ======
To further test, error handling I need an actual session construct. The tests currently assume only a single session.

Where should this be constructed? In the Rcvr or the Operator?


===== 2025-01-29 ======
RANDOM NOTE: Auth0 has a pretty nice free tier with 25K users, 5 orgs, unlimited Okta & Social connections (not sure what that means), a custom domain with branded domains, and some DoS protection. No credit card. Seems cool.


Initial Domain Modelling for Operator:

Customer---hasMany--->User
|
+---hasMany---> Script

Both Customer and User have the same properties: countryCode, languageCode, brblId, platformId*
A Script is an interface with properties: id, next[] (returns a list of Script ids).
A Session is an interface with properties: id, currentNode, user (User)

An MT is an output from the execution of a Script in the context of a Session.
An MO is an input to the execution of a Script in the context of a Session.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Side-notes:
To address both the "insufficient resolution" and the back-in-time problem with System.currentTimeMillis() I found a class--NanoClock--the provides the wall-clock utility of currentTimeMillis and the better resolution of System.nanoTime() avoiding the impact host adjustments of the clock:
    https://github.com/jenetics/jenetics/blob/master/jenetics/src/main/java/io/jenetics/util/NanoClock.java

For testing purposes if/when we have ML support we could initiate a chat from the MO side and have the system respond. Validation would be weird though.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

===== 2025-01-30 ======
https://www.bandwidth.com/pricing/ shows some reasonable figures e.g. $0.004/message on a U.S. 10DLC.

Random Note of the Day: turso.tech created a fork of SQLite that includes accommodation for vector search--commonly used with RAG-based ML applications. This might be an interesting thing to use instead of spinning up a shared Postgres instance. The folks behind this also founded ScyllaDB. In an interview, Glauber Costa commented that SQLite's creators reserved implementation

===== 2025-02-01 ===
From https://openjdk.org/jeps/499: "Structured concurrency is an approach to concurrent programming that preserves the natural relationship between tasks and subtasks, which leads to more readable, maintainable, and reliable concurrent code. The term 'structured concurrency' was coined by Martin Sústrik and popularized by Nathaniel J. Smith. Ideas from other languages, such as Erlang's hierarchical supervisors, inform the design of error handling in structured concurrency."

Neat. Let's try it out. **Still** in preview for the upcoming Java 24 which is annoying but if the API is close to complete...


===== 2025-02-04 ===

https://www.allareacodes.com/canadian_area_codes.htm

Once I started using StructuredConcurrency/virtual thread preview features I had to add

        <configuration>
            <argLine>--enable-preview</argLine>
        </configuration>

to the maven-surefire-plugin. You can almost hear that old code creaking despite the fact that it continues to make new releases...

The built-in test runners were less pedantic. The IDE added the feature flag to the compiler automatically but not surefire.

One of the tests failed with an ExceptionInInitializer. This was enough to cause surefire (ironically) to fail to produce the surefire.html file.

com.enoughisasgoodasafeast.integration.EndToEndMessagingTest complained "Caused by: org.testcontainers.containers.ContainerLaunchException: Local Docker Compose exited abnormally with code 1 whilst running command: compose up -d"

It didn't fail a second time so we'll move on...

===== 2025-02-05 ===

https://www.rabbitmq.com/docs/shovel is interesting for operational purposes. It can move messages between queues.

https://www.rabbitmq.com/docs/publishers#concurrency drops a bomb on multiple threads writing to the same channel:
    "In general, publishing on a shared 'publishing context' (channel in AMQP 0-9-1, connection in STOMP, session in AMQP 1.0 and so on) should be avoided and considered unsafe."
    " Doing so can result in incorrect framing of data frames on the wire. That leads to connection closure."
NOTE: this applies to _publishing_ not consuming. Even so, it makes consider sticking with a single channel...

Looking at https://www.rabbitmq.com/docs/publishers#connection-recovery it appears that the Java client supports automatic recovery of connections and topology (queues, exchanges, bindings, and consumers) so maybe the heartbeat setup isn't something we need to explicitly handle. Perhaps it is enough to handle the cases where we receive a message and the queue is incommunicado.

Confirmed that if a queue comes back after being gone. The existing client/code will send the message quite happily.

Use a local temporary store to continue accepting messages? sqlite?

===== 2025-02-06 =====

Should we change Script to Page and create a new Script that represents a collection of Pages?
Or go the opposite direction and create a Book construct that represents a graph of Scripts?

Got a simple, state machine working with a test that attaches a chain of Scripts.

===== 2025-02-07 =====

Typically, we deal with directed, acyclic graphs but a Customer might want to engage in conversation that might loop around.
We can and probably should drop the acyclic part.
Our reporting metrics will want to be able to show numbers not just for how many users reached a particular node but also how they got there (the sequence of edges.)

There are libraries that provide more general graph functionality. For example, https://jgrapht.org/guide/UserOverview.
Question: do we need more functionality? I don't think so. At least not for the purposes of walking a User through a graph of Scripts.


===== 2025-02-08 =====

Thinking about the more complex logic processing needs, I think the Script record will need some additional data (e.g. the list of choices displayed in the previous node) as well as specialized logic. This could be done as a single object or the data and logic could be a separated into a structured string (as a new field to Script) and a static function that mapped to the Script's type.

If a single object the data would still need to be parsed at initialization time.

===== 2025-02-13 =====

Steve Carey kindly offered to provide a login to his sftp server (scarey.net) where I could put backups. Also, an Ubuntu container that I could use for load test or remote access, etc. What a mensch!
He needs a public key for this purpose.

~> ssh-keygen -t ed25519 -C "Key for scarey's sftp server/other services"
Generating public/private ed25519 key pair.
Enter file in which to save the key (/Users/mark/.ssh/id_ed25519):
Created directory '/Users/mark/.ssh'.
Enter passphrase (empty for no passphrase):
I left the passphrase empty because I've not been the best about remembering such things.

I emailed him the public key.

When I told him about this project and asked him about his thoughts on hosting it from home he sent a link to a thread on reddit that pointed to https://www.servethehome.com/introducing-project-tinyminimicro-home-lab-revolution/

This led me to looking at some other stuff including gitea.com which offers a free-for-open-source self-hosted version of their GitHub-like software.  GitLab, which Steve uses on his home lab, also offers a free version for "personal projects."

We're not quite at this point yet, I think, but useful for when we have a baseline ready.



===== 2025-02-16 =====

Sitting in Logan, waiting to get on a plane to Florida.

Next: replace the List<Script> field, "next" in the Script class with a SequencedSet<ResponseLogic>

Discovered an interesting wrinkle of sorts about Records with fields that are collections.
I was passing List.of() as a parameter to one of the constructors which resulted in the "next" field being an immutable collection.
In my canonical constructor I had logic that initializes the field with an empty, mutable ArrayList.
For the kind of objects graphs we're building we **have to be able** to add elements to the next list to be able to link objects together in a graph of Scripts.
Even then the order of object creation is difficult.

Created a OperatorTest.


===== 2025-02-19 =====
In Orlando...
The problem with EndToEndMessagingIT were due to changes in the host name not reflected in the rcvr.properties file. I'm not sure of the best way to programmatically deal with this.
Compounding the problem, the integration test container images are baked with the .properties files and jar inside. It would be better if we could use some of the dockerfile machinery to have it read those resources from the file system...

===== 2025-02-20 =====
Flying home.
I'm giving up for now on the integration tests. Neither of them are working now.
Both of the test runner plugins make the dubious choice of not rendering a report if there are failures. Not helpful. Maybe I should fork them and make my own.

At any rate, I added a buffer to the Session that will hold the messages produced in the course of an MOMessage event. This raises some questions about how to avoid concurrently modification of a Session and all it's bits, however.

===== 2025-02-26 =====
Sick for the last five days... ugh.

So, yeah, access to the Session likely needs to synchronized to avoid problems when there are 2+ messages received for the same instance. Further complication: if we use a shared cache (e.g. Redis) the object equality of two deserialized Sessions might be an issue.

This assumes that the callbacks from the rabbit client are

We might use the solution to this to help deal with the lack of thread-safety in the Rabbit channel used to add messages.

https://www.rabbitmq.com/client-libraries/java-api-guide#concurrency

They recommend pooling channels. There's a Spring-based connection pool impl, but I'd rather not bring in all that stuff.

There's a project that uses some of the Apache Commons' pooling framework: https://github.com/sytuacmdyh/amqp-client-pool

===== 2025-03-09 =====

The RabbitQueueConsumer needs to be refactored to remove the MTHandler. This is too hardwired to the FakeOperator and test setups we had before. MTConsumer, likewise, needs to be changed/removed.

===== 2025-03-10 =====
Completed the refactoring for RabbitQueueConsumer and its use by Operator.

Sndr is next...

===== 2025-03-11 =====

As I started work on the Sndr, I realized there wasn't a compelling reason to make MOMessage and MTMessage different classes. These have been merged.

Continue to explore how we want the node logic to work. Pivot and TopicSelection represent common cases we know from our previous experience.

Do we need the 'previous' field in the Script class? We could instead track previously executed Scripts (and navigate back to them) via a list on the Session. Removing previous would make constructing graphs of Scripts easier and less brittle. Detours and deviations from the initial chain of Scripts (because the User wants to talk about something else or asks to go to the top of the conversational tree) could then also be handled dynamically. In any case, we'll want to keep a history of a User's actual path through a conversation (the nodes they visited vs the Script graph that was constructed.)

< User input (short code)
> Present ("welcome"...ask question)
< User input (answer)
> Process (match answer)
> Present (acknowledge and ask another question)
< User input ("change topic")
> Process (no match, detect change topic)
     [optional]
        > Present (ask navigational question) [optional]
        < User input ("start new topic")
        > Process (match answer)
> Present (display new topics and ask for selection)
...


===== 2025-03-16 =====
https://www.rabbitmq.com/client-libraries/java-api-guide#concurrency:
    "When manual acknowledgements are used, it is important to consider what thread does the acknowledgement. If it's different from the thread that received the delivery (e.g. Consumer#handleDelivery delegated delivery handling to a different thread), acknowledging with the multiple parameter set to true is unsafe and will result in double-acknowledgements, and therefore a channel-level protocol exception that closes the channel. Acknowledging a single message at a time can be safe."

===== 2025-03-17 =====
Tachometer is an interesting plugin for Docker. It shows cpu and memory usage of containers in real-time.
Also, https://docs.fluentd.org/container-deployment/docker-logging-driver looks like it might be useful.

Actually..."NOTE: Currently, the Fluentd logging driver doesn't support sub-second precision." Oof! That might be a problem...

===== 2025-03-21 =====
TODO
_ Update OperatorMessageFlowIT to use testcontainers instead of assuming a running instance of RabbitMQ.
X Figure out how to structure the replacement for the generation of the top level topic node chain.
    --> just static functions in a class namespace?
X Update the docker files to point at jar's on the host filesystem.
_ Add build section to our docker-compose.


docker exec -it e016a29bccd69dde8e6a28d3c1dd35d84db8ae2ef7e1e0eeb79d6a114169b19c /bin/bash
wget --post-data=<message text> http://<host>:<port>/<pathInfo>


===== 2025-03-25 =====

With no additional networking config, I can use wget to reach the host machine from inside a bash shell running in the sndr container with either host.docker.internal or 192.168.1.155.
I cannot use localhost, however. Nor the 0.0.0.0 pseudo address used for listeners to attach to all interfaces.

Added a "reachability" test send in Sndr's main. This removes all the noise from the stack trace. Here we find that we're able to reach host.docker.internal but are receiving a 404 for some reason.

This led me to testing via the HttpMTHandler directly. Doing so confirmed where the problem was actually happening.
Looks like we were effectively specifying the pathInfo **twice** when making the POST...

Yes, confirmed. Annoyingly the HttpMTHandler code is littered with comments about its shitty state but it didn't occur to me that it was the problem. Argh....

The running thread is listed in the "Processed message:" log written by sndr-1's OperatorConsumer. We were wondering how many threads the Rabbit driver would use to deliver messages. Here's the munged log listing of counts and distinct thread ids:

   5 [pool-1-thread-10]
   1 [pool-1-thread-3]
   1 [pool-1-thread-4]
   2 [pool-1-thread-5]
   2 [pool-1-thread-6]
   1 [pool-1-thread-7]
   2 [pool-1-thread-8]
   2 [pool-1-thread-9]

NOTE: 1 of the occurrences is for the "handleConsumeOk called with consumerTag" log. The actual total here is 15 because we sent the batch of 5 three times. This is only anecdotal obviously but also informative. Note: the Macbook we're using has 8 cores.
 Also anecdotal, the log indicate the messages were processed in order.


===== 2025-03-26 =====

Noticed that the list_queues command for rabbitmqctl shows four entries. Two are expected (test.mo and test.mt) but for the other pair are, according to my google research, the names used are random strings prefixed by "amq.gen-".

The total system makes four Connections (1 each for Sndr and Rcvr and 2 for Operator.) These are all managed by the two classes, RabbitQueueProducer and RabbitQueueConsumer which always (I think) pass in names for the queue. So where are these extras coming from?

Also, the message counts list_queues displays suggest that the messages sent to each of intended queues--test.mo and test.mt--are also sent to the anonymous queues, as if they were paired with the former.

Hmm, in the logs I'm noticing this:
brkr-1      | 2025-03-25 14:51:36.300810+00:00 [info] <0.652.0> connection 172.18.0.1:63666 -> 172.18.0.2:5672: user 'guest' authenticated and granted access to vhost '/'
operator-1  | 2025-03-25 10:51:36,319 INFO  [] [main] c.e.RabbitQueueConsumer - AMQP.Queue.DeclareOk: queue=amq.gen-7cnH4S2ocnwNzTSHZb1wvw consumerCount=0 messageCount=0
operator-1  | 2025-03-25 10:51:36,321 INFO  [] [main] c.e.RabbitQueueConsumer - AMQP.Queue.BindOk: protocolClassId=50 protocolMethodId=21 protocolMethodName=queue.bind-ok
operator-1  | 2025-03-25 10:51:36,324 WARN  [] [pool-1-thread-3] c.e.OperatorConsumer - handleConsumeOk called with consumerTag amq.ctag-RU77QLZAZiDGLk1UYHB3dQ

Resolved the problem. We weren't using the queueName to create the queue. Doh.


===== 2025-03-28 =====

https://medium.com/@skillcate/sentiment-analysis-using-nltk-vader-98f67f2e6130 talks about using VADER.
https://www.youtube.com/watch?v=szczpgOEdXs includes link to code using BERT and PyTorch for sentiment analysis. Includes params for using Nvidia CUDA support. The model used includes support for Spanish though I suspect it's European Spanish. English support, too, of course.

Note, the downloads of the pretrained models require 600-700mb. This is a lot less than the Llama models (measured in hundreds of gigabytes.)


To handle multiple responses to the same question (think Holy Grail's "blue..no green!")
in the response evaluation logic handle add an error check that considers whether the user input matches one of the responses in the previously presented question. If it does it may be that they're trying to change their answer. In those cases, ask for confirmation. If confirmed, rewind to that question prefixed by a "Did you want to change your answer?". When we get to the stage of turning message history into statistics we'll need to be able to address these response changes.

TODO
_ Update OperatorMessageFlowIT to use testcontainers instead of assuming a running instance of RabbitMQ.
X Add build section to our docker-compose? I *think* this is something for later actually...

My updated test isn't receiving the input message in the named queue. The caller doesn't complain at all but the Rabbit broker log shows:


2025-03-30 11:32:52 2025-03-30 15:32:52.232928+00:00 [error] <0.1665.0> Channel error on connection <0.1656.0> (172.17.0.1:55644 -> 172.17.0.2:5672, vhost: '/', user: 'guest'), channel 1:
2025-03-30 11:32:52 2025-03-30 15:32:52.232928+00:00 [error] <0.1665.0> operation basic.publish caused a channel exception not_found: no exchange 'test.mo' in vhost '/'

Ah, okay. So we broke this when we addressed the machine generated queue name issue. See RabbitQueueFunctions.exchangeForQueueName().
Reviewing their documentation, I realized that the argument passed to channel.basicPublish() is the **exchangeName**, not the queueName. This didn't matter before because these names were the same.

Side note: while working on this, the power to the house was shut off. During that period, running one of the tests, threw "java.net.SocketException: Network is unreachable." My interpretation is that using IP addresses required a functioning router even though they were all effectively local.

Still some troubles here...running each of the test in OperatorMessageFlowIT individually via IDEA works fine but running the test class(also via IDEA) does not. Three of the four tests fail waiting for output to arrive in the InMemoryQueueProducer.

Executing mvn verify (the goal that runs integration tests) fails for three of the tests as well but this time because of a java.lang.UnsupportedClassVersionError.
Adding the same configuration.argLine we use for the surefire plugin to the failsafe config in pom.xml solves this:

    <configuration>
        <argLine>--enable-preview</argLine>
    </configuration>

Now we see the same issue when using IDEA to run all the tests in the class:
    org.awaitility.core.ConditionTimeoutException: Condition with Lambda expression in com.enoughisasgoodasafeast.integration.OperatorMessageFlowIT was not fulfilled within 5 seconds.

Running the OperatorMessageFlowIT class with just a single test (commenting out the @Test annotation for the others) works via IDEA and mvn verify. Something to do with state?

===== 2025-04-01 =====

Debugging...

with two connection setups in the same test method, the first succeeds and the second fails.

Paused the latter after the message was sent to the queue, the output from rabbitmqctl list_queues shows zero entries in test.mo.
So is the problem in the producer rather than the consumer?

It appears that the problem stemmed from the first Rabbit producer and consumer clients still being connected to the broker.
So, at least for the purposes of unit/integration tests, I've added shutdown() methods to the QueueProducer and QueueConsumer interfaces and the Operator class. These call close() on the Channel and Connection instances. Unlike some supported Rabbit types (topics, streams, etc) a queue doesn't deliver messages to multiple consumers. By contrast, having multiple producers is fine.

===== 2025-04-03 =====
Revisited EndToEndMessagingIT and managed to get it working. One caveat, running it repeatedly often fails with this:
    "org.testcontainers.containers.ContainerLaunchException: Local Docker Compose exited abnormally with code 1 whilst running command: compose up -d"
The failure appears to happen after it's created the network and all four containers while it's starting the RabbitMQ container (brkr).
The start operation appears to have succeeded, however. I can shell into it and run rabbitmqctl commands. The failure triggers the teardown of the other containers but the brkr container continues to run. So really not ideal from a test automation/continuous integration perspective.

The Testcontainers team seems to reluctantly support the use of docker compose. So maybe this is the best I can expect. :-{

===== 2025-04-07 =====

While debugging ServiceAvailabilityIT I was trying to examine the burble-jvm image to see why it could find the jar file.

Finally figured out how to run the image without having it immediately exit. Here's how:

    docker run -it --rm --name rcvr --entrypoint "sh" burble-jvm:0.1.0

The jar file *does* exist per the COPY command in Dockerfile.jvm.

    /opt/app # ls -la /opt/app/sndrRcvr-1.0-SNAPSHOT-jar-with-dependencies.jar
    -rw-r--r-- 1 root root 4668198 Apr  5 20:59 /opt/app/sndrRcvr-1.0-SNAPSHOT-jar-with-dependencies.jar

Is it a permission problem? Doesn't seem like it. I can run the entrypoint and cmd from the shell without issue. Provided there's a RabbitMQ container running, all the producers and consumers are able to connect.

Running it manually, the container works as expected, connecting to the broker without complaint.

Ah. The problem was that our dev version of the Dockerfile (Dockerfile.dev.jvm) doesn't copy the jar file into the image. It expects the docker run command will include a volume mapping of /Users/mark/Development/sndrRcvr on the host to /opt/app on the container.


===== 2025-04-10 =====

Time to create some tools for interacting with a running system.

Picocli and jline3 look to be useful and lightweight building blocks for an interactive CLI.
Use an extended version of PlatformGateway.



1) create persona
    > input telephone number
    > input shortcode
    < display newly created persona
2) list personas
    < display all available personas with id number (write these to a serialized file that can be loaded at start up)
3) send message (initially just a send but being able to queue messages and send as a batch will be useful.)
    > display "Which persona will send the message?" then list available personas by id
    > input selected id
    < display selected persona then prompt "Enter the text of the message:"
    > input message text
    < display confirmation dialog "Send message? y/n"
    > input choice (if yes, send message)
    > wait for response (5 seconds) and display it. Otherwise "No response received."
4) exit

Take a param (-l) to signal desire to load any serialized files found in a special directory.



===== 2025-04-16 =====

brblcli is working w/r/t to sending messages and triggering node chains. The hardcoded node for short code "45678" will correctly
track the state for the user.

Lots of work yet to make the CLI easy to use but a good start.

Made some additional changes to the Session:
    - Changed the inputHistory implementation in Session to be fixed size.
    - Added requirement that constructor params be non null and started a unit test for the class.


===== 2025-04-17 =====

Starting on the persistence layer...
Considered SQLite but decided to go with Postgres.

Simplest task is to record receipt of MOs and transmission of MTs.
Ideally we could use the basic records but we'd like to relate them to the Script state.

User  <---:fk:--- Profile

UUID id                           --> String or UUID
Map<Platform, String> platformIds --> punt on this for now?
String countryCode                --> CREATE TYPE country_code AS ENUM ('us', 'ca', 'mx');
List<String> languages)           --> CREATE TYPE language_code AS ENUM ('en', 'es', 'fr', ...);

Installing Postgres...

Used the postgres.app version which is quite permissive with its default pg_hba.conf settings but seems pretty simple.

Learned about server-side prepared statements:
    https://jdbc.postgresql.org/documentation/server-prepare/#server-prepared-statements
Probably a good idea, if we use them, to also set on the connection:
    autosave=conservative
This to avoid having to bounce the server if/when the query plan is changed. There is, however, a warning about the performance impact on long transactions with (auto)save points.

Let's comment it out and wait till we have the code worked out before trying it out.

Created a properly named database:
    brbl_dev

===== 2025-04-17 =====

psql -U mark -d brbl_dev -a -f <ddl-or-dml>.sql

See dev.ddl in src/main/resources/sql.

Note "timestamp with time zone" is, perhaps surprisingly, the right data type for a Java Instant value:
    See https://wiki.postgresql.org/wiki/Don't_Do_This#Don.27t_use_timestamp_.28without_time_zone.29_to_store_UTC_times

Using psql to show all tables
 \dt *.*

Using psql to show all users
 \du

java.time.Instance supports nanos --> java.sql.Timestamp --> Postgres

Gemini suggests using the timestamp9 extension instead of separate fields for the
CREATE EXTENSION timestamp9;

mark@yakko sndrRcvr % brew info postgresql@17
==> postgresql@17: stable 17.2 (bottled) [keg-only]
Object-relational database system
https://www.postgresql.org/
Not installed
From: https://github.com/Homebrew/homebrew-core/blob/HEAD/Formula/p/postgresql@17.rb
License: PostgreSQL
==> Dependencies
Build: docbook ✘, docbook-xsl ✘, gettext ✘, pkgconf ✘
Required: icu4c@76 ✘, krb5 ✘, lz4 ✔, openssl@3 ✔, readline ✘, zstd ✔, gettext ✘
==> Caveats
This formula has created a default database cluster with:
  initdb --locale=C -E UTF-8 /opt/homebrew/var/postgresql@17

When uninstalling, some dead symlinks are left behind so you may want to run:
  brew cleanup --prune-prefix

postgresql@17 is keg-only, which means it was not symlinked into /opt/homebrew,
because this is an alternate version of another formula.

To start postgresql@17 now and restart at login:
  brew services start postgresql@17
Or, if you don't want/need a background service you can just run:
  LC_ALL="C" /opt/homebrew/opt/postgresql@17/bin/postgres -D /opt/homebrew/var/postgresql@17
==> Analytics
install: 6,954 (30 days), 21,611 (90 days), 36,400 (365 days)
install-on-request: 6,307 (30 days), 19,641 (90 days), 32,337 (365 days)
build-error: 23 (30 days)

After installing the brew package and starting the server I can connect via:
    psql -d postgres

This connects as user 'mark' to the only non-template database available, 'postgres'. Not confusing at all.
I'm used to 'postgres' being the db super user but here it's 'mark'

Creating a table with a column of type 'timestamp with time zone' looks like this:

    postgres=# insert into test_time(received_at_ms) values(NOW());
    INSERT 0 1
    postgres=# select * from test_time ;
            received_at_ms
    -------------------------------
     2025-04-22 09:28:49.010806-04
    (1 row)

This only gets us to the milliseconds level.

Google's Gemini suggests using time9, a Postgres extension, as a way of supporting nanosecond level timestamps. This instead of splitting the milliseconds and nanosecond components into two columns.

The extension isn't part of the regular distro and requires compiling the code here (https://github.com/optiver/timestamp9)

Needed to install cmake which is very dumb about checking it's version requiring me to hack the timestamp9's CMakeLists.txt file.

...and also the c.h file to comment out references to ENABLE_NLS. I tried #undef'ing it at the top of the file as well as setting

    #define ENABLE_NLS 0

in pg_config.h but neither was sufficient. At first blush, it doesn't seem to be something that will impact Brbl's functionality.

mark@yakko build % sudo make install

Password:
[  0%] Built target controlfile
[ 66%] Built target timestamp9
[100%] Built target sqlfile
[100%] Built target sqlupdatescripts
Install the project...
-- Install configuration: "Release"
-- Installing: /opt/homebrew/lib/postgresql@17/timestamp9.so
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.4.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--0.1.0--0.2.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--0.2.0--0.3.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--0.3.0--1.0.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.0.0--1.0.1.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.0.1--1.1.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.1.0--1.2.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.2.0--1.3.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.3.0--1.4.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9.control

Restarted the database and connected via psql as superuser (mark) then ran

    CREATE EXTENSION timestamp9;

which spits out:

    ERROR:  could not access file "$libdir/timestamp9": No such file or directory

Ah, the produced library file is named with linux convention with the .so extension.

mark@yakko timestamp9 % file /opt/homebrew/lib/postgresql@17/timestamp9.so
/opt/homebrew/lib/postgresql@17/timestamp9.so: Mach-O 64-bit bundle arm64

So just rename the file? Apparently so...

    postgres=# CREATE EXTENSION timestamp9;
    CREATE EXTENSION

When declaring a column of this type use timestamp9 leaving off the "WITH TIME ZONE" used with the regular TIMESTAMP type.

Hmm... populating a timestamp9 column via JDBC with Timestamp.from(message.receivedAt()) compared to one with using "timestamp with time zone" -->

    timestamp with time zone      |   timestamp9
    ------------------------------|------------------------------------
    2025-04-23 22:03:03.177013-04 | 2025-04-23 22:03:03.177013000 -0400

    It seems like it's just padding the same number by 1000.

Printing the numbers as jdk is running the Message creation code:
    message.receivedAt() ==> 2025-04-24T02:03:03.177013083Z
    message.receivedAt().getEpochSecond()    ==> 177013083
    message.receivedAt().getNano()           ==> 1745460183



See PersistenceManager.instantInvestigation

nanoclock produced instant...
Instant:        2025-04-24T22:08:02.454020250Z --> year-month-day-hours-minutes-seconds-nanoseconds
    --> The last part of this is 9 digits
toEpochMilli:   1745532482454
    --> plug this into https://www.epochconverter.com/ --> number of milliseconds since epoch. Nanos are divided by 1 million so only the leading three digits (of a 9 digit number) are added as milliseconds. As the method name suggests its resolution is only ms.
getEpochSecond: 1745532482    --> Just the number of seconds since the epoch.
getNano:        454020250   --> the nanos that are added
instant converted to timestamp...
as timestamp time: 1745532482454
as timestamp nanos: 454020250
timestamp converted back to instant
Instant:        2025-04-24T22:08:02.454020250Z --> year-month-day-hours-minutes-seconds-nanoseconds
toEpochMilli:   1745532482454
getEpochSecond: 1745532482
getNano:        454020250

Original instant equal to round tripped instant? true

The complete timestamp expressed in nanoseconds would be:
    1745532482454020150 (19 digits)
    9223372036854775807 is the max positive value of a Java long.
Checking and comparing the values while converting from Instant to Timestamp...looks good. Nothing getting dropped.

It seems like the JDBC driver is truncating all but the leading three places of nanoseconds from the java.sql.Timestamp value when it writes the value to the column. This makes sense if the db's timestamp type limits resolution to microseconds.
Also, weirdly, the timezone offset goes from two digits to four. For example, Eastern Daylight Savings Time it becomes -0400 instead of -04 when psql displays the column value.

We wanted to get better than the thousandth of a second resolution of milliseconds (that means 0-999) but regular Instant/Timestamp stored in a Timestamp column in Postgres gets us another three places of resolution that seems to be equivalent to microseconds. Perhaps this is enough? It would eliminate the need to overcome the JDBC and display issues using the timestamp9 data type.

Note: I am thinking/writing this on the train back from NYC and I'm super tired so totally possible I'm getting it all wrong.


===== 2025-04-27 =====

https://www.postgresql.org/docs/current/datatype-datetime.html confirms that the timestamp data type supports microseconds. So our observations were correct.

This is enough for our purposes and removes the need for non-standard type extensions. Moving on...

Thinking about where, in the pipeline, we write the incoming MO...
We want to be able to associate the Message with the Script element that was used to process it.
Likewise we want to associate the outgoing MT with the Script that generated it.
So how about ...

For each incoming Message:
    - Rcvr writes the full Message with its UUID, the timestamp, and the rest. Before or after enqueuing?
            CREATE TABLE brbl_logs.messages_mo (
                id          UUID NOT NULL,
                rcvd_at     TIMESTAMP WITH TIME ZONE NOT NULL,
                _from       VARCHAR(15) NOT NULL,
                _to         VARCHAR(15) NOT NULL,   --> how long are non-SMS (WhatsApp, FB Messenger, etc) identifiers?
                _text       VARCHAR(2000) NOT NULL  --> should match the MT length
            );
    - Operator writes an abbreviated row with just the UUID of the Message, the Session UUID, and the UUID of the handling node.
            CREATE TABLE brbl_logs.messages_mo_prcd (
                id          UUID NOT NULL,
                prcd_at     TIMESTAMP WITH TIME ZONE NOT NULL,
                session_id  UUID NOT NULL,
                script_id   UUID NOT NULL
            );

            To join the two, showing including messages that were received but not processed:
            SELECT
                rcvd.id, prcd.session_id, prcd.script_id, rcvd._text
            FROM
                brbl_logs.messages_mo AS rcvd
            LEFT JOIN
                brbl_logs.messages_mo_prcd AS prcd
            ON rcvd.id = prcd.id;

For each outgoing Message:
    - Operator writes the full Message immediately after enqueuing it.

    CREATE TABLE brbl_logs.messages_mt (
        id          UUID NOT NULL,
        sent_at     TIMESTAMP WITH TIME ZONE NOT NULL,
        _from       VARCHAR(15) NOT NULL,
        _to         VARCHAR(15) NOT NULL,   --> how long are non-SMS (WhatsApp, FB Messenger, etc) identifiers?
        _text       VARCHAR(2000) NOT NULL,  --> should match the MO length
        session_id  UUID NOT NULL,
        script_id   UUID NOT NULL
    );

For each delivered Message acked by the 3rd party gateway:
    - Sndr writes an abbreviated row with just the UUID and the delivery timestamp.

    CREATE TABLE brbl_logs.messages_mt_dlvr (
        id          UUID NOT NULL,
        dlvr_at     TIMESTAMP WITH TIME ZONE NOT NULL
    );

===== 2025-04-29 ====
Finished the insert methods for all four record types.

In tests, inserting two records (brbl_logs.messages_mo and brbl_logs.messages_mo_prcd) appears to take a whopping 112 milliseconds.

delete from brbl_logs.messages_mo;
delete from brbl_logs.messages_mo_prcd;
delete from brbl_logs.messages_mt;
delete from brbl_logs.messages_mt_dlvr ;

select prcd_at - rcvd_at from brbl_logs.messages_mo mos inner join brbl_logs.messages_mo_prcd prcd on mos.id = prcd.id;
select dlvr_at - sent_at from brbl_logs.messages_mt mts inner join brbl_logs.messages_mt_dlvr dlvr on mts.id = dlvr.id;

No doubt creating a brand new Connection object for each isn't helping to make this quick.

===== 2025-04-29 ====

Plugged in a c3p0 connection pool. This helps reduce the insert times especially if I initialize it before running the timed tests.
The init call, by itself, took 117 ms to setup 5 connections.
But subsequent calls to our fetchConnection method return in microseconds.

The batch mode insert functions seem to be a good bit slower (5ms) than the single inserts.
Is this because they are writing more data?
Let's try removing the batch setup...helps a little.
Looking at the times for the insert of the two records we'll do for each and every MO:
    insertMO: b 2025-04-30T14:22:50.188750583Z a 2025-04-30T14:22:50.192991833Z: d PT0.00424125S
        ~> 4.2 ms
    insertProcessedMO: b 2025-04-30T14:22:50.194844917Z a 2025-04-30T14:22:50.195541667Z: d PT0.00069675S
        ~> 0.6 ms
And for MTs...
    insertMT: b 2025-04-30T14:22:50.195807458Z a 2025-04-30T14:22:50.196264708Z: d PT0.00045725S
        ~> 0.4 ms
    insertDeliveredMT: b 2025-04-30T14:22:50.196432250Z a 2025-04-30T14:22:50.197066292Z: d PT0.000634042S
        ~> 0.6 ms
Okay, so not terrible anymore. To be fair, my testing is really not all that meaningful since it is unrealistically optimized (no other overhead from related application code, database running on localhost) and, at the same time, not ideal (no JVM warmup.) Its quite terrible even as seat-of-the-pants micro benchmarks go.


===== 2025-04-30 ====

Cleaned up and pushed the new PersistenceManager class.


===== 2025-05-01 ====

Integrating PersistenceManager with Rcvr...

Note: With the Postgres server running locally we need to use host.docker.internal as the hostname for the database connection pool.

Ok, got the MO write working in Rcvr.


===== 2025-05-06 ====

Processed MO, MT, and Delivered MT writes now working...

SELECT prcd_at - rcvd_at FROM brbl_logs.messages_mo mos INNER JOIN brbl_logs.messages_mo_prcd prcd ON mos.id = prcd.id;
SELECT dlvr_at - sent_at FROM brbl_logs.messages_mt mts INNER JOIN brbl_logs.messages_mt_dlvr dlvr ON mts.id = dlvr.id;

SELECT
    mos.id, mts.id, mts._text
FROM
    brbl_logs.messages_mo_prcd mos
INNER
    join brbl_logs.messages_mt mts
ON
    mos.session_id = mts.session_id;

To calculate the elapsed time to receive, process and respond to an MO:

    SELECT (dlv.dlvr_at - mos.rcvd_at) AS total_response_time
    FROM
        brbl_logs.messages_mo mos
    INNER JOIN
        brbl_logs.messages_mo_prcd prc ON mos.id = prc.id
    INNER JOIN
        brbl_logs.messages_mt mts ON prc.script_id = mts.script_id
    INNER JOIN
        brbl_logs.messages_mt_dlvr dlv ON mts.id = dlv.id
;

For the 5 messages we send from PlatformGateway the results look something like:

total_response_time
---------------------
 00:00:00.274_010
 00:00:00.210_633
 00:00:00.203_084
 00:00:00.197_272
 00:00:00.195_147
(5 rows)

Run a second time, the numbers improve a bit...

 total_response_time
---------------------
 00:00:00.108_010
 00:00:00.067_296
 00:00:00.052_973
 00:00:00.046_465
 00:00:00.044_713

Not neck snapping performance for individual message processing but the concurrency numbers are better.

The inter-message delta for the rcvd time shrinks dramatically proving the value of the database connection pool and threaded http server:

    1)  2025-05-06 09:02:08.063024-04
    2)  2025-05-06 09:02:08.134534-04 --> #2 - #1 = 128_510
    3)  2025-05-06 09:02:08.145519-04 --> #3 - #2 = 010_985
    4)  2025-05-06 09:02:08.154437-04 --> #4 - #3 = 008_918
    5)  2025-05-06 09:02:08.159682-04 --> #5 - #4 = 005_245

We should revisit implementing a RabbitMQ producer channel pool at some point...

Delta between MT delivered and MO processed time:

    SELECT (dlv.dlvr_at - prc.prcd_at) AS total_response_time
    FROM
        brbl_logs.messages_mo_prcd prc
    INNER JOIN
        brbl_logs.messages_mt mts ON prc.script_id = mts.script_id
    INNER JOIN
        brbl_logs.messages_mt_dlvr dlv ON mts.id = dlv.id
    ;

 total_response_time
---------------------
 00:00:00.133810
 00:00:00.136067
 00:00:00.134400
 00:00:00.130744
 00:00:00.129300

     SELECT (dlv.dlvr_at - mts.sent_at) AS mt_queued_to_delivered
     FROM
         brbl_logs.messages_mt mts
     INNER JOIN
         brbl_logs.messages_mt_dlvr dlv
     ON mts.id = dlv.id
     ;

 mt_queued_to_delivered
------------------------
 00:00:00.166_193
 00:00:00.138_192
 00:00:00.136_617
 00:00:00.131_253
 00:00:00.133_792


===== 2025-05-07 ====

Working on the models for Users, Profiles.

See dev.ddl.

How to encode languages and indicate their order of preference?

Just a string of the two character language codes separated by commas? e.g. "ES,EN" or "FR" where list order indicates preference. We wouldn't really need commas given that language code.

The case for using ISO-3 codes for language: 'zh' is the two character code for Chinese but there are two distinct such languages: Cantonese (yue) and Mandarin (cmn).

Changed our existing language support to use the three character codes.

Debating where to place the language and country code. We need the former when starting new conversations. The latter seems less critical but if scripts reference the user's nickname (for general friendliness) then we'd end up making a 1:1 join with the profile table. I guess language seems more like a cross-cutting concern where all the User records for a given user would want to present the same.
On the other hand, if the user is anonymous (no profile) we still need to know what language to use with them. Likewise, to know what legal rules we have to follow we need their country code. So, alas, we need both in the User record.

It's unclear whether we need/benefit from a Session table.


===== 2025-05-12 ====

Got the User/Profile models working. When running the regular docker compose setup with PlatformGateway sending a batch of messages through I realized that the database connections needed read/write access to both the brbl_logs and brbl_users schemas.

Restructured the roles for our two schemas so that we separate privilege definitions from the "users" (really just roles login) that possess them.

===== 2025-05-15 ====

CREATE SCHEMA IF NOT EXISTS brbl_logic AUTHORIZATION brbl_admin ;

CREATE TABLE brbl_logic.nodes (
    id          UUID PRIMARY KEY,
    created_at  TIMESTAMP WITH TIME ZONE NOT NULL,
    text        VARCHAR(255),       --> SMS is limited to 160 chars but other platform have higher limits.
    type        SMALLINT NOT NULL,  --> see ScriptType enum for meaning.
    label       VARCHAR(32)         --> the name given to the node element in a UI
);

CREATE TABLE brbl_logic.edges (
    id              UUID PRIMARY KEY,
    created_at      TIMESTAMP WITH TIME ZONE NOT NULL,
    match_text      VARCHAR(128),       --> the text that must be matched to direct the conversation to the dst node
    response_text   VARCHAR(255),       --> the text emitted when the edge is selected.
    src             UUID NOT NULL,      --> FK to scripts table
    dst             UUID NOT NULL      --> FK to scripts table
    -- CONSTRAINT fk_script_src
    --     FOREIGN KEY(id) REFERENCES brbl_logic.nodes(id),
    -- CONSTRAINT fk_script_dst
    --     FOREIGN KEY(id) REFERENCES brbl_logic.nodes(id)
);

-- Script 1
INSERT INTO brbl_logic.nodes VALUES('89eddcb8-7fe5-4cd1-b18b-78858f0789fb', NOW(),
    'What is your favorite color? 1) red 2) blue 3) flort', 4, 'ColorQuiz') RETURNING *;
-- Script 2
INSERT INTO brbl_logic.nodes VALUES('2ed4ceed-a229-4e82-ab89-668a15835058', NOW(),
    'Oops, that is not one of the options. Try again with one of the listed numbers or say "change topic" to start talking about something else.', 5, 'EvaluateColorAnswer') RETURNING *;
-- Script 3
INSERT INTO brbl_logic.nodes VALUES('f9420f0c-81ca-4f9a-b1d4-7e25fd280399', NOW(),
    'That is all. Bye.', 1, 'EndOfConversation') RETURNING *;

INSERT INTO brbl_logic.edges VALUES('d9d9d89b-3047-4b18-8c97-5fb870fc1ced', NOW(), '1|red', 'Red is the color of life.',
    '89eddcb8-7fe5-4cd1-b18b-78858f0789fb', 'f9420f0c-81ca-4f9a-b1d4-7e25fd280399') RETURNING *;
INSERT INTO brbl_logic.edges VALUES('fee09a2a-5595-43a9-8228-72182789800e', NOW(), '2|blue', 'Blue is my fave, as well.',
    '89eddcb8-7fe5-4cd1-b18b-78858f0789fb', 'f9420f0c-81ca-4f9a-b1d4-7e25fd280399') RETURNING *;
INSERT INTO brbl_logic.edges VALUES('49a0e06a-fff6-4bbc-91f7-fcda4b800cc4', NOW(), '3|flort', 'Flort is for the cool kids.',
    '89eddcb8-7fe5-4cd1-b18b-78858f0789fb', 'f9420f0c-81ca-4f9a-b1d4-7e25fd280399') RETURNING *;

NOTE: gen_random_uuid() is a useful function available in psql.

Next question: how to escape single quotation marks? Likely JDBC already handles this but useful to know when using psql.


===== 2025-05-16 ====

Found an ad for Twilio while reading stackoverflow. Led me down the rabbit hole of SMS providers.
Some leads on low-cost SMS here: https://www.reddit.com/r/rails/comments/175e0fk/which_provider_for_sending_sms_messages_is_the/
Remember to learn more about the differences between GSM-7 and UCS-2 character sets since we'd like to support non-English languages.
Climbing out before it drives me mad...

Work on node-related schema moved to dev.ddl.

The design seems plausible for basic interactions. Will it be open enough to support other things? What are those other things?

Useful queries:
    SELECT id,created_at,type,label,SUBSTR(text,0,40) FROM brbl_logic.nodes;
    SELECT created_at, match_text,response_text, src, dst FROM brbl_logic.edges ORDER BY src;

Keyword mapping:
To find the initial node node for a new session...

CREATE TABLE brbl_logic.keywords (
    id          UUID NOT NULL UNIQUE,
    pattern     VARCHAR(128),   --> the default entry likely has no value so this can't be NOT NULL
    platform    platform,
    script_id   UUID,
    is_default  BOOLEAN DEFAULT FALSE,
    CONSTRAINT fk_script_id
        FOREIGN KEY(id)
            REFERENCES brbl_logic.nodes(id),
    CONSTRAINT unique_pattern_platform
        UNIQUE(pattern, platform)_
);

https://www.postgresql.org/docs/current/sql-createtable.html has some interesting notes about column storage and compression options which might be useful if node text gets large.

===== 2025-05-16 ====

Reading https://stackoverflow.com/questions/54907495/postgresql-recursive-parent-child-query led me to writing the following:

WITH RECURSIVE c AS (
    SELECT <SCRIPT_TABLE_ID>::UUID AS script_id
    UNION ALL
    SELECT e.dst
    FROM brbl_logic.edges AS e
        JOIN c ON (c.script_id = e.src)
)
SELECT
    s.id, s.label, s.text,
    e.match_text, e.response_text, e.dst
FROM
    brbl_logic.nodes s
INNER JOIN
    brbl_logic.edges e ON s.id = e.src
WHERE
    s.id IN (SELECT DISTINCT(c.script_id) FROM c);

The use of the recursive "common table expression" to pull all the data we're looking for recursively starting with the id of the Script, <SCRIPT_TABLE_ID>
 The Stackoverflow post also suggests a stored procedure that does pretty much the same thing --> https://stackoverflow.com/a/54909559/3524850

http://explain.dalibo.com offers an interesting Postgres EXPLAIN plan analyzer. Might be useful for performance tuning.

The "only" trouble with our recursive query (above) is that the terminal node node doesn't get included (because there are no edges that reference it as the src. The code written to turn the ResultSet into a graph can patch up the temporarily missing references but not that one.

Do we want to support terminating scripts? I think so. The query results won't include the final node because it doesn't have any edges.

To address that problem maybe define a single edge that itself has a null dst? This works to include the final node in the result. But this makes me think...

To support cycles in the conversational graph--to allow loops--we need a different solution unless we can do it programmatically with the same data. The problem is that cycles will blow up the (now endless) recursive CTE graph query (taking the database along with it.)


===== 2025-05-23 ====

Still thinking about a way to support cycles in the conversational graph that will still provide queryable

Reading https://www.postgresql.org/docs/current/queries-with.html#QUERIES-WITH-CYCLE presents a possible solution using the built-in CYCLE syntax:

Example from the Postgres documentation:

WITH RECURSIVE search_graph(id, link, data, depth) AS (
    SELECT g.id, g.link, g.data, 1
    FROM graph g
  UNION ALL
    SELECT g.id, g.link, g.data, sg.depth + 1
    FROM graph g, search_graph sg
    WHERE g.id = sg.link
) CYCLE id SET is_cycle USING path
SELECT * FROM search_graph;

Let's try:

WITH RECURSIVE cte AS (
        SELECT '89eddcb8-7fe5-4cd1-b18b-78858f0789fb'::UUID AS script_id
        UNION ALL
        SELECT e.dst
        FROM brbl_logic.edges AS e
            JOIN cte ON (cte.script_id = e.src)
    ) CYCLE script_id SET is_cycle USING path
    SELECT
        s.id, s.created_at, s.text, s.type, s.label,
        e.id, e.created_at, e.match_text, e.response_text, e.src, e.dst
    FROM
        brbl_logic.nodes s
    INNER JOIN
        brbl_logic.edges e ON s.id = e.src
    WHERE
        s.id IN (SELECT DISTINCT(cte.script_id) FROM cte);

This executes without complaint. Let's add a edge that creates a cycle and see if it does what we want...

UPDATE brbl_logic.edges e
    SET dst = '89eddcb8-7fe5-4cd1-b18b-78858f0789fb'
    WHERE e.id = '4bce9e23-2dc4-42d9-aea9-94744a74d005';

which changes the final edge in the graph to point at the first node.
Re-running the recursive, cycle-detecting CTE produces a beautiful bounded result set. Just what we wanted!

It does! Postgres rocks!

===== 2025-05-24 ====

Interesting linear time performance regex lib for Java:

    https://github.com/google/re2j  <--- Only 35k .jar file


Interesting realization about the recursive query and our current data set:
Do a query for any of the node id's and you get all the scripts in the results. Since it's a cycle this is actually correct but, for a moment, I thought the query was wrong (because changing the script_id parameter didn't alter the results.) Doh!

Let's create a node chain that is separate and doesn't cycle:

    --> starting and ending scripts
    INSERT INTO brbl_logic.nodes VALUES('525028ae-0a33-4c80-a22f-868f77bb9531', NOW(),
        'True or false: people are the worst?', 4, 'BadPeople') RETURNING *;
    INSERT INTO brbl_logic.nodes VALUES('cf72ce06-50fc-4bf1-852b-dbdbd9f97f66', NOW(),
        'There is nothing left to say.', 4, 'AnotherEndOfConversation') RETURNING *;

    --> edges connecting them
    INSERT INTO brbl_logic.edges VALUES('2361468c-571d-43e1-a5cc-a5580841253c', NOW(), 'yes|true', 'Sadly, you are correct.',
        '525028ae-0a33-4c80-a22f-868f77bb9531', 'cf72ce06-50fc-4bf1-852b-dbdbd9f97f66') RETURNING *;
    INSERT INTO brbl_logic.edges VALUES('f6c08bf6-a984-4619-97be-3bb2526ba81d', NOW(), 'no|false', 'You are a hopeless optimist.',
        '525028ae-0a33-4c80-a22f-868f77bb9531', 'cf72ce06-50fc-4bf1-852b-dbdbd9f97f66') RETURNING *;
    --> terminating edge (with NULL destination)
    INSERT INTO brbl_logic.edges VALUES('053c7425-dfd9-4c32-b305-a15b50453274', NOW(), 'NOOP', 'NOOP',
        'cf72ce06-50fc-4bf1-852b-dbdbd9f97f66', NULL) RETURNING *;

Verified. The recursive node query only returns elements that are linked.
Asking for the graph starting with the BadPeople element yields:
                 id                   |    label    |                 src                  | match_text |                 dst
--------------------------------------+-------------+--------------------------------------+------------+-------------------------------------
 525028ae-0a33-4c80-a22f-868f77bb9531 | BadPeople   | 525028ae-0a33-4c80-a22f-868f77bb9531 | no|false   | cf72ce06-50fc-4bf1-852b-dbdbd9f97f66
 525028ae-0a33-4c80-a22f-868f77bb9531 | BadPeople   | 525028ae-0a33-4c80-a22f-868f77bb9531 | yes|true   | cf72ce06-50fc-4bf1-852b-dbdbd9f97f66
 cf72ce06-50fc-4bf1-852b-dbdbd9f97f66 | AnotherEn.. | cf72ce06-50fc-4bf1-852b-dbdbd9f97f66 | NOOP       |

 Perfect.


===== 2025-05-26 ====

Got the keywords table/query logic working. This won't be a simple, fast cache using Caffeine since we want/need to support
some amount of regular expressions. Explored embedding the keyword query inside the recursive node/edge query but dropped
it because it would require doing the pattern match in sql (not necessarily bad but possibly unwieldy) and it was hard to determine
from the result set where the graph "started."



===== 2025-05-28 ====

Trying out the combined keyword/node/edge logic in a full running system...

Using PlatformGateway + curl scripts make evaluating this difficult.

Let's bust out the BrblCli hack to see if it helps...

    java -cp target/sndrRcvr-1.0-SNAPSHOT-jar-with-dependencies.jar com.enoughisasgoodasafeast.cli.BrblCli -l

Actually working reasonably well although...
It shows an issue where the penultimate message in the node graph doesn't send the edge response before moving to the
end node. i.e. sending 'Iggy' as our choice of best third Stooge doesn't trigger the "Oops, wrong Stooges!" message.
Instead it just replies with the EchoWithPrefix response "That is all. Bye.: Iggy"

The current data in the scripts and edges table *looks* correct so maybe the Multi.Process logic preceded the node/edge model...

===== 2025-05-29 ====

ALTER ROLE <your_login_role> SET search_path TO a,b,c;

Idea: Leverage the evaluatedScripts list in the Multi.Process.evaluate() method to access the previous Script (the Multi.Present) to reference ResponseLogic elements to process the current input and route the conversation to the next Script.

Implemented a printGraph function for the Script class to better visualize the graph we're producing.
The terminal node node for each branch ends up with three redundant ResponseLogic elements (the 'NOOP' edges.)
This could be a problem with the query or the way we're assembling the graph from the query results.
Otherwise, the structure looks like what we intended.

Which means the behavior we see walking through it via the BrblCli is a problem with the node processing.

Realized that the Multi.Present -> Process combo is supposed to have the former pointing (only) at the latter.
Also that its the latter that contains the references to the choices. So we need to fix the data in brbl_logic.nodes/edges.

===== 2025-05-30 ====

Fixing the data...

TABLE brbl_logic.nodes:
 id         | uuid                     |           | not null |
 created_at | timestamp with time zone |           | not null |
 text       | character varying(255)   |           |          |
 type       | smallint                 |           | not null |
 label      | character varying(32)    |           |

=== Create the new Multi.Process scripts
insert into brbl_logic.nodes values(gen_random_uuid(), now(), 'Error message for ColorQuizProcess', 5, 'ColorQuizProcess') returning id;          --> 95441b9a-3636-4f45-bd0a-ec35e84dd5f7
insert into brbl_logic.nodes values(gen_random_uuid(), now(), 'Error message for StoogeQuiz_Process', 5, 'StoogeQuiz_Process') returning id;          --> 3f467ee3-4874-4ad1-82b1-99be06f575ab
insert into brbl_logic.nodes values(gen_random_uuid(), now(), 'Error message for UtensilQuiz_Process', 5, 'UtensilQuiz_Process') returning id;            --> 1bda7846-31b4-4983-92ec-161eba46c758
insert into brbl_logic.nodes values(gen_random_uuid(), now(), 'Error message for ShapeQuiz_Process', 5, 'ShapeQuiz_Process') returning id;            --> 23f00a69-55b7-4c29-b777-5c278b7088ed
insert into brbl_logic.nodes values(gen_random_uuid(), now(), 'Error message for BadPeople_Process', 5, 'BadPeople_Process') returning id;            --> ae28c0e3-9303-4807-979f-694bc9981dd7
insert into brbl_logic.nodes values(gen_random_uuid(), now(), 'Error message for AnotherEndOfConversation_Process', 5, 'AnotherEndOfConversation_Process') returning id;          --> aec8789f-546f-42c2-b1d5-c80bd10014f0

=== Point the existing edges to the new scripts:
UPDATE brbl_logic.edges set src = '95441b9a-3636-4f45-bd0a-ec35e84dd5f7' where src = '89eddcb8-7fe5-4cd1-b18b-78858f0789fb' ;
UPDATE brbl_logic.edges set src = '3f467ee3-4874-4ad1-82b1-99be06f575ab' where src = '0b2861b6-a16a-4197-910a-158610967dd9' ;
UPDATE brbl_logic.edges set src = '1bda7846-31b4-4983-92ec-161eba46c758' where src = '385a1f99-d844-42e6-9fa3-a0e3a116757d' ;
UPDATE brbl_logic.edges set src = '23f00a69-55b7-4c29-b777-5c278b7088ed' where src = 'b48d36ce-2512-4ee0-a9b9-b743d72e95e9' ;
UPDATE brbl_logic.edges set src = 'ae28c0e3-9303-4807-979f-694bc9981dd7' where src = '525028ae-0a33-4c80-a22f-868f77bb9531' ;
UPDATE brbl_logic.edges set src = 'aec8789f-546f-42c2-b1d5-c80bd10014f0' where src = 'cf72ce06-50fc-4bf1-852b-dbdbd9f97f66' ;

=== Create new edges linking the original Multi.Present scripts (type 4) to the new Process scripts (type 5):
id            | uuid                     |           | not null |
created_at    | timestamp with time zone |           | not null |
match_text    | character varying(128)   |           |          |
response_text | character varying(255)   |           |          |
src           | uuid                     |           | not null |
dst           | uuid                     |           |          |

INSERT INTO brbl_logic.edges VALUES (gen_random_uuid(),now(),'n/a','n/a', '89eddcb8-7fe5-4cd1-b18b-78858f0789fb','95441b9a-3636-4f45-bd0a-ec35e84dd5f7') returning id ;
INSERT INTO brbl_logic.edges VALUES (gen_random_uuid(),now(),'n/a','n/a', '0b2861b6-a16a-4197-910a-158610967dd9','3f467ee3-4874-4ad1-82b1-99be06f575ab') returning id ;
INSERT INTO brbl_logic.edges VALUES (gen_random_uuid(),now(),'n/a','n/a', '385a1f99-d844-42e6-9fa3-a0e3a116757d','1bda7846-31b4-4983-92ec-161eba46c758') returning id ;
INSERT INTO brbl_logic.edges VALUES (gen_random_uuid(),now(),'n/a','n/a', 'b48d36ce-2512-4ee0-a9b9-b743d72e95e9','23f00a69-55b7-4c29-b777-5c278b7088ed') returning id ;
INSERT INTO brbl_logic.edges VALUES (gen_random_uuid(),now(),'n/a','n/a', '525028ae-0a33-4c80-a22f-868f77bb9531','ae28c0e3-9303-4807-979f-694bc9981dd7') returning id ;
INSERT INTO brbl_logic.edges VALUES (gen_random_uuid(),now(),'n/a','n/a', 'cf72ce06-50fc-4bf1-852b-dbdbd9f97f66','aec8789f-546f-42c2-b1d5-c80bd10014f0') returning id ;

New edge ids:
65078637-9862-4682-8bbe-0a52ed29bf6b
ed53f6d3-3e66-4321-9ba7-ad3c36e79eb4
31ec69f2-c618-492a-aaa7-68bb6a4475f5
77ff5c56-ac17-4410-8cf9-439b34e7c587
83408cce-85a9-4824-b622-651c6839f399
4aaf8877-f0dc-4182-94f9-86f05e87de3a

Hmm, what is the "current" node at the end of the chain?


===== 2025-06-01 ====

Scripts are now chaining correctly albeit with some ugly code that I'll have to fix up.

The ScriptType enum currently defines the following types:
*   EchoWithPrefix(1),
    ReverseText(2),
    HelloGoodbye(3),
*   PresentMulti(4),
*   ProcessMulti(5),
    Pivot(6),
    TopicSelection(7);

Of these, only the first, fourth and fifth are being used and can be said to be useful.
#2 and #3 were only for early stage development testing.
#6 and #7 are not implemented.

Let's clear the deck and only consider the *Multi types to be in play.
What additional scenarios do we want/need to support in addition to these two?

A) A notification. Just a one and done message (possibly split into multiple text messages.) This could include instructions on how to start new conversations. No response expected. It chains to an EndOfChat node.
B) Ask a question that invites a free text input and reply with something canned e.g. "Thanks for the feedback" **
C) Same thing but apply sentiment or other textual analysis to the input and, based on the result, select the response sent back. For example, if the response was negative reply with "Uh oh. We're sorry you weren't happy with X..." The selection of next node would be determined by this evaluation, meaning that we require a bounded set of possibilities. In practice, this only requires that there be a default response. The sentiment analysis done by Vader and NLTK both return a value within a bounded range (-1..1 or some variation.)
D) The TopicSelection type is meant to provide an entry point if the user doesn't arrive by keyword (or they continue after completing their original node.) This function could be implemented with either Present/ProcessMulti or the free form option.
E) The Pivot is meant to be used when the user wants to break out of the current node either by using the "change topic" input or by repeatedly failing input validation. "If you want to talk about something else (enter "1" or continue with the current chat (enter "2")" This would chain the 1 response to a TopicSelection node and the 2 response back to the previous node requiring input (found using the Session.evaluated stack.) The implementation of a Pivot could be done simply with a *.Multi pair. The big difference is that Pivot isn't part of the graph. They are injected dynamically to deal with user requested breaks in the flow. Q: How do we reference the Pivot that should be used for a given graph? By Customer?
F) EndOfChat is a terminal node. It is used to avoid overloading null with the same meaning. Null should only indicate an error. An existing  Session where the currentNode is EndOfChat would trigger a call to the same findStartingScript method used when creating a new Session.

** We will have the raw text of the MO recorded in the brbl_logs.message_mo table but it might be useful to have a dedicated "user_poll" table that contains the evaluated response. For example, if the question was "Did you like our cupcakes?" and the MO response was "yup" then the user_poll table might be set with a value from a fixed set (e.g. YES, NO, UNKNOWN) presumably using some kind of sentiment or textual analysis.




===== 2025-06-04 ====

pysentimiento is the name of a sentiment analysis library trained on Spanish. NLTK-Vader is English only.

---

Some proposed rules for interaction between Script, Session and Operator.

Scripts evaluate the Session's current Script and return the Script that should become the new current on the basis of that evaluation.
    -> Session.getCurrentScript()
Scripts may queue MTs
    -> Session.registerOutput(Message)
Scripts may, e.g. Pivots, may access the stack of previously evaluated Scripts.
    -> Session.getEvaluatedScripts()

Operator may retrieve the Session's currentNode.
    -> Session.getCurrentScript()
Operator calls a Script's evaluate method.
Operator may update the Session's currentNode
    -> Session.setCurrentScript(Script)
Operator calls Session.flush().

Defined ScriptContext to constrain the functionality needed by node functions.
Using this class in the various node functions changes help mitigate the problem, I think.

Testing with the brblcli, changes don't appear to have caused any regressions.

Note: discovered an issue where we get into an unusable state if the keyword->node lookup doesn't find anything. Need to setup a default node to address this.


===== 2025-06-05 ====

Watching a video about the Java 25 release...
Ahead of Time Method Profiling is supposed to make programs running in standard JVM (i.e. not GraalVM) warm up much faster.
    (train) -XX:AOTMode ...
    then apply the profiling info...

Project Lilliput has been working on making object headers more compact. Looks easy to try out:
java -XX:+UseCompactObjectHeaders

Cool stuff!



INSERT INTO brbl_logic.nodes VALUES('abfd6e3c-b71d-445d-87fd-89744e66e5d1', now(), 'Thanks for dropping by our bakery!', 9, 'ThanksForVisit') RETURNING id;
INSERT INTO brbl_logic.nodes VALUES('35ab8b42-fbdf-47e1-ac62-bae86c3c7178', now(), 'How did you like our new matcha cupcake?', 7, 'AskCupcakePoll') RETURNING id;
INSERT INTO brbl_logic.nodes VALUES('23b09d2a-18f7-46ac-bff2-3968bd3a4dbe', now(), 'n/a', 8, 'ProcessCupcakePoll') RETURNING id;
INSERT INTO brbl_logic.nodes VALUES('669eea27-2dc1-4dff-bd0f-bc7d029bf714'', now(), 'Thanks for letting us know! We hope to see you again soon <3', 6, 'CyaLater') RETURNING id;
--link thanks to input request
INSERT INTO brbl_logic.edges VALUES ('abe3e848-8d89-4949-b881-0017f10c10cb',now(),'n/a','n/a',
        'abfd6e3c-b71d-445d-87fd-89744e66e5d1','35ab8b42-fbdf-47e1-ac62-bae86c3c7178') returning id ;

--link input request to process response
INSERT INTO brbl_logic.edges VALUES ('c6e395ed-eb0e-4954-bf51-da80242da49a', now(), 'n/a', 'n/a',
        '35ab8b42-fbdf-47e1-ac62-bae86c3c7178','23b09d2a-18f7-46ac-bff2-3968bd3a4dbe') returning id ;

--link process response to endOfChat
INSERT INTO brbl_logic.edges VALUES (gen_random_uuid(),now(),'n/a','n/a',
        '23b09d2a-18f7-46ac-bff2-3968bd3a4dbe','669eea27-2dc1-4dff-bd0f-bc7d029bf714') returning id ;

--finally link to null required to have the node node included in the graph
INSERT INTO brbl_logic.edges VALUES ('5e57ede3-0f45-4fc5-b7cb-80de35c94f71',now(),'n/a','n/a',
        '669eea27-2dc1-4dff-bd0f-bc7d029bf714', null) returning id ;


With these additional rows added to the database, I'm able to test and confirm that the new message types (SendMessage, RequestInput, ProcessInput and EndOfChat) work as expected. Cool cool cool...



===== 2025-06-08 ====

https://vladmihalcea.com/hibernate-with-recursive-query/ has some interesting info about using recursive CTEs with Hibernate.
Hopefully still compatible with the newly released Hibernate 7.

Started a new project/repo: BrblEdit.

To reference the sndrRcvr code (which we should rename to just Brbl as is reflected on GitHub) its simple enough to run:

    mvn install -DskipTests=true

This generates and installs the jar and .pom file in my local ~/.m2 folder.

select * from brbl_logs.messages_mo mos
inner join brbl_logs.messages_mo_prcd prcd on mos.id=prcd.id
where rcvd_at >= '2025-06-09 17:00:00';


===== 2025-06-09 ====
BrblEdit progress...
Got the basic domain models for the record-based models used in our runtime working in Hibernate.
Note, I'm using the direct Hibernate interfaces/classes rather than the Jakarta Persistence API.

===== 2025-06-10 ====

In testing the script graphs produced by BrblEdit I once again realized how brittle the Operator is to exploding when a session attempts to continue talking after the end of the graph is reached. The resulting NPE will put it into a state in which newly arriving messages are not processed. (Rcvr puts them on the queue but Operator won't get called because the BrblConsumer has shutdown:
    operator-1  | java.lang.NullPointerException: Cannot invoke "com.enoughisasgoodasafeast.operator.Node.evaluate(com.enoughisasgoodasafeast.operator.ScriptContext, com.enoughisasgoodasafeast.Message)" because "session.currentNode" is null
    operator-1  |   at com.enoughisasgoodasafeast.operator.Operator.process(Operator.java:143)
    operator-1  |   at com.enoughisasgoodasafeast.operator.Operator.process(Operator.java:112)
    operator-1  |   at com.enoughisasgoodasafeast.OperatorConsumer.handleDelivery(OperatorConsumer.java:41)
    operator-1  |   at com.rabbitmq.client.impl.ConsumerDispatcher$5.run(ConsumerDispatcher.java:149)
    operator-1  |   at com.rabbitmq.client.impl.ConsumerWorkService$WorkPoolRunnable.run(ConsumerWorkService.java:111)
    operator-1  |   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    operator-1  |   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    operator-1  |   at java.base/java.lang.Thread.run(Unknown Source)
    operator-1  | 2025-06-11 09:47:50,000 WARN  [] [pool-1-thread-10] c.e.BrblConsumer - handleShutdownSignal called with consumerTag amq.ctag-uXoJ6yAukegn-3FMeh5NFQ and exception: com.rabbitmq.client.ShutdownSignalException: clean channel shutdown; protocol method: #method<channel.close>(reply-code=200, reply-text=Closed due to exception from Consumer (amq.ctag-uXoJ6yAukegn-3FMeh5NFQ) method handleDelivery for channel AMQChannel(amqp://guest@192.168.1.155:5672/,1), class-id=0, method-id=0)

The same problem exists if the keyword sent doesn't find a match: NPEs and death of the queue consumer.


===== 2025-06-14 ====

While trying to figure out a Hibernate equivalent to our PersistentPostgresManager.getScriptForKeyword() method of retrieval (pulling the graph of nodes and edges in a single query) I started to consider whether we could simplify the script model by merging the node and edge and using the more common single table approach. This is how the Hibernate docs and many relevant blogs assume you're doing it.

We need to support the ability to branch at any point based on the node type (e.g. Multi.Process or Input.Process.)
Initially, I was concerned about overloading the use of the match_text or response_text columns if we only had a single type of object and I wanted to be able to create cycles, reusing nodes across different scripts.

A -----> B -----> C
          \
           * --> D

X -----> Y -----> C

table Node
id       parent_id
======|==========
A     | null
B     |  A
D     |  B
X     |  null
Y     |  X

To find the branches for B we query for rows where parent_id = B
Q: How is C reached from either B or Y?
A: It cannot be reached from both if id is the primary key

The latter concern (reusing nodes/creating cycles) CANNOT be addressed with the merged model while still supporting the former (branch from a node.)
Best case we can copy the properties of C and set them on a separate node.
Is there a way to signal/hint that C and clone of C should be equivalent?
Or a way to separate the mappings from the nodes' other properties?
For example...

table NodeMappings
id    |  parent_id
===================
C     |  B
C     |  Y

table Node
id    | type  | match_text | response_text | label   | etc.
==========================================================
C     |  3    | "foo"      | "Welcome"     | "Intro" | ...

This might work but leads us back to the implementation challenge using Hibernate.

But what if we decided that scripts couldn't be cyclic? That they always have an end point and continuing past the point would trigger some special logic that interpreted the user input (in the absence of a scripted multiple choice element) and routed them to a new script graph.
It seems like we have to solve that problem anyway since we know we won't always be responding to keywords. (A simple implementation would simply provide a default script without attempting to understand the user's context.) As well, we want the user to be able to ask, at any point, "change topic," breaking out of the current script and switching to a different one.

So I guess it comes down to either keeping the Node uncluttered with special case columns that are only used for certain NodeTypes (storing them on the Edge table instead) or simplifying the code for fetching script graphs.




===== 2025-06-16 ====

Let's try a native query with a entity class that combines the Node and Edge types. Maybe it's really not that complicated.
Hmm...the native query used with simple bean that merges the fields of the two types is all but a JDBC query; it doesn't recognize any object references. If we have to use just their id (an UUID instead of a Node, for example,) then we'll have to query again for each object to resolve them. Not what we want.

===== 2025-06-17 ====

Watched an interesting video talking about using a bunch of the newest java features together. One that I hadn't recognized as being useful is sealed classes used with records and the enhanced switch. The idea that now occurs to me is to use them to replace the null case when a script has no next elements. Instead of a value or null define records that represent the two. The record class of the value and the null surrogate would both implement a marker interface. We could also use something similar to enrich the Operator methods that currently return boolean.

We should also take a look at Stream Gatherers that are introduced in JDK 24. (We should also migrate our project though there were changes to the Structured concurrency API that we may have to adapt to.)

...wrote a version of the code that uses the Gatherer API to convert the result set returned by the recursive CTE into a connected graph. Maybe not a great use case; the non-functional code is simpler.

===== 2025-06-29 ====

Mostly working on BrblEdit these days.

Some shared changes need to be done however.
_ Customer table with integration into keywords.
X Adding updatedAt columns to Node and Edge.

We'd been using a composite primary key for the USERS table combining platform_id and platform_code. We knew this was likely overkill given that the platform_id is unique (it's a phone number or WhatsApp number or our own Brbl ID.) And, it creates a problem linking to customer.

The better alternative, I think, is to link Customer to Profile. The latter's primary key references the User's group_id.


===== 2025-07-03 ====

As part of the work to support script authoring (BrblEdit) we need to clean up our organization of keywords and create a means to track scripts.

Currently keywords requires an exact match of the Message.text. It doesn't even trim the input.

If we scope keywords by the short/long code that receives the message the subset could be small enough to use regexes to signal a match.

- Exact match must support case insensitivity, of course
- With or without spaces (if multiword)
- Misspellings
- Common abbreviations
- Common homophones (possibly indistinct from the previous two)

Easy to deal with casing issues (just trim and lower case both the patterns and the input)

    String pattern = "(color|colour|colr).*(quiz|q|kwiz).*";
    Pattern compiledPattern = Pattern.compile(pattern);
    ...
    String mutatedInput = input.trim().toLowerCase();
    return compiledPattern.matcher(mutatedInput).matches();

Other thoughts: matching on homophones of the keyword. Lucene has functionality to support this but dragging all that seems like pretty massive feature creep. Using Lucene via Hibernate Search might be possible but still a big increment and I'm not sure how much flexibility is really needed.


===== 2025-07-04 ====

Realized that I'd really been conflating the keyword and script fetching functionality in my mind. How I'm thinking it should work now:

- the keyword cache population method should pull all the entries in the database.
- the keyword get operation will iterate over the subset of keywords (for the given short code.) Any shortcuts we can take?

The return value is simply the final matching id.
- the corresponding Node would then be pulled from a separate Caffeine cache populated with the graphs provided by PersistenceManager.getScript().

Some thoughts around merging keywords and the new scripts model (see NOTES.md in BrblEdit from 2025-07-01) but since keywords don't make as much sense for non-SMS platforms I think it might be better to keep them separate for now.

We do have to add the notion of short/long codes (aka 10DLC) to keywords.

Adding support for shortcodes and long codes to the keywords table. They allow an additional level of scoping.

    ALTER TABLE brbl_logic.keywords ADD COLUMN short_code VARCHAR(10);
    COMMENT ON COLUMN brbl_logic.keywords.short_code IS
        'The short (5-6 digits) or long (10 digit) code for which the keyword is scoped. Global if none is specified.';
    ALTER TABLE brbl_logic.keywords DROP CONSTRAINT unique_pattern_platform;
    ALTER TABLE brbl_logic.keywords ADD CONSTRAINT unique_platform_shortcode_pattern UNIQUE(platform, short_code, pattern);

===== 2025-07-06 ====

Also need the combined created_at and updated_at columns to the keywords table:

    ALTER TABLE brbl_logic.keywords ADD COLUMN created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW();
    ALTER TABLE brbl_logic.keywords ADD COLUMN updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW();


keyword cache: keyed by a record (KeywordCacheKey) composed of the "to" and text fields of the Message with a UUID value of the Node.

script cache: keyed by the UUID of the starting node,  values: being the Node with all its connections.

The trick here is that, we need the full set of patterns (exact matches or regexes) in hand to evaluate the incoming expressions.
It's the evaluation of the patterns against the incoming message (which we assume to be the keyword) that is cached.  Not sure this will be an actual benefit performance-wise. The value might simply be the UUID of a Node which would then need to be used to pull the connected graph from a separate cache (scriptCache.)

===== 2025-07-08 ====

Ok, for keyword matching evaluation we setup a special, single entry cache. The key is a constant string ("ALL") and the value is a <Pattern,Keyword> map containing the full content of the brbl_logic.keywords table. We evaluate the regexes in the iteration order (whatever it may be) and return the first match, logging misses loudly. The code then uses the returned UUID to pull the connected graph from the scriptCache (also a LoadingCache instance.)

Lightly tested. Seems to be working as designed.

Next up: a means of identifying other types of scripts for specific circumstances per specific customers/short codes. This includes "top level" topic conversations and handlers for when the user asks to "talk about something else."

Extend the scripts table to include another enum column that defines its purpose?
Initial values of the enum might be:
    TOPICS -> the list of subjects that the customer offers to talk about. Triggered by "change" or "change topic" or "topics."
    ASK    -> Use an Input.Request/Process element to find a topic. Requires additional processing to interpret the input.
    END    -> Short cut any conversation to get to a "termination" point. Triggered by the user typing "stop" or "end" or the like.

Add a composite constraint on the customer id plus this column to prevent duplicates?

Let's think on this for a bit but defer building it for now...

RANDOM: Read an interesting post warning against using v4 UUIDs with Postgres. v7 fixes most of the described problems but the author still recommended against using them as primary keys.

===== 2025-07-10 ====

Back to BrblEdit.


===== 2025-08-15 ====

The next version of Postgres is looking super sweet with a bunch of amazing features: async i/o (!!!), faster foreign key joins on large tables, UUID v7 (uuidv7, which includes a time component for natural ordering and better btree performance), virtual generated columns (think derived properties that are deferred to query time for space efficiency.) There's even a function for extracting the time info from the uuid (uuid_extract_timestamp(uuidv7()). The RETURNING clause for updates can return both the old and new values. This could be clutch for auditing, logging and implementing saga patterns for transactional undo. An older feature--time ranges aka TSTZRANGE--introduced back in Postgres 9.2 now supports a constraint (WITHOUT OVERLAP) which can be added as part of a composite primary key to prevent, for example, double booking a resource (the thing represented by the other part of the primary key.) This could be super useful for keyword management. Finally, version upgrades no longer require a possibly long running ANALYZE to rebuild database statistics after the upgrade. All fantastic features!

Also of possible interest despite it's dubious legality: browse.ai which claims to scape websites and turn it into structured data.

Also, on the topic of data generation, DataFaker née JavaFaker is a library modelled after Faker, the well-known Python lib for generating random-like data.


===== 2025-08-21 ====

Over in brbl-admin we have a basic customer input form working. Time to more fully flesh out the billing address parts of the Company model. It currently only has a single long column for the address. For (snail) mailing purposes this might be fine (assuming we embed the lines breaks) but what are the requirements for submitting a payment request to a credit card processor? We should probably also figure out how to handle changes to billing.

Thinking about billing more broadly...
ACH via Paypal. D. says this is cheap, at least for non-profits. Recurring payments are possible.
See https://docs.stripe.com/payments/ach-direct-debit/accept-a-payment for description of Stripe's API.

3-6% fee with credit card.

It may not make sense for us to build our own billing system at all. Stripe and the like have hosted pages that can be integrated. What do we give up business wise? Knowledge of our customers...

re: the relational structure of a customer, found a plausible schema here: https://dbfiddle.uk/V9ohQXLE
Referenced in https://dba.stackexchange.com/questions/111101/storing-a-billing-address-best-practice-in-orders-table

Separates the Address into it's own table and links it to the Customer via a CustomerAddress table (which includes a flag to denote the type of address--billing or shipping--that applies.)
It also defines two auditing tables, for the Address and CustomerAddress pairing, to handle changes over time.

It also has an Order table that we might revamp to represent a unit of service (e.g. a month's worth of the Brbl service for which we might keep a message count.)

So far we have three schemas: brbl_logs, brbl_logic, and brbl_users. For the customer data, creating a fourth schema: brbl_biz.


===== 2025-08-28 ====

Lost the terminal that was running Postgres. The server continues to run but I can't see the log messages.
Apparently, we need to turn on the logging_collector in the postgresql.conf file in /opt/homebrew/var/postgresql@17.
It is currently not running:
    #logging_collector = off

===== 2025-10-22 ====
Starting design for storing sessions persistently:

    CREATE TABLE brbl_logic.sessions (
        group_id                UUID PRIMARY KEY,
        data                    BYTEA NOT NULL,
        created_at              TIMESTAMP WITH TIME ZONE NOT NULL,
        updated_at              TIMESTAMP WITH TIME ZONE NOT NULL
    );
    COMMENT ON COLUMN brbl_logic.sessions.group_id IS 'Effectively (but not actually) the foreign key to brbl_logic.users.';
    COMMENT ON COLUMN brbl_logic.sessions.data     IS 'The latest serialized session data for the referenced user.';

Implementation uses Postgres' INSERT INTO ... ON CONFLICT syntax. Verified correct update behavior:

    brbl_db_dev=> select group_id, created_at, updated_at, (updated_at - created_at) as delta from sessions;
                   group_id               |          created_at           |          updated_at           |      delta
    --------------------------------------+-------------------------------+-------------------------------+-----------------
     2c9683b8-3118-4b73-a92c-d7f0d38bc24e | 2025-10-22 12:17:02.553688-04 | 2025-10-22 12:18:18.043024-04 | 00:01:15.489336
     8d8e3123-b208-497c-b51f-18d517362433 | 2025-10-22 09:51:07.23876-04  | 2025-10-22 12:19:09.070032-04 | 02:28:01.831272

Issues:
The messages_mt table has a session_id column though it predates our persisting session here.
Previously, our notion of sessions were like web sessions. With the link to the group_id, however, sessions could be effectively eternal.
In some ways this is an improvement. But how do we keep separate the conversations with different Customers?
Along the same line, should we limit a Customer's interaction with a single User to a single context at a time?

===== 2025-11-04 ====

Took a fresh look at the way we're handling the currentNode of the Session. It's not as bad I recall.
Cleaned up what needed cleaning. Decided to leave the use of null to terminate the conversation graph for now. Using Optionals doesn't solve the problem when something is null.

Turning our attention to how we initiate conversations:
    We were assuming keywords were needed because short codes would often be shared. However, short codes hardly exist anymore in the wonderful world of SMS so, while they could still be useful, they are not as common.
    We also want to start implementing push messaging so customers can start conversations. This is likely the most important feature.
        This needs to be done without necessarily pushing the session context into the cache. Instead we want to serialize the Session state to brbl_logic.sessions. This acknowledges that a lot of push messages don't get responses so the pressure it puts on the session cache--pushing out active conversations--is a bad bet.


TODO
X tests for the keyword/script cache

===== 2025-11-22 ====

Graphing the MO call flow of the Operator. Courtesy of Gemini.

   * Operator.process(Message)
       * SessionKey.newSessionKey(Message) constructs a SessionKey derived from the Message.
       * Use the returned SessionKey to fetch the Session from sessionCache...
       * Operator.createSession(SessionKey) (if session is not in sessionCache)
           * Operator.findOrCreateUser(SessionKey) (if user is not in userCache)
               * PersistenceManager.getUser(SessionKey) queries the database
               * User.<init>(...) if the User is not found in database
               * PersistenceManager.insertUser(User) adds the new User to the database and userCache.
           * KeywordCacheKey.newKey(SessionKey) constructs a KeywordCacheKey from the SessionKey
           * Operator.findScriptForKeywordShortCode(KeywordCacheKey) (if script is not cached)
               * Operator.loadAllKeywords(String) (if keywords are not cached)
                   * PersistenceManager.getKeywords()
               * Operator.findMatch(Map, KeywordCacheKey) figure out if the Message text matches one of the keywords
               * Operator.getScript(UUID)
                   * PersistenceManager.getScript(UUID) (if script is not cached)
               * Operator.loadDefaultScriptsByChannelCache(String) (if no keyword match and default script is not cached)
                   * Operator.defaultScript(Platform, String)
           * Session.<init>(...) the new Session is now added to the sessionCache and passed to...
       * Operator.process(Session, Message)
           * Session.registerInput(Message) adds the Message to the Session's set of inputs.
           * Session.previousInput() return previously processed Message and compares it to the current message, logging error if the receivedTime ordering is not correct.
           * Operator.evaluate(Node, ScriptContext, Message)
               * This calls a specific evaluate method based on the node type, most commonly:
               * Multi$Present.evaluate(ScriptContext, Message)
               * Input$Process.evaluate(ScriptContext, Message)
               * SendMessage.evaluate(ScriptContext, Message)
           * Session.registerEvaluated(Node) appends to Sessions's evaluatedNodes list.
           * The call to Operator.evaluate is repeated in a loop until a node requires user input.
           * Session.flush() sends all buffered output, logs activity and clears the buffers.


===== 2025-11-23 ====

Hmm, the current keywords table has some properties I'd forgotten:
    - "short_code" is used for what we're trying now to call "channel"
    - It doesn't reference the customer table but _does_ include a boolean is_default. Selecting the right row doesn't require knowledge of the customer, only the channel. Still it leaves a gap referentially. We should probably add that.
    - The is_default column isn't currently being included in the Keyword records returned from the query. The idea was to reuse the result set to find the default for the channel when there are no pattern matches found. Problem: there's no way to allow only a single row to have is_default be true.

    We could separate the default script from keywords entirely.


===== 2025-11-28 ====

Using roughly the same structure as the Gemini outline. Here's what I'm thinking might be appropriate for push messages:

* Operator.process(User, Keyword)
    * Operator.getSession(User.id) from sessionCache.
        [There may be circumstances where we abort here to avoid interrupting an active session.]
        [Using a SessionKey for the cache would be nice for consistency but not sure how to handle the keyword part. Generating a complex generated string could work]
        * Operator.getOrCreateSession(SessionKey) (if session is not in sessionCache). DO NOT add this session to sessionCache!
            * Operator.getUser(User.id) (if user is not in userCache)
                * PersistenceManager.getUser(User.id) queries the database. User must exist.
            * Operator.getScript(UUID)
                * PersistenceManager.getScript(UUID) (if script is not cached)
        * Operator.evaluatePush(Node, ScriptContext)
            * The starting push message will need to be generated at this point. Possibly we send a few messages before awaiting User response.
        * Operator.registerOutput(Message)
        * Session.flush()
* PersistenceManager.upsertSession(Session)

There are two ways we create Users.
1) they messaged in to us using information they acquired on their own (ads, personal interactions, etc.)
2) they checked a box on a Customer form somewhere (online or off) that is being tracked somehow for verification. If the User sends a stop message without any other interaction we might consider this a signal to drop them completely. A "hard drop."

TODO
_ complete the keywords and default scripts implementation including the table design.


===== 2025-12-03 ====

Worked out the design for default scripts. Completely different result:
brbl_logic.routes tracks all the registered combinations of platform and channel (formerly shortcodes), the customer that "owns" them, and the default
script (node.) Since channels are assigned through an application process by the platform holders we also need a status column to track their progress (and egress.)

The Keywords table needs to be overhauled to reflect this new scope. Keywords need only be unique per route (NB: route is the combination of platform and channel.)

Now that we have the design we gotta decide how to structure the caches.
Given the semi-static nature of a default mapping we could do as we do for keywords (reading them all in one shot) and cache just the id of the script then pull through the main script cache.
Let's try that...


===== 2025-12-04 ====

Completed work on keywords and routes, implementing default scripts and the caching thereof.

Read an interesting post-mortem of a project that ran containerized Java apps and moved from JDK8 to 17. They ran afoul of a regression in the latter version that ignored cgroup cpu limits (2 vCPUs) and read the host's native core count (96!). The JVM used the much larger number when scaling thread creation. At 2mb per thread that almost blew out the container's total memory size. Yikes. The same bug increased the number of malloc arenas created by glibc. Neither of these show up when you're looking at heap size; they're both in the native parts of the JMV. The G1GC in 17 is also known to use more native memory than previous versions.
The suggested solution is to set -XX:ActiveProcessorCount=<N> (for the thread problem, matching the number in the container config) and set export MALLOC_ARENA_MAX=<1 or 2> (for the malloc issue.)
For the garbage collector issue either tune it by adding something like this:
    -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:G1HeapRegionSize=16m
Or, ParallelGC instead. It's apparently it has the lowest native memory footprint of all the collectors.
And, to observe native memory:
    -XX:NativeMemoryTracking=summary
then use jcmd <pid> VM.native_memory summary to see the values.
The article is here: https://dzone.com/articles/jdk-memory-bloat-containers


===== 2025-12-05 ====

Implement database persistence of sessions... (This is currently stubbed out.)
Requires some thinking about how we store sessions:
    Per user globally (via the group_id) or per platform that the user has used (platform & channel?)

We've thought about this before but hadn't made any decisions. Its theoretically useful to be able to transition a session from one platform to another (less expensive one) but totally unclear how appealing this would be for users. When transitioning we can provide (via the link) additional context to determine how we handle shared session data.

What I hadn't thought about before: a user could interact with multiple routes, each owned by a different Customer. The user lookup for the second Customer route will find the common user record but shouldn't, in that case, restore the session because it belongs to the first Customer and would be confusing to the user. Same problem occurs when spanning platforms.

The current query to fetch the user will return all the rows bound to the group_id and merge them together. The various platform specific ids are included in a map. So we are aware of when a user exists for multiple routes. However, it doesn't have any knowledge of which customer brought them in/has knowledge of them. And it wouldn't make sense to the user to be recognized by a Customer who hadn't contacted them before. Nor would Customers be pleased to share their Users this way.

I think what we need to do is add a customer_id field to the User entity and include it in the primary key: platform, platform_code, customer_id.
Then reorder/rework the logic in the Operator.createSession() method; the userCache will need the route information.

This means that Profiles will need to be scoped by Customer as well. So that each Customer has their own distinct collection of User entities.

SessionKeys should continue to work fine since they already include the "to" (aka channel) field that is unique. We don't know which Customer owns it but its enough to distinguish the same user sending a message to a different route.



===== 2025-12-08 ====

While adding customer_id to the User's primary key I re-remembered that the existing query was left joining with Profiles but the created User model was not using any of the data it provides. It does bundle all the combined set of languages and build a map of the other User associated by the shared Profile. Should we include the Profile as a property of the User? Or vice versa? Or create a tuple-like record that holds them both.

We knew the object model and schema were intentionally asymmetric but the addition of the Customer id might change how we decide to manage it. The latter creates a more limited scope; the profile and the user records now all belong to the customer. I think this is more sensible than imagining a single Profile shared between multiple customers.

Added customerId and map of nickNames to User. Add Profile, too?


===== 2025-12-09 ====
TODO
X create test for PostgresPersistenceManager.getUser
X create tests for Operator.findCustomerIdByRoute and findDefaultScriptByRoute

===== 2025-12-10 ====

We're getting to the point where we really need to clean up our test data set.
One more bit to add before we do that:
    adding the optional Profile to the User record...

Presumably we will provide a script/script fragment function that allows a Customer to capture the Profile info, attach it to the user and persist it.
Also, update some of the scripts to use the optional Profile properties in their processing.

===== 2025-12-12 ====

Thinking about the way we handle push messaging.
- Scheduling & how push messaging interacts with a system like Quartz or JobRunr.
    - scenarios more complex than simply sending an MT at a specific time to a specific set of users require custom state management. So the interaction with the scheduler needs to be at a functional level rather than at a data level. No single user scheduling. State management will likely be customized by use case. Triggering different scripts to be sent sequentially (as we considered at N*v*) would be cool but is super complex.
- session handling (what to do when there's an active session for a targeted user, and generally how to "expire" a session.)
    If a User is in the midst of interacting with a Customer script (ie their session is active) we should provide the option to _not_ overwrite their session with the push messaging.
- User segmentation possibilities.
    How do we
- Notions of a campaign (is "season" a useful term for a collection of campaigns?)
    Maybe define subtypes of campaigns, starting with the simple one which is allowing a Customer to push an offer/notification (just a script) to a subset of their Users one time but at a specified date-time in the future.
    Obvious follow on work to present a simple report of the results.

Schema sketch...

create table push_campaigns (
    id              uuid,
    customer_id     uuid,       --> brbl_users.customers.id, also referenced by brbl_logic.scripts
    description     varchar(64)
    script_id       uuid,       --> brbl_logic.scripts
    created_at      timestamp,
    updated_at      timestamp,
    completed_at    timestamp   --> set when all messages are sent regardless of success
);
create table campaign_users (
    campaign_id     uuid, --> campaign.id
    user_id         uuid, --> brbl_users.users.group_id
    platform_code   public.platform, --> Caveat: different platforms have different rules which could impact
                                        the execution of the campaign. Better to create separate campaigns/segments?
    delivered       status enum (PENDING, INFLIGHT, COMPLETE) or timestamp?

);

Issues:
    If campaign_users also uses a composite key we end up storing them twice.
    Add a long int primary key to users? Or just add a long column used just for the connection to campaign_users?

Possible Solutions:
    Use just the group_id in campaign_users, dropping platform_code and moving it to campaign. The business rules for each platform are different (as mentioned above) so keeping the segment separate seems a reasonable simplification. It also reduces the amount of denormalization.

User segments won't be reusable across campaigns--because the users' opt-in state can change quickly--but the definition of the segment should be.
Segment definition stored as a constrained set of pseudo-SQL conditions that could be appended to a WHERE clause?
Examples:
    Customer wanting to send their best offer to users who had previously replied in the last N days/weeks/months to previous campaigns.
    Or only to new, unknown (no Profile) users.
    Or who had replied with a specific answer to a poll.

See dev.ddl for 12/14/25.
    How do we correlate the push campaign message and the MT log that's generated for it?
    More generally how do we report on script playback?


===== 2025-12-16 ====

Outline for scheduled push campaigns:

The scheduler
 |
 V
CampaignManager.execAndReport
    |--> read the Campaign from PUSH_CAMPAIGNS, read the specified script graph, read TargetUsers (joined with data from USERS) from associated CAMPAIGN_USERS (possibly in batches)
    |       (We might want to call scriptCache.refresh(scriptID) to insure we have the latest variant.)
    |--> for each TargetUser
    |  |--> check if user has an active session (in cache). Configurable decision: clear the session or skip this user.
    |  |--> check User status, skip if their status is not IN. If KNOWN or OUT, log this for the report.
    |  |--> populate a new instance of Session but don't add it to the cache.
    |  |--> execute the first node of the script, using the associated session, generating an MT and pushing it onto the regular, rate-limited output queue with a lower priority.
    |  |--> update the delivered column of the TargetUser record (CAMPAIGN_USERS table)
    |--> update the completed_at column of PUSH_CAMPAIGNS table:
    |--> generate the run report. Is this a table? push_campaign_report?


===== 2025-12-17 ====

TODO
X Get per-message session updating working.
X Define the SQL/JDBC method for fetching push_campaign records: PostgresPersistenceManager.getPushCampaign(uuid)
X Define the SQL/JDBC method for pulling/joining campaign_users with users: PostgresPersistenceManager.getUsersForPushCampaign(uuid)
_ Get integration tests working again.

===== 2025-12-19 ====

"They were careless people...they smashed up things and creatures and then retreated back into their money or their vast carelessness or whatever it was that kept them together, and let other people clean up the mess they had made." F. Scott Fitzgerald, The Great Gatsby


===== 2025-12-21 ====

_ Have a (re)think about user/session scopes...

Currently we're using the Session's id property (a randomly generated UUID) as the Postgres table's primary key (see loadSession()/saveSession()). This is fine when we're persisting a new session but won't work when searching the database for a given CampaignUser.

For push campaigns, as well as MOs when checking for sessions that aged out of cache, could we use a SessionKey (platform, user's platform id, and **channel**)?

What are the consequences of that choice?
    If we include the platform code we could have different sessions for each User even if the channel is the same. (A customer's WhatsApp business numbers is not likely to be the same as their SMS long code but its theoretically possible.)
    We want to support migrating a conversation from one platform to another so the platform/channel uniqueness means we'd need to copy the origin session to a new one on the destination platform/channel, overwriting anything that already exists. More work but, rare as it would likely be, not too bad. If they migrate back we'd do the same thing.

On the other hand, the table schema names the primary key, "group_id." This suggest that the session is per logical user, including all the user records that share the group_id.
For MO initiated conversations it would mean that we'd need to find the user before looking for their record in SESSIONS.

I'm leaning towards this latter decision. It reinforces the notion of a single Customer <--> User relationship. It would also show off the platform migration feature more clearly.

Speaking of platform migration, how do we handle the case where we've created two unconnected Users, then discover they are, in fact, the same user and want to connect them? How do we preserve their history?
    --> Users, Profile, and Sessions are the only tables that reference group_id.

    The brute force approach is to update the group_id in each of these tables. Their foreign keys relationships make this challenging; they need to get updated simultaneously,

It might be better to make the grouping mechanism its own table that would be outer joined to the users. The implementation would be fairly lightweight and gets us out of having to update records to link Users.

What would be affected by this change?

The User record is structured as an aggregate of records in the USERS table left joined with PROFILES (on group_id) and ROUTES (on customer_id.)
The group_id column in PROFILE is its primary key but there's no foreign key relationship linking to the group_id column in USERS.

The use case for Profile doesn't require more than a single User and we don't have a means of creating them programmatically at all.
Thinking ahead, I'm assuming we'll have special scripts or web apps that take input from the user and create a Profile. The Customer would
Because of this, we could end up with a profile for each user even and have to consider merging the profiles

Our current data set is pretty junky:

    SELECT p.group_id, given_name, surname, platform_code, platform_id FROM profiles p RIGHT OUTER JOIN users u ON u.group_id = p.group_id order by p.group_id;

    ...reveals that we have a bunch of profiles without a single matching user record. The lack of the foreign key makes this possible even though it makes no sense to allow it.
    We could handle this the same way we do for multiple Users, merging the entries into maps keyed by the platform/id.


Query in PostgresPersistenceManager: USER_PROFILE_BY_PLATFORM_ID_ROUTE

# To extract the schemas from the database into a form that can be re-used (with additional massaging.)
pg_dump -U brbl_admin -h localhost brbl_db_dev -s --schema=brbl_user -f brbl_users.ddl
pg_dump -U brbl_admin -h localhost brbl_db_dev -s --schema=brbl_logic -f brbl_logic.ddl
Note: The ordering of the DDL is a little random.


Experimenting with brbl_users.* tables using a user_groups to model the linking of user records.
Populating the modified schemas (copies of the originals with _t appended to their name):

    INSERT INTO brbl_users.companies_t      (SELECT * FROM companies);
    INSERT INTO brbl_users.profiles_t       (SELECT distinct(p.group_id), surname,given_name,other_languages,p.created_at,p.updated_at, roles FROM profiles p
                                                INNER JOIN users u ON u.group_id = p.group_id order by p.created_at);
    INSERT INTO brbl_users.customers_t      (SELECT c.* FROM customers c INNER JOIN profiles_t p ON p.group_id = c.profile_id);
    INSERT INTO brbl_users.users_t          (SELECT id, platform_code, platform_id, customer_id, country, language, nickname, status, created_at, updated_at FROM USERS);
    INSERT INTO brbl_users.user_groups_t    (SELECT p.group_id, u.id, NOW() from users u join profiles p on p.group_id = u.group_id);

Okay, so now let's figure out what the new query looks for USER_PROFILE_BY_PLATFORM_ID_ROUTE (called by PersistenceManager.getUser())...

There's only one logical record in our current data set that could be returned. The platform is 'S', the channel is 21249, and the platform_id is '17817299468'

SELECT
    u.id,u.group_id,u.platform_id,u.platform_code,u.country,u.language,u.nickname,u.created_at,p.surname,p.given_name,p.other_languages as profile_other_languages,p.created_at as profile_created_at,p.updated_at as profile_updated_at,r.customer_id as routes_customer_id
FROM
    brbl_users.users u
LEFT JOIN
    brbl_users.profiles p
    ON u.group_id = p.group_id
LEFT JOIN
    brbl_logic.routes r
    ON r.customer_id = u.customer_id
WHERE
    r.platform = 'S'::public.platform
    AND
    r.channel = '21249'
    AND
	u.group_id = (
		SELECT group_id
		FROM brbl_users.users
		WHERE platform_id = '17817299468'
	);

This returns three records which are merged into a single User.
The updated query should add users_groups as the link between users and profiles.

To link two users together fully we need to
1) update user_groups.group_id for newer User
2) update profiles.group_id for the newer User if any profile exists
3) update the customer.group_id matching the updated profile.

Wasn't this approach meant to be simpler than the original?

What if we added columns to user_groups for the profile_id and customer_id? Making it more like a fact table in a star schema.
When creating a new user we'd initially enter provide the user_id and group_id (both randomly generated) then set profile_id and customer_id as needed.
To link users together we just update the record's group_id column in this fact table. The other tables don't need to change.
NB: the admin web app can reasonably constrain multiple Customers from being created. Multiple Profiles can be merged as we do for some of the User details.


-- Populating clone of brbl_logic.routes table:
    INSERT INTO brbl_logic.routes_t (SELECT * FROM brbl_logic.routes WHERE customer_id = '4d351c0e-5ce5-456e-8de0-70e04bd5c0fd');



SELECT
    u.id,u.group_id,u.platform_id,u.platform_code,u.country,u.language,u.nickname,u.created_at,
    p.surname,p.given_name,p.other_languages as profile_other_languages,p.created_at as profile_created_at,p.updated_at as profile_updated_at,
    r.customer_id as routes_customer_id
FROM
    brbl_users.users_t u
INNER JOIN
    brbl_users.amalgams a
        ON a.user_id = u.id
LEFT JOIN
    brbl_users.profiles_t p
        ON p.id = a.profile_id
LEFT JOIN
    brbl_users.customers_id c
        ON c.id = a.customer_id
LEFT JOIN
    brbl_logic.routes r
    ON r.customer_id = c.id
;

Given all this work and the insane effort required to massage the existing data to fit the new model I think it's finally time to develop some data generation tools.


===== 2025-12-28 ====

Used DataFaker and EasyRandom to develop Generator. Requires a set of additional record classes that more directly map to the updated Postgres tables.

Created copies of several tables appending "_t" to their names.
We're going to need to update a bunch of SQL to support use of the new amalgams table.

Starting with USER_PROFILE_BY_PLATFORM_ID_ROUTE query.

SELECT
    u.id,
    a.group_id,
    u.platform_id,
    u.platform_code,
    u.country,
    u.language,
    u.nickname,
    u.created_at,
    p.surname,
    p.given_name,
    p.other_languages as profile_other_languages,
    p.created_at      as profile_created_at,
    p.updated_at      as profile_updated_at,
    r.channel,
    a.customer_id as routes_customer_i
FROM
    amalgams_t a
INNER JOIN users_t u
    ON a.user_id = u.id
INNER JOIN profiles_t p
    ON p.id = a.profile_id
INNER JOIN customers_t c
    ON c.id = a.customer_id
INNER JOIN routes_t r
    ON r.customer_id = c.id
WHERE
    u.platform_code = 'S'
    AND u.platform_id = '13054379229'
    AND r.channel = '21249';

Happily this doesn't seem to require rewriting the code that reads the result set and turns it into a User record.

select * from amalgams_t where user_id = 'df31cb95-1492-82bc-61ba-56c9b21660f1'

Inserts into the new version of the USERS table need to be done in conjunction with an insert into the new AMALGAMS table.
How about using something like this:

WITH new_user_cte AS (
        INSERT INTO brbl_users.users_t
            (id, status, platform_id, platform_code, country, language, nickname, created_at, updated_at)
        VALUES
            (?::UUID, ?::user_status, ?, ?::platform, ?, ?, ?, ?, ?)
        RETURNING
            id AS nuc_id, created_at AS nuc_created_at
)
INSERT INTO brbl_users.amalgams_t
    (group_id, user_id, profile_id, customer_id, created_at, updated_at)
SELECT
    ?::UUID, nuc_id, NULL, ?::UUID, nuc_created_at, nuc_created_at
FROM
    new_user_cte
RETURNING
    group_id
;

Ack! Discovered that the Generator is produced records with created_at in the future. MUST FIX!
Ack! Realized I'd created the routes_t table in brbl_logs instead of brbl_logic where it belongs. Fixed.

NB: Just realized that we don't really need a group_id when we've decided that customers owns a given instance of a User and any other records created with a channel owned by said customer. Hmm...would it still be useful to have a single column to track the grouping? Well, yes, the session data upsert benefits from being able to reference a single column.
So let's keep it for now, at least.



===== 2026-01-02 ====

All the current queries in PostgresPersistenceManager appear to be working as intended with the updated *_t tables.

The following still reference the older version of brbl_users.customers, however:

    brbl_logic.push_campaigns
        |
        +--> brbl_logic.scripts
        |       |
        |       V
        +--> brbl_users.customers

Let's complete the cleanup work and get push_campaigns pointing to customers_t and scripts_t and scripts_t pointing to customers_t.

Apparently I'd also made a copy of scripts in the brbl_logs schema. The data there is linked to brbl_users.customers_t so let's just move it there...

    ALTER TABLE brbl_logs.scripts_t SET SCHEMA brbl_logic;

    -- Add the constraints for the scripts_t table.
    ALTER TABLE ONLY brbl_logic.scripts_t
        ADD CONSTRAINT scripts_t_pkey1 PRIMARY KEY (id);

    ALTER TABLE ONLY brbl_logic.scripts_t
        ADD CONSTRAINT fk_customers_t_id FOREIGN KEY (customer_id) REFERENCES brbl_users.customers_t(id);

    ALTER TABLE ONLY brbl_logic.scripts_t
        ADD CONSTRAINT fk_nodes_id FOREIGN KEY (node_id) REFERENCES brbl_logic.nodes(id);

The data in brbl_logic.push_campaign doesn't point at the customers_t or scripts_t tables.
Recreate the fk constraints:

    ALTER TABLE ONLY brbl_logic.push_campaigns DROP CONSTRAINT fk_campaign_customers_id ;
    ALTER TABLE ONLY brbl_logic.push_campaigns DROP CONSTRAINT fk_campaign_scripts_id ;
    UPDATE push_campaigns SET customer_id = '762a353b-0597-0a15-b57d-389b21686463' <a row in customers_t> ...for all two existing rows
    ALTER TABLE ONLY brbl_logic.push_campaigns
        ADD CONSTRAINT fk_campaign_customers_id FOREIGN KEY (customer_id) REFERENCES customers_t(id) ;
    ALTER TABLE ONLY brbl_logic.push_campaigns
        ADD CONSTRAINT fk_campaign_scripts_id FOREIGN KEY (script_id) REFERENCES scripts_t(id) ;

Got all the unit tests working again with the new variants of the brbl_user and brbl_logic tables (the ones with the _t suffix.)


===== 2026-01-03 ====


TODO
X "finalize" the schema changes

ok
brbl_biz   | addresses          --> OK
brbl_biz   | customer_address   --> repoint to customers_t, recreate composite primary key, no rows to migrate

    ALTER TABLE brbl_biz.customer_addresses DROP CONSTRAINT customer_addresses_pkey ;
    ALTER TABLE brbl_biz.customer_addresses DROP CONSTRAINT customer_addresses_customer_id_fkey ;
    ALTER TABLE brbl_biz.customer_addresses ADD CONSTRAINT customer_addresses_pkey PRIMARY KEY(customer_id, address_id) ;
    ALTER TABLE brbl_biz.customer_addresses ADD CONSTRAINT customer_addresses_customer_id_fkey FOREIGN KEY (customer_id) REFERENCES brbl_users.customers_t(id) ;

brbl_biz   | payment_cc         --> OK

brbl_logic | campaign_users     --> users_t
    DELETE FROM brbl_logic.campaign_users;
    ALTER TABLE brbl_logic.campaign_users DROP CONSTRAINT fk_campaign_users_user_id;
    ALTER TABLE brbl_logic.campaign_users ADD CONSTRAINT fk_campaign_users_user_id FOREIGN KEY(user_id) REFERENCES brbl_users.users_t(id);
    INSERT INTO brbl_logic.campaign_users SELECT 'eb7aa81a-b314-420c-8f3d-df4755faa9bb', id, 'PENDING' from users_t where status = 'IN' and platform_code = 'S' and language = 'POR';
    INSERT INTO brbl_logic.campaign_users SELECT '95497de1-a092-430d-a37c-25ef35d7d208', id, 'PENDING' from users_t where status = 'IN' and language= 'ENG' and platform_code = 'W';

brbl_logic | default_scripts    --> customer_id, add fk constraint. no rows to migrate; same values are already in new table.
    ALTER TABLE brbl_logic.default_scripts ADD CONSTRAINT fk_default_scripts_customer_id FOREIGN KEY(customer_id) REFERENCES brbl_users.customers(id);

brbl_logic | edges              --> OK
brbl_logic | keywords           --> OK
brbl_logic | nodes              --> OK
brbl_logic | push_campaigns     --> OK (already points to customers_t and scripts_t)
brbl_logic | routes             --> OK ready to drop
brbl_logic | routes_t           --> create matching constraints

    ALTER TABLE brbl_logic.routes_t ADD CONSTRAINT routes_pkey PRIMARY KEY(id) ;
    ALTER TABLE brbl_logic.routes_t ADD CONSTRAINT unique_routes_t_platform_channel UNIQUE(platform, channel) ;
    ALTER TABLE brbl_logic.routes_t ADD CONSTRAINT fk_routes_t_customers_id FOREIGN KEY(customer_id) REFERENCES customers_t(id) ;
    ALTER TABLE brbl_logic.routes_t ADD CONSTRAINT fk_routes_nodes_id FOREIGN KEY(default_node_id) REFERENCES nodes(id) ;

brbl_logic | schedule           --> drop and rename to plural form, repointing fk to scripts_t

    DROP TABLE brbl_logic.schedule ;
    CREATE TABLE brbl_logic.schedules (
        id                      UUID PRIMARY KEY,
        expression              VARCHAR(24) NOT NULL,
        is_recurring            boolean NOT NULL DEFAULT FALSE, -- convenience flag to distinguish one time and recurring schedules.
        script_id               UUID NOT NULL,
        created_at              TIMESTAMP WITH TIME ZONE NOT NULL,
        updated_at              TIMESTAMP WITH TIME ZONE NOT NULL,
        CONSTRAINT fk_scripts_id FOREIGN KEY(script_id)
            REFERENCES brbl_logic.scripts_t(id)
    );
    COMMENT ON COLUMN brbl_logic.schedules.expression   IS 'The cron string describing the schedule.';
    COMMENT ON COLUMN brbl_logic.schedules.is_recurring IS 'TRUE is recurring, FALSE is one-time execution';
    COMMENT ON COLUMN brbl_logic.schedules.script_id    IS 'The script to be processed.';

brbl_logic | scripts            --> OK ready to drop
brbl_logic | scripts_t          --> OK
brbl_logic | sessions           --> OK (consider adding FK to amalgams)

brbl_logs  | messages_mo        --> OK
brbl_logs  | messages_mo_prcd   --> OK
brbl_logs  | messages_mt        --> OK
brbl_logs  | messages_mt_dlvr   --> OK
brbl_users | amalgams_t         --> OK
brbl_users | companies          --> OK
brbl_users | companies_t        --> OK
brbl_users | customers          --> OK
brbl_users | customers_t        --> OK
brbl_users | profiles           --> OK
brbl_users | profiles_t         --> OK
brbl_users | users              --> OK
brbl_users | users_t            --> OK

Rename tables that have a replacement (_t) version then rename the _t to its predecessor.

ALTER TABLE brbl_logic.routes      RENAME TO old_routes   ;
ALTER TABLE brbl_logic.scripts     RENAME TO old_scripts  ;
ALTER TABLE brbl_users.companies   RENAME TO old_companies;
ALTER TABLE brbl_users.customers   RENAME TO old_customers;
ALTER TABLE brbl_users.profiles    RENAME TO old_profiles ;
ALTER TABLE brbl_users.users       RENAME TO old_users    ;
ALTER TABLE brbl_logic.routes_t    RENAME TO routes     ;
ALTER TABLE brbl_logic.scripts_t   RENAME TO scripts    ;
ALTER TABLE brbl_users.companies_t RENAME TO companies  ;
ALTER TABLE brbl_users.customers_t RENAME TO customers  ;
ALTER TABLE brbl_users.profiles_t  RENAME TO profiles   ;
ALTER TABLE brbl_users.users_t     RENAME TO users      ;
ALTER TABLE brbl_users.amalgams_t  RENAME TO amalgams   ;


===== 2026-01-04 ====

TODO
X Add support for the remaining tables in Generator.

brbl_biz   . addresses
brbl_biz   . customer_addresses
brbl_biz   . payment_cc

brbl_logic . nodes          >> \copy brbl_logic.nodes TO 'dml/nodes.csv' CSV HEADER ;
brbl_logic . edges          >> \copy brbl_logic.edges TO 'dml/edges.csv' CSV HEADER ;
brbl_logic . scripts
brbl_logic . push_campaigns
brbl_logic . campaign_users
brbl_logic . routes
brbl_logic . default_scripts >> not sure we need this given we already have routes.
brbl_logic . keywords
brbl_logic . schedules
brbl_logic . sessions

Then, given all the previously generated data in brbl_users and brbl_logic, simulate messages being sent/received by those entities:

brbl_logs  . messages_mo
brbl_logs  . messages_mo_prcd
brbl_logs  . messages_mt
brbl_logs  . messages_mt_dlvr

NB:
Super easy to store and load data from .csv files locally:

    \copy brbl_logic.nodes TO   'dml/nodes.csv' CSV HEADER ;
    \copy brbl_logic.nodes FROM 'dml/users.csv' CSV HEADER ;



===== 2026-01-07 ====

Completed (despite it being ugly as sin) ConversationGenerator which formats random questions & answers generated by Gemini into SQL.

TODO
X add Routes and Scripts connecting them to Nodes and Customers.


===== 2026-01-13 ====

Neglected to log work for the past several days...

Completed (for now, at least) work to generate business data in the brbl_users and brbl_logic schemas.
Not the prettiest or most meaningful bit of code but a decent start, I think...

Given the substantial refactoring I've done I should have a think about how it plays with the user linking and push messaging processes we want to implement.

For user linking, the goal is to connect two existing User records via the amalgams table: The method is to send a message to the source platform containing a URL to the destination platform. The URL will include an opaque identifier that will have been mapped to the user's amalgams.group_id. The handler for the get request will look for group_id that's mapped to this opaque id and, if found, update the newest amalgam row to have the same group_id. The trickier part will be deciding how to handle the possibly multiple sessions. Does it make sense to merge them somehow? Or clobber the session for the destination with the one from the linking (source) platform? The profiles for the person being linked could conceivably remain untouched.

Related note: the code that constructs a com.enoughisasgoodasafeast.operator.User assumes there may be multiple database records representing the same person on multiple platforms. But the query used, USER_PROFILE_BY_PLATFORM_ID_ROUTE, uses the specific platform, platform id, and route in its where clause. Per the new design, this will only ever return a single user. For now we'll leave it as is.


===== 2026-01-14 ====

Given all the work we had to do for data generation and the updated schemas, I thought it might be time to dig into schema management.
Started researching the usual suspects: Liquibase, Atlasgo, and Flyway. All are ostensibly open-source but with lots of paid features. Liquibase has been doing the bait-and-switch relicensing thing. I'm suspicious of all three.

Found an interesting alternative: https://pgroll.com/blog/introducing-pgroll-zero-downtime-reversible-schema-migrations-for-postgres

It's Postgres centric (exclusive?) but that's not a bother. In fact, for something like this, leveraging features of the database engine itself, seems like a good idea. Apparently it follows a "Expand and Contract Pattern" that uses views heavily to apply changes (mostly) without locks and still allow rollbacks.

Backed by Xatabase, a Postgres added-value shop that also provides features like copy-on-write tables. Their blog has a lot of interesting articles on Postgres.

Cleanup before we initialize the migrations tool:

    Drop timestamp9 extension:
        [Connected as 'mark' aka super user and ran 'DROP EXTENSION timestamp9;' to remove the unused feature.]
    Should we upgrade from version 17.4 to v18 first? We know we want it...Yes, let's do that...

Created a full backup of the "cluster" in the backups folder:
    pg_dumpall > full_cluster_backup_20260114_01.sql
    NOTE: This is pretty old-school; the output file is purely text and uses the copy command with stdin provided by the script itself.
Also created a dump of just the database (schema and data, without users/roles):
    pg_dump -d brbl_db_dev -Fc -Z6 > db_only_backup_compressed_20260114_01.sql
        [NB: There's a tool that includes scheduling & uploading backups called https://postgresus.com/]
NOTE: Our effective database PGDATA is /opt/homebrew/var/postgresql@17. However, I don't have PGDATA set in my shell currently. Probably should...
Ran 'VACUUM (ANALYZE, VERBOSE);' just for good measure. It didn't have much to do which is what I'd expect.

We used homebrew to install our current postgres (17.4) so it makes sense to install 18 the same way.
brew info postgresql@18 notes that we can start the new version and set it up to restart at login using the command:
    brew services start postgresql@18
It also notes that this is "keg-only"; the default version where brew is concerned is 14.20!!! Yikes, that's old! So we need to specify the version.

Reading the docs re: upgrades it looks simpler to avoid pg_upgrade and simply load a copy of the cluster made with pg_dumpall.



psql -U postgres -c 'SHOW config_file;' --> reveals the directory that contains the configuration for the current instance:
    /opt/homebrew/var/postgresql@17/postgresql.conf
        pg_hba.conf     >> version 18 is the same as 17.
        postgresql.conf >> version 18 is almost the same. Just the locale is different: lc_* parameters went from 'C' to 'en_US.UTF-8'
psql -U postgres -c 'SHOW data_directory;' --> shows the data directory.
    /opt/homebrew/var/postgresql@17/

From https://www.postgresql.org/docs/current/upgrading.html#UPGRADING-VIA-PG-UPGRADE:
    "2. Shut down the old server:"
        pg_ctl stop [assumes PGDATA is pointing at the 17 data directory.]
    "4. Install the new version of PostgreSQL as outlined [...]"
    "5. Create a new database cluster if needed."
        [The brew install process did this for us already.]
    "7. Start the database server, again using the special database user account:"
        /opt/homebrew/Cellar/postgresql@18/18.1_1/bin/postgres -D /opt/homebrew/var/postgresql@18
    "8. Finally, restore your data from backup with:"
        /opt/homebrew/Cellar/postgresql@18/18.1_1/bin/psql -d postgres -f /Users/mark/Development/sndrRcvr/backups/full_cluster_backup_20260114_01.sql

Cool, seems to have worked fine.

Setting up the login/restart boot setup:
    brew services stop  postgresql@17
    brew services start postgresql@18

Should we get rid of the old_* tables? Any data there we'd like to keep?

Started thinking about updating to use UUIDv7...probably simplest to find a Java impl then just regenerate the data.
Side-note: it appears that the storage for UUIDs hasn't changed so, technically, we don't have to update the data.
We could just start using the new type; I verified that they're compatible.

Moving on to trying out pgroll for schema management/migration.

brew tap xataio/pgroll
brew install pgroll

sndrRcvr/ddl on  main [!+?]
❯ mkdir migrations
sndrRcvr/ddl on  main [!+?]
❯ pgroll init --postgres-url "postgres://mark:postgres@localhost:5432/brbl_db_dev?sslmode=disable"
 SUCCESS  Initialization complete
sndrRcvr/ddl on  main [!+?]
❯ pgroll baseline 0001_initial_schema ./migrations --postgres-url "postgres://mark:postgres@localhost:5432/brbl_db_dev?sslmode=disable"
Creating a baseline migration will restart the migration history.
Please confirm [y/N]: Yes
 SUCCESS  Baseline created successfully. Placeholder migration "migrations/0001_initial_schema.yaml" written

sndrRcvr/ddl on  main [!+?] took 6m29s
❯ psql -U mark -d brbl_db_dev
brbl_db_dev=# \dt pgroll.*;

             List of tables
 Schema |      Name      | Type  | Owner
--------+----------------+-------+-------
 pgroll | migrations     | table | mark
 pgroll | pgroll_version | table | mark
(2 rows)


===== 2026-01-15 ====

Rebooted today when the machine started getting warm. I think it was just Chrome misbehaving but thought it wouldn't be the worst idea to let the hardware reset.
My uptime right before that:
 9:05  up 382 days, 19:29, 1 user, load averages: 2.51 3.77 4.52
Wow, pretty f'ing cool. Take that Windows!

Next steps...
Planning to make the next schema change via pgroll but otherwise not taking it further for now.

1. Find a UUIDv7 impl for our Java code.
2. Organize the DDL and DML folders. (Mostly getting rid of the junk temp files.)
3. Figure out how to use the results to bootstrap a fresh postgres for testing in a container.

Since we upgraded to the latest Postgres (18) and installed the pgroll schema and created a baseline, let's create another full backup of the cluster:
    pg_dumpall > data/full_cluster_backup_20260115_01.sql
    pg_dump -d brbl_db_dev -Fc -Z6 > data/db_only_backup_compressed_20260115_01.sql

Also updated the ddl/ folder with

pg_dump -d brbl_db_dev --schema-only > brbl_db_schema_20260115.ddl

NOTE: this ends up including the pgroll schema and all its associated parts (indexes, triggers, etc.)

