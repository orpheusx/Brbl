https://medium.com/helidon/helidon-logging-and-mdc-5de272cf085d

explains, imperfectly, how to use JUL logging with Helidon.
Turns out the configuration file that Helidon expects is logging.properties at the root of the context.
The format is a little weird.

NAMING SCHEMES:

We're going to use the terminology from our SMS/SMPP days.

for queues

    <platform>.<region>.<direction>

    e.g.

    whatsapp.us.mo  - the platform is whatsapp, the region is the United States, the direction is Mobile Originated.

    test.local.mo   - the platform is a developer/test, the region is the scope of the test resources (likely a dev machine), the direction is Mobile Originated.

Current design uses Topics with routingKeys.
I'm thinking that we'd likely have separate endpoints running for each platform gateway but using topics & routingKeys gives us flexibility.

Test Setup:

Run the container image with the RabbitMQ server:

    docker run -it --rm --name rabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:4.0-management

FakePlatformMO (client)
    Reads messages from file, for each
    HTTP POST to
     --> Rcvr (port: 4242)
            --> RabbitMQ client enqueue to "test.mo"

     --> "Operator"
         RabbitMQ client dequeues from "test.mo"
         Processes the message (side effects galore)
         Creates response
         Enqueues response to "test.mt"

     --> Sndr
        RabbitMQ client dequeues from "test.mt"
        HTTP POST to

     --> FakePlatformMT (port: 2424)
        Writes each received message to file.

 Validator compares input file with output file.

Test Program:
FakePlatformMO generates messages with known values.
FakePlatformMT receives messages and validates that it corresponds to a previously sent message.


Helidon has libraries that support tracing (https://helidon.io/docs/v4/se/tracing) for both the web server and web client. Note: OpenTracing (https://opentracing.io/) has been retired in favor of OpenTelemetry (https://opentelemetry.io/docs/languages/js/instrumentation/)

OpenAPI is also supported in case parts of the software needs to be used with AWS Lambda.

Scheduling support is available (https://helidon.io/docs/v4/se/scheduling) which should allow us to avoid shit like EventBridge. Under the covers it uses cron-utils (https://github.com/jmrozanec/cron-utils)


=========== Building an executable jar ===========
Added the following components to the <build/> section of the pom.xml file:
    exec-maven-plugin
    maven-compiler-plugin
    maven-jar-plugin
    maven-assembly-plugin
Now we can run:
    mvn clean compile assembly:single

to yield an artifact that can be run with 'java -jar'
The artifact ends up being around 23MB as of 12/12/2024.

Java Microbenchmark Harness:
    https://www.baeldung.com/java-microbenchmark-harness


Trying to compile the project with native-image yielded errors related to logback.
The GraalVM output was pretty helpful. It suggested adding to the file that tracks usages of reflection.
While researching the problem someone suggested switching to the new configuration system available for logback: logback-tyler
This provides a class generator that provides a fairly easy to edit Configurator implementation that doesn't use XML or reflection.
In addition to making Graal happy the initialization will be faster and more efficient for the lack of XML.

Of course, it couldn't be that simple ;-) There seems to be a bug that causes the generated class to not be marked public. The service-provider mechanism added JDK9 in support of the new package system complained of not having access. Simply editing the class to be public appears to solve the problem. I posted a question to the GitHub discussion board:

    https://github.com/qos-ch/logback-tyler/discussions/5

Needed to add a reachability-metadata.json file to META-INF/native-image to have the .properties files included in the binary.

Some notable/interesting bits from the compiler output:

Top 10 origins of code area:                                Top 10 object types in image heap:
  11.60MB java.base                                            8.48MB byte[] for embedded resources
   2.59MB java.xml                                             5.24MB byte[] for code metadata
   1.22MB svm.jar (Native Image)                               3.31MB byte[] for java.lang.String
 747.48kB logback-core-1.5.12.jar                              2.31MB java.lang.String
 468.30kB amqp-client-5.23.0.jar                               2.20MB java.lang.Class
 326.55kB java.rmi                                           843.80kB byte[] for general heap data
 244.26kB logback-classic-1.5.12.jar                         785.30kB com.oracle.svm.core.hub.DynamicHubCompanion
 207.22kB helidon-http-http2-4.1.4.jar                       554.23kB heap alignment
 203.91kB java.naming                                        513.00kB int[][]
 203.76kB helidon-webclient-api-4.1.4.jar                    491.98kB byte[] for reflection metadata
   1.46MB for 37 more packages                                 5.34MB for 2375 more object types

Recommendations:
 HEAP: Set max heap for improved and more predictable memory usage.
 CPU:  Enable more CPU features with '-march=native' for improved performance.

 Questions: Why are we getting java.xml and java.rmi included?
 The binary size is significant: 50MB.

 We need a way to produce containers that will run locally on my M1
 Can we run a Linux VM locally to run the build? Or even a container?

 NOTE: For a moment I thought the --target option of native-image would let us produce an executable for different platforms.
 This is Java (Write-Once Run Everywhereâ„¢) after. But it turns out it's bullshit. After six years of development this is still on the to do list.
 Given the prevalence of containers which are _only_ supported for Linux its amazing that this hasn't been prioritized.
 As it stands, we'll need to create a bunch of extra container rigging to produce something that we then copy over into another image.

    https://github.com/spotify/dockerfile-maven is a project for building container images with Docker. It's not still in development, however.

=========== STUFF WE WILL WANT ===========

- Software BOM with security information for our dependencies.
- Qodana or something free to perform structured code analysis (we need to take some time to configure this to remove bogus or just unhelpful problems.)
- Container images for arm64 and amd64.

=========== STUFF WE SHOULD LEARN MORE ABOUT ===========
The modules system introduced in JDK 9.
The inner workings of Docker so I can evaluate images, how to extend, etc.

Docker progress:

Finally figured out how to specify/pass arguments when running Burble inside a docker container.
Changed the

docker run -it --rm --name burble -p 2424:2424 -p 4242:4242 burble:0.1 FakeOperator


native-image -jar target/sndrRcvr-1.0-SNAPSHOT-jar-with-dependencies.jar -o burble
    produces a file, burble, that is still fucking massive at 44MB. :-(

The container that builds on eclipse-temurin:23 and includes the binary weighs in at 719MB.

Switching to eclipse-temurin:23-alpine reduces the image to a "mere" 581MB.
Most of the container comes from the installation of
    apk add --no-cache fontconfig ttf-dejavu gnupg ca-certificates p11-kit-trust musl-locales musl-locales-lang binutils tzdata coreutils openssl (63MB)
    OpenJDK23U-jdk_aarch64_alpine-linux_hotspot_23.0.1_11.tar.gz (310MB uncompressed)
The Burble executable is the third-largest item.

We need to add config to the native-image build to specify the musl C lib. This is only applicable to the Linux version, however.
Musl is not a thing for macOS.

Most people apparently use a container to build their linux image. There's an "official" Maven image repo

docker run -it --rm --name mvnc -v "$HOME/.m2":/root/.m2 -v "$(pwd)":/Users/mark/Development/sndrRcvr/src -w /Users/mark/Development/sndrRcvr/src maven:latest mvn package

We want a container
- with graalvm 23
- 3.x Maven
Initially this doesn't need to be alpine based but the musl thing might require it

This repo looks promising: https://github.com/vegardit/docker-graalvm-maven/blob/main/README.md

https://github.com/vegardit/docker-graalvm-maven/blob/main/README.md

=== Example use of the vegardit image to build a native binary for linux ===
docker run --rm -it \
  -v $PWD:/mnt/myproject:rw \       # what's the effect of the :rw part of this?
  -w /mnt/myproject \
  vegardit/graalvm-maven:latest-java23 \
  mvn clean package

Here's how we're using it to

> docker run --rm -it -v "$HOME/.m2/repository":/root/.m2/repository -v "$(pwd)":/mnt/sndrRcvr -w /mnt/sndrRcvr vegardit/graalvm-maven:latest-java23 mvn clean package

Works! Produces a jar file in the target folder of our host OS.

> docker run --rm -it -v "$HOME/.m2/repository":/root/.m2/repository:rw -v "$(pwd)":/mnt/sndrRcvr -w /mnt/sndrRcvr vegardit/graalvm-maven:latest-java23 mvn -Pnative clean package

Works! Produces an ELF executable using the aarch64 instruction set.
NOTE: the unix `file` command points out it's dynamically linked, however:
    target/burble: ELF 64-bit LSB pie executable, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-aarch64.so.1, BuildID[sha1]=99c5d1bf7a89955c2461187540350e00678a5fa1, for GNU/Linux 3.7.0, not stripped

Once the artifact is produced, we can build the docker image that will execute it...

Made a copy of Dockerfile then renamed both.
    Dockerfile.bin - builds an image that uses a native-image generated executable.
    Dockerfile.jvm - builds an image that runs a normal jar file using eclipse-temurin:23-alpine.

Use them like so to build the image:
    docker build -t burble-bin:0.1.0 -f Dockerfile.bin .
    docker build -t burble-jvm:0.1.0 -f Dockerfile.jvm .

Then to run them:
    The binary executable version:
    docker run -it --rm -p 4242:4242 --name burble-bin-rcvr burble-bin:0.1.0 Rcvr
    docker run -it --rm --name burble-bin-fkop burble-bin:0.1.0 FakeOperator
    docker run -it --rm --name burble-bin-sndr burble-bin:0.1.0 Sndr

    The JVM version:
    docker run -it --rm -p 4242:4242 --name burble-jvm-rcvr burble-jvm:0.1.0 Rcvr
    docker run -it --rm --name burble-jvm-fkop burble-jvm:0.1.0 FakeOperator
    docker run -it --rm --name burble-jvm-sndr burble-jvm:0.1.0 Sndr

Where the general form is:
    docker run -it --rm [-p 2424:2424] | [-p 4242:4242] --name burble-jvm burble-jvm:0.1.0 <programName: Rcvr | FakeOperator | Sndr>

The Dockerfiles both default to running FakeOperator.

To run the image without executing the entrypoint/cmd:
    docker run -it --entrypoint /bin/sh burble-bin:0.1.0

Actually it appears that the binary version running on the alpine container fails because, as noted above, the executable is dynamically linked.
It fails with a very terse and somewhat misleading error:
    "exec /opt/app/burble: no such file or directory"
I think this means that it can't find the expected libc rather than the program itself.

I added a block to the configuration section of the native-maven-plugin setup:
<buildArgs>
    <buildArg>--static --libc=musl --enable-sbom</buildArg>
</buildArgs>

Which seems to be signalling the correct switches in the native-image program but the container that produces the linux binary lacks the referenced library.

docker run --rm -it -v "$HOME/.m2/repository":/root/.m2/repository:rw -v "$(pwd)":/mnt/sndrRcvr -w /mnt/sndrRcvr mstewart/graalvm-maven-musl mvn -Pnative clean package

Notable output from native-image:

"HEAP: Set max heap for improved and more predictable memory usage."

Would be nice if they mentioned the switch/param/flag that controls this. Is it the normal JVM flag? Where does it get set?

 "CPU:  Enable more CPU features with '-march=native' for improved performance."

 Added <buildArg>-march=native</buildArg> to the native-maven-plugin config which eliminates the message, so I assume it was the right way to do it.

 1 experimental option(s) unlocked:
 - '-H:IncludeResources': Use a resource-config.json in your META-INF/native-image/<groupID>/<artifactID> directory instead. (origin(s): 'META-INF/native-image/com.rabbitmq/amqp-client/native-image.properties' in 'file:///root/.m2/repository/com/rabbitmq/amqp-client/5.23.0/amqp-client-5.23.0.jar')

This is annoying. The lib is providing the needed data so it can be used to produce the native executable, but they've (I guess) changed their mind how this is to be implemented? Worse, they leave it to me to figure out how this is to be represented in the new file.
I think, for now, we will leave this unaddressed. At least until we're certain of the lib versions we want to use.

::IMPORTANT NOTE::
As of 12/26/2024, building a fully statically linked graalvm native image is *only* supported on linux for x86_64:
    https://github.com/oracle/graal/issues/9490
This is sad. The project seems old enough that it shouldn't still be a problem. There's no cross-compilation either though that is a little more understandable.
We should try building the musl/statically linked binary on one of our x86 machines just to be sure it will work.

NEXT STEPS:
X create a working docker-compose.yml to coordinate testing (leave rabbitmq out initially)
_ create some end-to-end tests
_ extend the rabbitmq image to add the admin user and any other customized bits we need then add it to the docker-compose.yaml.
_ create a management CLI using JLine/Jansi, etc.


I swear, where Docker and GraalVM is concerned, every step is a fucking struggle.

Running just a single container via docker-compose.yaml I seem unable to get an HTTP call to the Rcvr web server.

mark@YA-T7RX9LX2PL sndrRcvr % docker compose up -d

mark@YA-T7RX9LX2PL sndrRcvr % curl --verbose http://localhost:4242/health
* Host localhost:4242 was resolved.
* IPv6: ::1
* IPv4: 127.0.0.1
*   Trying [::1]:4242...
* Connected to localhost (::1) port 4242
> GET /health HTTP/1.1
> Host: localhost:4242
> User-Agent: curl/8.7.1
> Accept: */*
>
* Request completely sent off
* Empty reply from server
* Closing connection
curl: (52) Empty reply from server

===== 2024-12-28 ======
Ran the jvm  version of the containers and no longer get the weird HTTP problem. Were the binaries not working?

I re-ran the build_linux.sh node to make sure the executable is what we want.

mark@YA-T7RX9LX2PL sndrRcvr % file target/burble
target/burble: ELF 64-bit LSB pie executable, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-aarch64.so.1, BuildID[sha1]=0c26671ef34dfacfcaaea9595bc5e32e0417980a, for GNU/Linux 3.7.0, not stripped

OK, check.

The build uses a Debian-based container while the execution container is "eclipse-temurin:23" (I think this uses ubuntu as its base.)
Let's try it with "eclipse-temurin:23-jre" as the base layer (this is definitely an Ubuntu image and might be a bit smaller than the JDK version.)

...and, confirmed, that the problem is only with the binary version. Possibly because of the Debian-Ubuntu schism?

No, even when I change the Dockerfile of the image that runs the program to use the same Debian-based image that built the program AND when running curl from within that container.

$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ cat /proc/version
Linux version 6.10.11-linuxkit (root@buildkitsandbox) (gcc (Alpine 13.2.1_git20240309) 13.2.1 20240309, GNU ld (GNU Binutils) 2.42) #1 SMP Thu Oct  3 10:17:28 UTC 2024

Why is the gcc showing Alpine?

Hmm, the RabbitMQ supplied image that we're using--running Noble Numbat, version 24.04.1 LTS--shows the same gcc info. Maybe this is normal?

$ ldd /opt/app/burble
        linux-vdso.so.1 (0x0000ffffa22a2000)
        libz.so.1 => /lib/aarch64-linux-gnu/libz.so.1 (0x0000ffffa2230000)
        libc.so.6 => /lib/aarch64-linux-gnu/libc.so.6 (0x0000ffff9ea50000)
        /lib/ld-linux-aarch64.so.1 (0x0000ffffa2265000)

===== 2024-12-30 ======

Curious if we get a smaller image if we build the jre instead of the jdk...
With JDK...
burble-jvm                               0.1.0            912143180f41   41 hours ago   801MB
With JRE...
burble-jvm                               0.1.0            7171dc515c27   3 minutes ago   507MB

So, yay. The first actual reduction we've seen. GraalVM binaries are still fucking huge.
Baseline manual test (sending messages through the pipeline) appears to work fine.

What could we gain by putting a JRE on top of something like debian:stable-slim (which is used as the base by vegardit/graalvm-maven, which we use for building linux binaries.)

debian                                   stable-slim      5f21ebd35844   7 days ago       136MB
eclipse-temurin                          23               c2a3aba09776   2 months ago     712MB

How does the Ubuntu base for eclipse-temurin compare to slim?

While investigating this I checked out https://hub.docker.com/_/eclipse-temurin.
Two things of note:
They show how to set up the JDK using a reference to one of their images:

    FROM <base image>
    ENV JAVA_HOME=/opt/java/openjdk
    COPY --from=eclipse-temurin:23 $JAVA_HOME $JAVA_HOME
    ENV PATH="${JAVA_HOME}/bin:${PATH}"

They recommend building a custom JRE using jlink. Is this worth the bother?
There's also an eclipse-temurin:23.0.1_11-jre-alpine image.
See https://github.com/docker-library/repo-info/blob/master/repos/eclipse-temurin/local/23-jre-alpine.md

eclipse-temurin                          23.0.1_11-jre-alpine   623a424ca41d   2 months ago     291MB
Inspecting the layers it appears that the JRE comprises ~161MB of this image.
NOTE: busybox, which combines ~400 commands into a single program, doesn't include curl but *does* include wget so we won't need the former to do basic, intra-container testing.

Adding our code only adds 8MB:
burble-jvm                               0.1.0                  52f06679217c   2 minutes ago   298MB

This is a decent tradeoff.

Another good tip here to avoid having to rebuild the image every time the Maven produced jar changes is to mount the host path onto the container. So given,

    FROM eclipse-temurin:21.0.2_13-jdk
    CMD ["java", "-jar", "/opt/app/japp.jar"]

We can run the container like this:
    docker build -t <tag> .
    docker run -it -v /path/on/host/system/jars:/opt/app <tag>

What syntax do we use for a docker-compose file to do this?

TODO
_ Since building a fully statically linked graalvm native image is *only* supported on linux for x86_64 we should bust out our large memory Macbook.
X Create a merged PlatformGatewayMT and MO generator for testing purposes.

Digging into the testcontainers project and found an image for RabbitMQ that's built on an Alpine base.
As expected it's smaller than the non -alpine version:
rabbitmq                                 4.0-management          14c30a03410f   3 months ago   425MB
rabbitmq                                 4.0-management-alpine   74bf73c53b96   3 months ago   295MB

Testcontainers supports running docker-compose files though the logging output is dramatically different from what we see running those files via docker. See https://codeal.medium.com/how-to-run-docker-compose-with-testcontainers-7d1ba73afeeb (it mentions the use of the volumes section of the compose file that, for things like postgres, can be used to run ddl scripts.


We want to be able to do this for integration testing.
That said, testing things we've noted to be possibly problematic--having different components come up out of order or restarting--would seem to require a per-container level of control.

Side-note: It appears possible to use Testcontainers with Docker alternatives, OrbStack and Colima. See http://rockyourcode.com/testcontainers-with-orbstack/ and http://rockyourcode.com/testcontainers-with-colima. Also https://github.com/testcontainers/testcontainers-java/issues/5034#issuecomment-1036433226

Probably not worth the extra effort unless we have problems with Docker.

===== 2025-01-02 ======

Working on EndToEndMessaging:

The docker setup code informs me:
    'container_name' property set for service 'brkr' but this property is not supported by Testcontainers, consider removing it
But clearly it's more than just a suggestion because it throws an ExceptionInInitializerError that kills the container startup.

This is tracked in https://github.com/testcontainers/testcontainers-java/issues/2472, but they don't seem to be interested in fixing it; a perfectly reasonable PR adding support for it--https://github.com/testcontainers/testcontainers-java/pull/2741--was closed.
So I've commented out the container_name in the docker-compose-jvm.yaml file.

Created an integration test, EndToEndMessaging that uses this file. The @Container annotation is supposed to start the docker process, but it doesn't seem to be doing so. Adding a static block that calls the .start() method after the annotation is a simple workaround even if it's a bit disappointing.

Reworked PlatformGatewayMT to act as both a MO source and the MT destination.

Next up: using specific containers to test ordering dependencies.

Scenarios:
- what should Rcvr do if the broker disappears?
    Currently, when messages are received we actually return the following to each calling request:
        connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)Complete.

    Obviously we shouldn't do this.
    Does it recover when the broker comes back?

    Restarted the broker container and got the following in the Rcvr log:

        2025-01-05 10:38:44,542 ERROR [] [AMQP Connection 192.168.1.155:5672] c.r.c.impl.ForgivingExceptionHandler - Caught an exception during connection recovery!
        java.io.IOException: null
        	at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:140)
        	at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:136)
        	at com.rabbitmq.client.impl.AMQConnection.start(AMQConnection.java:406)
        	at com.rabbitmq.client.impl.recovery.RecoveryAwareAMQConnectionFactory.newConnection(RecoveryAwareAMQConnectionFactory.java:71)
        	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.recoverConnection(AutorecoveringConnection.java:628)
        	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.beginAutomaticRecovery(AutorecoveringConnection.java:589)
        	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.lambda$addAutomaticRecoveryListener$3(AutorecoveringConnection.java:524)
        	at com.rabbitmq.client.impl.AMQConnection.notifyRecoveryCanBeginListeners(AMQConnection.java:839)
        	at com.rabbitmq.client.impl.AMQConnection.doFinalShutdown(AMQConnection.java:816)
        	at com.rabbitmq.client.impl.AMQConnection$MainLoop.run(AMQConnection.java:700)
        	at java.base/java.lang.Thread.run(Thread.java:1575)
        Caused by: com.rabbitmq.client.ShutdownSignalException: connection error
        	at com.rabbitmq.utility.ValueOrException.getValue(ValueOrException.java:66)
        	at com.rabbitmq.utility.BlockingValueOrException.uninterruptibleGetValue(BlockingValueOrException.java:36)
        	at com.rabbitmq.client.impl.AMQChannel$BlockingRpcContinuation.getReply(AMQChannel.java:552)
        	at com.rabbitmq.client.impl.AMQConnection.start(AMQConnection.java:336)
        	... 8 common frames omitted
        Caused by: java.io.EOFException: null
        	at java.base/java.io.DataInputStream.readUnsignedByte(DataInputStream.java:297)
        	at com.rabbitmq.client.impl.Frame.readFrom(Frame.java:91)
        	at com.rabbitmq.client.impl.SocketFrameHandler.readFrame(SocketFrameHandler.java:199)
        	at com.rabbitmq.client.impl.AMQConnection$MainLoop.run(AMQConnection.java:687)
        	... 1 common frames omitted

However, when we sent more traffic, the Rcvr handled it and was able to send it to the broker. Then I started the FakeOperator and the messages again passed through.

So is there a callback available to know when the broker is available?

===== 2025-01-07 ======
Having yet more fun with Testcontainers. >:-[

To explore the availability scenarios above, I'm programmatically creating containers for each application.

If I try starting a Rcvr container without a RabbitMQ broker running I get a complaint from InternalCommandPortListeningCheck that seems to stem from the call to withExposedPorts() that's used to connect with the Rcvr's web server.

It's clear, however, that to really have our components be testable in this way we need to provide a direct means of configuring ports.
Added an override mechanism to ConfigLoader that reads the properties file and the environment vars, overriding the former with the latter where they overlap.

Need a similar mechanism for PlatformGateway to provide it the effective http port for the Rcvr...
Hacked a fix for this but still running into the InternalCommandPortListeningCheck:
    https://github.com/testcontainers/testcontainers-java/issues/6730

Fucking hell, maybe it's just easier to add bash.
Did that but also needed to rebuild the jar file (which I had been neglecting.) Time to set up a shared mount for the containers so we don't need to rebuild the container everytime we make a change to the Java code (assuming we even remember to do that!)

===== 2025-01-09 ======
ServiceAvailabilityTest.testRcvrReconnect:
I have most of the containers running and passing messages along, up to where Sndr is supposed to call the PlatformGateway's web server to deliver the generated MTs.
Having trouble communicating with the PlatformGateway program that's running directly on the host (not in a container like the rest.)

Let's try...
Run PlatformGateway (its web server using the same sndr.properties configuration, just listening for calls)
Run the Sndr container with bash:

    docker run -it --entrypoint /bin/bash burble-jvm:0.1.0

then use wget to post some data:

    wget --post-data 'wtf dude 2'  http://192.168.1.155:2424/mtReceive
    Connecting to 192.168.1.155:2424 (192.168.1.155:2424)
    saving to 'mtReceive'
    mtReceive            100% |********************************************************************************************|     2  0:00:00 ETA
    'mtReceive' saved

So as long as we got the url right, it should work.
Someone on stackoverflow suggested using 'host.docker.internal' as the host name for the container host. That actually works from
a bash shell inside a running container:
    wget --post-data 'wtf dude 2'  http://host.docker.internal:2424/mtReceive
    Connecting to host.docker.internal:2424 (192.168.65.254:2424)
    saving to 'mtReceive'
    mtReceive            100% |********************************************************************************************|     2  0:00:00 ETA
    'mtReceive' saved

Confirmed that the running program handled it:
    2025-01-09 15:58:12,929 INFO  [] [[0x67cf6f99 0x537fdca4] WebServer socket] c.e.PlatformGateway - Received content: wtf dude 2

When running PlatformGateway, it tells us
    2025-01-09 15:57:27,162 INFO  [] [start @default (/0.0.0.0:2424)] io.helidon.webserver.ServerListener - [0x67cf6f99] http://0.0.0.0:2424 bound for socket '@default'


RANDOM NOTE: https://eclipse.dev/openj9/ OpenJ9 JVM touts faster startup, less memory.

===== 2025-01-18 ======

ServiceAvailabilityTest is still broken due to some innocuous refactoring...

Sndr.init is called (and throws an exception) _after_ the test message are sent and received by the Rcvr:

    2025-01-14 15:54:43,187 INFO  [] [docker-java-stream-788704569] c.e.i.ServiceAvailabilityTest - STDERR: Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method<channel.close>(reply-code=406, reply-text=PRECONDITION_FAILED - inequivalent arg 'durable' for exchange 'test.mt' in vhost '/': received 'true' but current is 'false', class-id=40, method-id=10)

Both the Sndr and FakeOperator can set up the test.mt exchange. Both use the sndr.properties to set the durable property.

That said, even before this the FakeOperator throws almost the same exception for the test.mo queue:

    2025-01-18 13:43:10,542 INFO  [] [docker-java-stream-1817243090] c.e.i.ServiceAvailabilityTest - STDERR: Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method<channel.close>(reply-code=406, reply-text=PRECONDITION_FAILED - inequivalent arg 'durable' for exchange 'test.mo' in vhost '/': received 'true' but current is 'false', class-id=40, method-id=10)

The ordering may simply be an artifact of the logging setup that uses Slf4jLogConsumer to view the container output.

Pared it back to just the rabbit broker and rcvr.
Verified that the messages can be sent to rcvr and  with rabbitmqctl list_queues (from a shell inside the broker container) that they are received.

Next, turned off the Rcvr and turned on the FakeOperator:
    2025-01-19 10:17:04,408 INFO  [] [docker-java-stream-1832978947] c.e.i.ServiceAvailabilityTest - STDOUT: 2025-01-19 15:17:04.406636+00:00 [error] <0.774.0> operation basic.consume caused a channel exception not_found: no queue 'test.mo' in vhost '/'

 Looks like some configuration file errors were causing the trouble.
 Working now.

===== 2025-01-20 ======

The Maven test goal wasn't picking up my integration tests. This is because IntelliJ had auto-imported the older JUnit Test class/annotation instead of the newer Jupiter JUnit 5 version.

Now they run as expected though the Surefire output is lacking the nice formatted .html file. (The regular output are a bunch of text files, one for each test. Really not useful.)

Figured out how to make the surefire plugins (which are so freaking old and pull in so many additional dependencies) generate the readable HTML test report whenever the test goal is run.

Oh, also it appears that running the two container-based tests individually works but running them sequentially via Maven's test goal does not. The PlatformGateway instance isn't shutdown (and port released) before the next instance tries to start.

    2025-01-20 13:50:29,449 ERROR [ff0af9f3-e9de-446f-b242-e4506119c47a] [main] io.helidon.webserver.LoomServer - Failed to start listener: /0.0.0.0:2424
    java.util.concurrent.ExecutionException: java.io.UncheckedIOException: Failed to start server
    ...
    Caused by: java.net.BindException: Address already in use

Hmm, added a stop() method to PlatformGateway that calls through to the contained WebServer's stop() method.
Seems to have resolved the problem.

===== 2025-01-23 ======

On to the next test...testGatewayUnavailable

Added code to support the "restart" of the simulated platform gateway; requires creating a new instance of the web server.

Remembered that the current Sndr really doesn't support retrying, it always acks the message so they won't be delivered ever if the endpoint was missing when the message was received.

While researching the topic I came across:
    https://github.com/joshdevins/rabbitmq-ha-client (NOTE: this is a fairly old project)

 https://www.rabbitmq.com/docs/consumers#acknowledgement-modes covers the basics here with additional documents for the details.

 If a message to a given user is rejected/nacked for gateway availability reasons we want all subsequent messages to be held (not sent) until the first message has been delivered. This consideration is only for individual sessions. Other messages behave according to their sessions.

Side-note: tried integrating spotify's dockerfile-maven-plugin (https://github.com/spotify/dockerfile-maven/blob/master/docs/usage.md) into our pom.xml. Didn't work. Will need to circle around later. It's too simple to write a shell node that does it to spend the time now.


===== 2025-01-24 ======
Wrote rbc.sh to handle rebuilding container after making changes to the non-test code.
TODO
_ restructure Dockerfile-jvm to move the application code to the end; it's not caching layers effectively.
_ Clean up the design of PlatformGateway and HttpMTHandler.

Continuing work on testGatewayUnavailable...

When I added the methods to PlatformGateway to simulate throttling or availability I found that the broker would only requeue/retry the first (failed) message in very rapid succession. This is because the queue consumer declares:
    channel.basicQos(1);
The parameter is named "prefetchCount" which seems like an internally appropriate name but a bit confusing to the developer

===== 2025-01-24 ======

Setting channel.basicQos(3) will change the behavior such that the first three messages are retried/re-queued.
At least anecdotally, they are getting re-processed in the same order as they were put onto the queue.

The scenario we want to handle is, for a given user session with an ordered sequence of events, where the first message is rejected and re-queued the second message is attempted and succeeds. A subsequent delivery attempt for the first message may succeed but the damage is done.

In addition to a some logic to detect send errors due to the structure of a message (rather than a service outage or throttling situation) we need some session logic to prevent out-of-order messaging.

Added a GatewaySimStrategy that simply rejects the first message it sees to approximate the problem.
Calling the com.rabbitmq.client.Channel impl's basicQos() method with a value of 3 we see a final ordering in the current version testGatewayUnavailable:

    Messages received: [27 goodbye, 28 goodbye, 26 goodbye, 29 goodbye, 30 goodbye]

 Here it tries to send a batch of 3 messages. The first is re-queued but the second and third can be sent immediately.

RANDOM NOTE: When we start measuring perf we should make sure to only use System::nanoTime. Apparently the getMillis() is subject to adjustment such that it's possible for it to go backwards!
Also, check out the JMH Java microbenchmark harness: https://github.com/openjdk/jmh and

======== Initial Thoughts about the internals of a real Operator implementation ========
I've been considering whether it would make sense to use Records for this application. Most of the articles I've read don't really explain what domains Records might improve. They wave their hands about Data Transfer Objects which, almost always, feel like a terrible idea. Then I found the following https://blogs.oracle.com/javamagazine/post/records-come-to-java where the author notes:
"...the 'records are nominal tuples' design choice means you should expect that records will work best where you might use tuples in other languages. This includes use cases such as compound map keys or to simulate multi-return from a method. An example compound map key might look like this: record OrderPartition(CurrencyPair pair, Side side) {}"

This has some appeal. Immutable tuples pulled from APIs or similar data sources that can be composed easily and without loads of boilerplate. The compact constructor form also provides a nice place for fail-fast validation.

The Operator, unlike the Editor tools (that will follow), should not need to mutate any of the data it uses. Also Record Patterns (see https://docs.oracle.com/en/java/javase/22/language/record-patterns.html) could prove useful for the processing logic we need to write.

Along these lines and making the Java parts of the system fit well with the functional design of RabbitMQ and (possibly) Phoenix LiveView, consider reading https://www.oreilly.com/library/view/a-functional-approach/9781098109912/ which discusses ways to make Java more functional for the benefits that brings. The author has a blog at https://belief-driven-design.com/looking-at-java-21-switch-pattern-matching-14648/.


===== 2025-01-27 ======
Continuing to think about the session based ordering logic...

We need to add a notion of session groups and sequence order state to our (probably overly simplistic) message model.
Then, I'm thinking, we would reject/re-queue any message from a given session that is not the next expected message.

Start by assuming all the messages are from the same session?

Okay, implemented logic that seems to enforce ordered message delivery by a hardcoded session.
It's hideous looking--we should take a crack at revamping it, possibly with pattern matching--but appears to do what we need.
Let's create some additional variations of the problem to make sure it covers everything we can think of.

===== 2025-01-28 ======
To further test, error handling I need an actual session construct. The tests currently assume only a single session.

Where should this be constructed? In the Rcvr or the Operator?


===== 2025-01-29 ======
RANDOM NOTE: Auth0 has a pretty nice free tier with 25K users, 5 orgs, unlimited Okta & Social connections (not sure what that means), a custom domain with branded domains, and some DoS protection. No credit card. Seems cool.


Initial Domain Modelling for Operator:

Customer---hasMany--->User
|
+---hasMany---> Script

Both Customer and User have the same properties: countryCode, languageCode, brblId, platformId*
A Script is an interface with properties: id, next[] (returns a list of Script ids).
A Session is an interface with properties: id, currentNode, user (User)

An MT is an output from the execution of a Script in the context of a Session.
An MO is an input to the execution of a Script in the context of a Session.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Side-notes:
To address both the "insufficient resolution" and the back-in-time problem with System.currentTimeMillis() I found a class--NanoClock--the provides the wall-clock utility of currentTimeMillis and the better resolution of System.nanoTime() avoiding the impact host adjustments of the clock:
    https://github.com/jenetics/jenetics/blob/master/jenetics/src/main/java/io/jenetics/util/NanoClock.java

For testing purposes if/when we have ML support we could initiate a chat from the MO side and have the system respond. Validation would be weird though.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

===== 2025-01-30 ======
https://www.bandwidth.com/pricing/ shows some reasonable figures e.g. $0.004/message on a U.S. 10DLC.

Random Note of the Day: turso.tech created a fork of SQLite that includes accommodation for vector search--commonly used with RAG-based ML applications. This might be an interesting thing to use instead of spinning up a shared Postgres instance. The folks behind this also founded ScyllaDB. In an interview, Glauber Costa commented that SQLite's creators reserved implementation

===== 2025-02-01 ===
From https://openjdk.org/jeps/499: "Structured concurrency is an approach to concurrent programming that preserves the natural relationship between tasks and subtasks, which leads to more readable, maintainable, and reliable concurrent code. The term 'structured concurrency' was coined by Martin SÃºstrik and popularized by Nathaniel J. Smith. Ideas from other languages, such as Erlang's hierarchical supervisors, inform the design of error handling in structured concurrency."

Neat. Let's try it out. **Still** in preview for the upcoming Java 24 which is annoying but if the API is close to complete...


===== 2025-02-04 ===

https://www.allareacodes.com/canadian_area_codes.htm

Once I started using StructuredConcurrency/virtual thread preview features I had to add

        <configuration>
            <argLine>--enable-preview</argLine>
        </configuration>

to the maven-surefire-plugin. You can almost hear that old code creaking despite the fact that it continues to make new releases...

The built-in test runners were less pedantic. The IDE added the feature flag to the compiler automatically but not surefire.

One of the tests failed with an ExceptionInInitializer. This was enough to cause surefire (ironically) to fail to produce the surefire.html file.

com.enoughisasgoodasafeast.integration.EndToEndMessagingTest complained "Caused by: org.testcontainers.containers.ContainerLaunchException: Local Docker Compose exited abnormally with code 1 whilst running command: compose up -d"

It didn't fail a second time so we'll move on...

===== 2025-02-05 ===

https://www.rabbitmq.com/docs/shovel is interesting for operational purposes. It can move messages between queues.

https://www.rabbitmq.com/docs/publishers#concurrency drops a bomb on multiple threads writing to the same channel:
    "In general, publishing on a shared 'publishing context' (channel in AMQP 0-9-1, connection in STOMP, session in AMQP 1.0 and so on) should be avoided and considered unsafe."
    " Doing so can result in incorrect framing of data frames on the wire. That leads to connection closure."
NOTE: this applies to _publishing_ not consuming. Even so, it makes consider sticking with a single channel...

Looking at https://www.rabbitmq.com/docs/publishers#connection-recovery it appears that the Java client supports automatic recovery of connections and topology (queues, exchanges, bindings, and consumers) so maybe the heartbeat setup isn't something we need to explicitly handle. Perhaps it is enough to handle the cases where we receive a message and the queue is incommunicado.

Confirmed that if a queue comes back after being gone. The existing client/code will send the message quite happily.

Use a local temporary store to continue accepting messages? sqlite?

===== 2025-02-06 =====

Should we change Script to Page and create a new Script that represents a collection of Pages?
Or go the opposite direction and create a Book construct that represents a graph of Scripts?

Got a simple, state machine working with a test that attaches a chain of Scripts.

===== 2025-02-07 =====

Typically, we deal with directed, acyclic graphs but a Customer might want to engage in conversation that might loop around.
We can and probably should drop the acyclic part.
Our reporting metrics will want to be able to show numbers not just for how many users reached a particular node but also how they got there (the sequence of edges.)

There are libraries that provide more general graph functionality. For example, https://jgrapht.org/guide/UserOverview.
Question: do we need more functionality? I don't think so. At least not for the purposes of walking a User through a graph of Scripts.


===== 2025-02-08 =====

Thinking about the more complex logic processing needs, I think the Script record will need some additional data (e.g. the list of choices displayed in the previous node) as well as specialized logic. This could be done as a single object or the data and logic could be a separated into a structured string (as a new field to Script) and a static function that mapped to the Script's type.

If a single object the data would still need to be parsed at initialization time.

===== 2025-02-13 =====

Steve Carey kindly offered to provide a login to his sftp server (scarey.net) where I could put backups. Also, an Ubuntu container that I could use for load test or remote access, etc. What a mensch!
He needs a public key for this purpose.

~> ssh-keygen -t ed25519 -C "Key for scarey's sftp server/other services"
Generating public/private ed25519 key pair.
Enter file in which to save the key (/Users/mark/.ssh/id_ed25519):
Created directory '/Users/mark/.ssh'.
Enter passphrase (empty for no passphrase):
I left the passphrase empty because I've not been the best about remembering such things.

I emailed him the public key.

When I told him about this project and asked him about his thoughts on hosting it from home he sent a link to a thread on reddit that pointed to https://www.servethehome.com/introducing-project-tinyminimicro-home-lab-revolution/

This led me to looking at some other stuff including gitea.com which offers a free-for-open-source self-hosted version of their GitHub-like software.  GitLab, which Steve uses on his home lab, also offers a free version for "personal projects."

We're not quite at this point yet, I think, but useful for when we have a baseline ready.



===== 2025-02-16 =====

Sitting in Logan, waiting to get on a plane to Florida.

Next: replace the List<Script> field, "next" in the Script class with a SequencedSet<ResponseLogic>

Discovered an interesting wrinkle of sorts about Records with fields that are collections.
I was passing List.of() as a parameter to one of the constructors which resulted in the "next" field being an immutable collection.
In my canonical constructor I had logic that initializes the field with an empty, mutable ArrayList.
For the kind of objects graphs we're building we **have to be able** to add elements to the next list to be able to link objects together in a graph of Scripts.
Even then the order of object creation is difficult.

Created a OperatorTest.


===== 2025-02-19 =====
In Orlando...
The problem with EndToEndMessagingIT were due to changes in the host name not reflected in the rcvr.properties file. I'm not sure of the best way to programmatically deal with this.
Compounding the problem, the integration test container images are baked with the .properties files and jar inside. It would be better if we could use some of the dockerfile machinery to have it read those resources from the file system...

===== 2025-02-20 =====
Flying home.
I'm giving up for now on the integration tests. Neither of them are working now.
Both of the test runner plugins make the dubious choice of not rendering a report if there are failures. Not helpful. Maybe I should fork them and make my own.

At any rate, I added a buffer to the Session that will hold the messages produced in the course of an MOMessage event. This raises some questions about how to avoid concurrently modification of a Session and all it's bits, however.

===== 2025-02-26 =====
Sick for the last five days... ugh.

So, yeah, access to the Session likely needs to synchronized to avoid problems when there are 2+ messages received for the same instance. Further complication: if we use a shared cache (e.g. Redis) the object equality of two deserialized Sessions might be an issue.

This assumes that the callbacks from the rabbit client are

We might use the solution to this to help deal with the lack of thread-safety in the Rabbit channel used to add messages.

https://www.rabbitmq.com/client-libraries/java-api-guide#concurrency

They recommend pooling channels. There's a Spring-based connection pool impl, but I'd rather not bring in all that stuff.

There's a project that uses some of the Apache Commons' pooling framework: https://github.com/sytuacmdyh/amqp-client-pool

===== 2025-03-09 =====

The RabbitQueueConsumer needs to be refactored to remove the MTHandler. This is too hardwired to the FakeOperator and test setups we had before. MTConsumer, likewise, needs to be changed/removed.

===== 2025-03-10 =====
Completed the refactoring for RabbitQueueConsumer and its use by Operator.

Sndr is next...

===== 2025-03-11 =====

As I started work on the Sndr, I realized there wasn't a compelling reason to make MOMessage and MTMessage different classes. These have been merged.

Continue to explore how we want the node logic to work. Pivot and TopicSelection represent common cases we know from our previous experience.

Do we need the 'previous' field in the Script class? We could instead track previously executed Scripts (and navigate back to them) via a list on the Session. Removing previous would make constructing graphs of Scripts easier and less brittle. Detours and deviations from the initial chain of Scripts (because the User wants to talk about something else or asks to go to the top of the conversational tree) could then also be handled dynamically. In any case, we'll want to keep a history of a User's actual path through a conversation (the nodes they visited vs the Script graph that was constructed.)

< User input (short code)
> Present ("welcome"...ask question)
< User input (answer)
> Process (match answer)
> Present (acknowledge and ask another question)
< User input ("change topic")
> Process (no match, detect change topic)
     [optional]
        > Present (ask navigational question) [optional]
        < User input ("start new topic")
        > Process (match answer)
> Present (display new topics and ask for selection)
...


===== 2025-03-16 =====
https://www.rabbitmq.com/client-libraries/java-api-guide#concurrency:
    "When manual acknowledgements are used, it is important to consider what thread does the acknowledgement. If it's different from the thread that received the delivery (e.g. Consumer#handleDelivery delegated delivery handling to a different thread), acknowledging with the multiple parameter set to true is unsafe and will result in double-acknowledgements, and therefore a channel-level protocol exception that closes the channel. Acknowledging a single message at a time can be safe."

===== 2025-03-17 =====
Tachometer is an interesting plugin for Docker. It shows cpu and memory usage of containers in real-time.
Also, https://docs.fluentd.org/container-deployment/docker-logging-driver looks like it might be useful.

Actually..."NOTE: Currently, the Fluentd logging driver doesn't support sub-second precision." Oof! That might be a problem...

===== 2025-03-21 =====
TODO
_ Update OperatorMessageFlowIT to use testcontainers instead of assuming a running instance of RabbitMQ.
X Figure out how to structure the replacement for the generation of the top level topic node chain.
    --> just static functions in a class namespace?
X Update the docker files to point at jar's on the host filesystem.
_ Add build section to our docker-compose.


docker exec -it e016a29bccd69dde8e6a28d3c1dd35d84db8ae2ef7e1e0eeb79d6a114169b19c /bin/bash
wget --post-data=<message text> http://<host>:<port>/<pathInfo>


===== 2025-03-25 =====

With no additional networking config, I can use wget to reach the host machine from inside a bash shell running in the sndr container with either host.docker.internal or 192.168.1.155.
I cannot use localhost, however. Nor the 0.0.0.0 pseudo address used for listeners to attach to all interfaces.

Added a "reachability" test send in Sndr's main. This removes all the noise from the stack trace. Here we find that we're able to reach host.docker.internal but are receiving a 404 for some reason.

This led me to testing via the HttpMTHandler directly. Doing so confirmed where the problem was actually happening.
Looks like we were effectively specifying the pathInfo **twice** when making the POST...

Yes, confirmed. Annoyingly the HttpMTHandler code is littered with comments about its shitty state but it didn't occur to me that it was the problem. Argh....

The running thread is listed in the "Processed message:" log written by sndr-1's OperatorConsumer. We were wondering how many threads the Rabbit driver would use to deliver messages. Here's the munged log listing of counts and distinct thread ids:

   5 [pool-1-thread-10]
   1 [pool-1-thread-3]
   1 [pool-1-thread-4]
   2 [pool-1-thread-5]
   2 [pool-1-thread-6]
   1 [pool-1-thread-7]
   2 [pool-1-thread-8]
   2 [pool-1-thread-9]

NOTE: 1 of the occurrences is for the "handleConsumeOk called with consumerTag" log. The actual total here is 15 because we sent the batch of 5 three times. This is only anecdotal obviously but also informative. Note: the Macbook we're using has 8 cores.
 Also anecdotal, the log indicate the messages were processed in order.


===== 2025-03-26 =====

Noticed that the list_queues command for rabbitmqctl shows four entries. Two are expected (test.mo and test.mt) but for the other pair are, according to my google research, the names used are random strings prefixed by "amq.gen-".

The total system makes four Connections (1 each for Sndr and Rcvr and 2 for Operator.) These are all managed by the two classes, RabbitQueueProducer and RabbitQueueConsumer which always (I think) pass in names for the queue. So where are these extras coming from?

Also, the message counts list_queues displays suggest that the messages sent to each of intended queues--test.mo and test.mt--are also sent to the anonymous queues, as if they were paired with the former.

Hmm, in the logs I'm noticing this:
brkr-1      | 2025-03-25 14:51:36.300810+00:00 [info] <0.652.0> connection 172.18.0.1:63666 -> 172.18.0.2:5672: user 'guest' authenticated and granted access to vhost '/'
operator-1  | 2025-03-25 10:51:36,319 INFO  [] [main] c.e.RabbitQueueConsumer - AMQP.Queue.DeclareOk: queue=amq.gen-7cnH4S2ocnwNzTSHZb1wvw consumerCount=0 messageCount=0
operator-1  | 2025-03-25 10:51:36,321 INFO  [] [main] c.e.RabbitQueueConsumer - AMQP.Queue.BindOk: protocolClassId=50 protocolMethodId=21 protocolMethodName=queue.bind-ok
operator-1  | 2025-03-25 10:51:36,324 WARN  [] [pool-1-thread-3] c.e.OperatorConsumer - handleConsumeOk called with consumerTag amq.ctag-RU77QLZAZiDGLk1UYHB3dQ

Resolved the problem. We weren't using the queueName to create the queue. Doh.


===== 2025-03-28 =====

https://medium.com/@skillcate/sentiment-analysis-using-nltk-vader-98f67f2e6130 talks about using VADER.
https://www.youtube.com/watch?v=szczpgOEdXs includes link to code using BERT and PyTorch for sentiment analysis. Includes params for using Nvidia CUDA support. The model used includes support for Spanish though I suspect it's European Spanish. English support, too, of course.

Note, the downloads of the pretrained models require 600-700mb. This is a lot less than the Llama models (measured in hundreds of gigabytes.)


To handle multiple responses to the same question (think Holy Grail's "blue..no green!")
in the response evaluation logic handle add an error check that considers whether the user input matches one of the responses in the previously presented question. If it does it may be that they're trying to change their answer. In those cases, ask for confirmation. If confirmed, rewind to that question prefixed by a "Did you want to change your answer?". When we get to the stage of turning message history into statistics we'll need to be able to address these response changes.

TODO
_ Update OperatorMessageFlowIT to use testcontainers instead of assuming a running instance of RabbitMQ.
X Add build section to our docker-compose? I *think* this is something for later actually...

My updated test isn't receiving the input message in the named queue. The caller doesn't complain at all but the Rabbit broker log shows:


2025-03-30 11:32:52 2025-03-30 15:32:52.232928+00:00 [error] <0.1665.0> Channel error on connection <0.1656.0> (172.17.0.1:55644 -> 172.17.0.2:5672, vhost: '/', user: 'guest'), channel 1:
2025-03-30 11:32:52 2025-03-30 15:32:52.232928+00:00 [error] <0.1665.0> operation basic.publish caused a channel exception not_found: no exchange 'test.mo' in vhost '/'

Ah, okay. So we broke this when we addressed the machine generated queue name issue. See RabbitQueueFunctions.exchangeForQueueName().
Reviewing their documentation, I realized that the argument passed to channel.basicPublish() is the **exchangeName**, not the queueName. This didn't matter before because these names were the same.

Side note: while working on this, the power to the house was shut off. During that period, running one of the tests, threw "java.net.SocketException: Network is unreachable." My interpretation is that using IP addresses required a functioning router even though they were all effectively local.

Still some troubles here...running each of the test in OperatorMessageFlowIT individually via IDEA works fine but running the test class(also via IDEA) does not. Three of the four tests fail waiting for output to arrive in the InMemoryQueueProducer.

Executing mvn verify (the goal that runs integration tests) fails for three of the tests as well but this time because of a java.lang.UnsupportedClassVersionError.
Adding the same configuration.argLine we use for the surefire plugin to the failsafe config in pom.xml solves this:

    <configuration>
        <argLine>--enable-preview</argLine>
    </configuration>

Now we see the same issue when using IDEA to run all the tests in the class:
    org.awaitility.core.ConditionTimeoutException: Condition with Lambda expression in com.enoughisasgoodasafeast.integration.OperatorMessageFlowIT was not fulfilled within 5 seconds.

Running the OperatorMessageFlowIT class with just a single test (commenting out the @Test annotation for the others) works via IDEA and mvn verify. Something to do with state?

===== 2025-04-01 =====

Debugging...

with two connection setups in the same test method, the first succeeds and the second fails.

Paused the latter after the message was sent to the queue, the output from rabbitmqctl list_queues shows zero entries in test.mo.
So is the problem in the producer rather than the consumer?

It appears that the problem stemmed from the first Rabbit producer and consumer clients still being connected to the broker.
So, at least for the purposes of unit/integration tests, I've added shutdown() methods to the QueueProducer and QueueConsumer interfaces and the Operator class. These call close() on the Channel and Connection instances. Unlike some supported Rabbit types (topics, streams, etc) a queue doesn't deliver messages to multiple consumers. By contrast, having multiple producers is fine.

===== 2025-04-03 =====
Revisited EndToEndMessagingIT and managed to get it working. One caveat, running it repeatedly often fails with this:
    "org.testcontainers.containers.ContainerLaunchException: Local Docker Compose exited abnormally with code 1 whilst running command: compose up -d"
The failure appears to happen after it's created the network and all four containers while it's starting the RabbitMQ container (brkr).
The start operation appears to have succeeded, however. I can shell into it and run rabbitmqctl commands. The failure triggers the teardown of the other containers but the brkr container continues to run. So really not ideal from a test automation/continuous integration perspective.

The Testcontainers team seems to reluctantly support the use of docker compose. So maybe this is the best I can expect. :-{

===== 2025-04-07 =====

While debugging ServiceAvailabilityIT I was trying to examine the burble-jvm image to see why it could find the jar file.

Finally figured out how to run the image without having it immediately exit. Here's how:

    docker run -it --rm --name rcvr --entrypoint "sh" burble-jvm:0.1.0

The jar file *does* exist per the COPY command in Dockerfile.jvm.

    /opt/app # ls -la /opt/app/sndrRcvr-1.0-SNAPSHOT-jar-with-dependencies.jar
    -rw-r--r-- 1 root root 4668198 Apr  5 20:59 /opt/app/sndrRcvr-1.0-SNAPSHOT-jar-with-dependencies.jar

Is it a permission problem? Doesn't seem like it. I can run the entrypoint and cmd from the shell without issue. Provided there's a RabbitMQ container running, all the producers and consumers are able to connect.

Running it manually, the container works as expected, connecting to the broker without complaint.

Ah. The problem was that our dev version of the Dockerfile (Dockerfile.dev.jvm) doesn't copy the jar file into the image. It expects the docker run command will include a volume mapping of /Users/mark/Development/sndrRcvr on the host to /opt/app on the container.


===== 2025-04-10 =====

Time to create some tools for interacting with a running system.

Picocli and jline3 look to be useful and lightweight building blocks for an interactive CLI.
Use an extended version of PlatformGateway.



1) create persona
    > input telephone number
    > input shortcode
    < display newly created persona
2) list personas
    < display all available personas with id number (write these to a serialized file that can be loaded at start up)
3) send message (initially just a send but being able to queue messages and send as a batch will be useful.)
    > display "Which persona will send the message?" then list available personas by id
    > input selected id
    < display selected persona then prompt "Enter the text of the message:"
    > input message text
    < display confirmation dialog "Send message? y/n"
    > input choice (if yes, send message)
    > wait for response (5 seconds) and display it. Otherwise "No response received."
4) exit

Take a param (-l) to signal desire to load any serialized files found in a special directory.



===== 2025-04-16 =====

brblcli is working w/r/t to sending messages and triggering node chains. The hardcoded node for short code "45678" will correctly
track the state for the user.

Lots of work yet to make the CLI easy to use but a good start.

Made some additional changes to the Session:
    - Changed the inputHistory implementation in Session to be fixed size.
    - Added requirement that constructor params be non null and started a unit test for the class.


===== 2025-04-17 =====

Starting on the persistence layer...
Considered SQLite but decided to go with Postgres.

Simplest task is to record receipt of MOs and transmission of MTs.
Ideally we could use the basic records but we'd like to relate them to the Script state.

User  <---:fk:--- Profile

UUID id                           --> String or UUID
Map<Platform, String> platformIds --> punt on this for now?
String countryCode                --> CREATE TYPE country_code AS ENUM ('us', 'ca', 'mx');
List<String> languages)           --> CREATE TYPE language_code AS ENUM ('en', 'es', 'fr', ...);

Installing Postgres...

Used the postgres.app version which is quite permissive with its default pg_hba.conf settings but seems pretty simple.

Learned about server-side prepared statements:
    https://jdbc.postgresql.org/documentation/server-prepare/#server-prepared-statements
Probably a good idea, if we use them, to also set on the connection:
    autosave=conservative
This to avoid having to bounce the server if/when the query plan is changed. There is, however, a warning about the performance impact on long transactions with (auto)save points.

Let's comment it out and wait till we have the code worked out before trying it out.

Created a properly named database:
    brbl_dev

===== 2025-04-17 =====

psql -U mark -d brbl_dev -a -f <ddl-or-dml>.sql

See dev.ddl in src/main/resources/sql.

Note "timestamp with time zone" is, perhaps surprisingly, the right data type for a Java Instant value:
    See https://wiki.postgresql.org/wiki/Don't_Do_This#Don.27t_use_timestamp_.28without_time_zone.29_to_store_UTC_times

Using psql to show all tables
 \dt *.*

Using psql to show all users
 \du

java.time.Instance supports nanos --> java.sql.Timestamp --> Postgres

Gemini suggests using the timestamp9 extension instead of separate fields for the
CREATE EXTENSION timestamp9;

mark@yakko sndrRcvr % brew info postgresql@17
==> postgresql@17: stable 17.2 (bottled) [keg-only]
Object-relational database system
https://www.postgresql.org/
Not installed
From: https://github.com/Homebrew/homebrew-core/blob/HEAD/Formula/p/postgresql@17.rb
License: PostgreSQL
==> Dependencies
Build: docbook âœ˜, docbook-xsl âœ˜, gettext âœ˜, pkgconf âœ˜
Required: icu4c@76 âœ˜, krb5 âœ˜, lz4 âœ”, openssl@3 âœ”, readline âœ˜, zstd âœ”, gettext âœ˜
==> Caveats
This formula has created a default database cluster with:
  initdb --locale=C -E UTF-8 /opt/homebrew/var/postgresql@17

When uninstalling, some dead symlinks are left behind so you may want to run:
  brew cleanup --prune-prefix

postgresql@17 is keg-only, which means it was not symlinked into /opt/homebrew,
because this is an alternate version of another formula.

To start postgresql@17 now and restart at login:
  brew services start postgresql@17
Or, if you don't want/need a background service you can just run:
  LC_ALL="C" /opt/homebrew/opt/postgresql@17/bin/postgres -D /opt/homebrew/var/postgresql@17
==> Analytics
install: 6,954 (30 days), 21,611 (90 days), 36,400 (365 days)
install-on-request: 6,307 (30 days), 19,641 (90 days), 32,337 (365 days)
build-error: 23 (30 days)

After installing the brew package and starting the server I can connect via:
    psql -d postgres

This connects as user 'mark' to the only non-template database available, 'postgres'. Not confusing at all.
I'm used to 'postgres' being the db super user but here it's 'mark'

Creating a table with a column of type 'timestamp with time zone' looks like this:

    postgres=# insert into test_time(received_at_ms) values(NOW());
    INSERT 0 1
    postgres=# select * from test_time ;
            received_at_ms
    -------------------------------
     2025-04-22 09:28:49.010806-04
    (1 row)

This only gets us to the milliseconds level.

Google's Gemini suggests using time9, a Postgres extension, as a way of supporting nanosecond level timestamps. This instead of splitting the milliseconds and nanosecond components into two columns.

The extension isn't part of the regular distro and requires compiling the code here (https://github.com/optiver/timestamp9)

Needed to install cmake which is very dumb about checking it's version requiring me to hack the timestamp9's CMakeLists.txt file.

...and also the c.h file to comment out references to ENABLE_NLS. I tried #undef'ing it at the top of the file as well as setting

    #define ENABLE_NLS 0

in pg_config.h but neither was sufficient. At first blush, it doesn't seem to be something that will impact Brbl's functionality.

mark@yakko build % sudo make install

Password:
[  0%] Built target controlfile
[ 66%] Built target timestamp9
[100%] Built target sqlfile
[100%] Built target sqlupdatescripts
Install the project...
-- Install configuration: "Release"
-- Installing: /opt/homebrew/lib/postgresql@17/timestamp9.so
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.4.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--0.1.0--0.2.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--0.2.0--0.3.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--0.3.0--1.0.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.0.0--1.0.1.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.0.1--1.1.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.1.0--1.2.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.2.0--1.3.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.3.0--1.4.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9.control

Restarted the database and connected via psql as superuser (mark) then ran

    CREATE EXTENSION timestamp9;

which spits out:

    ERROR:  could not access file "$libdir/timestamp9": No such file or directory

Ah, the produced library file is named with linux convention with the .so extension.

mark@yakko timestamp9 % file /opt/homebrew/lib/postgresql@17/timestamp9.so
/opt/homebrew/lib/postgresql@17/timestamp9.so: Mach-O 64-bit bundle arm64

So just rename the file? Apparently so...

    postgres=# CREATE EXTENSION timestamp9;
    CREATE EXTENSION

When declaring a column of this type use timestamp9 leaving off the "WITH TIME ZONE" used with the regular TIMESTAMP type.

Hmm... populating a timestamp9 column via JDBC with Timestamp.from(message.receivedAt()) compared to one with using "timestamp with time zone" -->

    timestamp with time zone      |   timestamp9
    ------------------------------|------------------------------------
    2025-04-23 22:03:03.177013-04 | 2025-04-23 22:03:03.177013000 -0400

    It seems like it's just padding the same number by 1000.

Printing the numbers as jdk is running the Message creation code:
    message.receivedAt() ==> 2025-04-24T02:03:03.177013083Z
    message.receivedAt().getEpochSecond()    ==> 177013083
    message.receivedAt().getNano()           ==> 1745460183



See PersistenceManager.instantInvestigation

nanoclock produced instant...
Instant:        2025-04-24T22:08:02.454020250Z --> year-month-day-hours-minutes-seconds-nanoseconds
    --> The last part of this is 9 digits
toEpochMilli:   1745532482454
    --> plug this into https://www.epochconverter.com/ --> number of milliseconds since epoch. Nanos are divided by 1 million so only the leading three digits (of a 9 digit number) are added as milliseconds. As the method name suggests its resolution is only ms.
getEpochSecond: 1745532482    --> Just the number of seconds since the epoch.
getNano:        454020250   --> the nanos that are added
instant converted to timestamp...
as timestamp time: 1745532482454
as timestamp nanos: 454020250
timestamp converted back to instant
Instant:        2025-04-24T22:08:02.454020250Z --> year-month-day-hours-minutes-seconds-nanoseconds
toEpochMilli:   1745532482454
getEpochSecond: 1745532482
getNano:        454020250

Original instant equal to round tripped instant? true

The complete timestamp expressed in nanoseconds would be:
    1745532482454020150 (19 digits)
    9223372036854775807 is the max positive value of a Java long.
Checking and comparing the values while converting from Instant to Timestamp...looks good. Nothing getting dropped.

It seems like the JDBC driver is truncating all but the leading three places of nanoseconds from the java.sql.Timestamp value when it writes the value to the column. This makes sense if the db's timestamp type limits resolution to microseconds.
Also, weirdly, the timezone offset goes from two digits to four. For example, Eastern Daylight Savings Time it becomes -0400 instead of -04 when psql displays the column value.

We wanted to get better than the thousandth of a second resolution of milliseconds (that means 0-999) but regular Instant/Timestamp stored in a Timestamp column in Postgres gets us another three places of resolution that seems to be equivalent to microseconds. Perhaps this is enough? It would eliminate the need to overcome the JDBC and display issues using the timestamp9 data type.

Note: I am thinking/writing this on the train back from NYC and I'm super tired so totally possible I'm getting it all wrong.


===== 2025-04-27 =====

https://www.postgresql.org/docs/current/datatype-datetime.html confirms that the timestamp data type supports microseconds. So our observations were correct.

This is enough for our purposes and removes the need for non-standard type extensions. Moving on...

Thinking about where, in the pipeline, we write the incoming MO...
We want to be able to associate the Message with the Script element that was used to process it.
Likewise we want to associate the outgoing MT with the Script that generated it.
So how about ...

For each incoming Message:
    - Rcvr writes the full Message with its UUID, the timestamp, and the rest. Before or after enqueuing?
            CREATE TABLE brbl_logs.messages_mo (
                id          UUID NOT NULL,
                rcvd_at     TIMESTAMP WITH TIME ZONE NOT NULL,
                _from       VARCHAR(15) NOT NULL,
                _to         VARCHAR(15) NOT NULL,   --> how long are non-SMS (WhatsApp, FB Messenger, etc) identifiers?
                _text       VARCHAR(2000) NOT NULL  --> should match the MT length
            );
    - Operator writes an abbreviated row with just the UUID of the Message, the Session UUID, and the UUID of the handling node.
            CREATE TABLE brbl_logs.messages_mo_prcd (
                id          UUID NOT NULL,
                prcd_at     TIMESTAMP WITH TIME ZONE NOT NULL,
                session_id  UUID NOT NULL,
                script_id   UUID NOT NULL
            );

            To join the two, showing including messages that were received but not processed:
            SELECT
                rcvd.id, prcd.session_id, prcd.script_id, rcvd._text
            FROM
                brbl_logs.messages_mo AS rcvd
            LEFT JOIN
                brbl_logs.messages_mo_prcd AS prcd
            ON rcvd.id = prcd.id;

For each outgoing Message:
    - Operator writes the full Message immediately after enqueuing it.

    CREATE TABLE brbl_logs.messages_mt (
        id          UUID NOT NULL,
        sent_at     TIMESTAMP WITH TIME ZONE NOT NULL,
        _from       VARCHAR(15) NOT NULL,
        _to         VARCHAR(15) NOT NULL,   --> how long are non-SMS (WhatsApp, FB Messenger, etc) identifiers?
        _text       VARCHAR(2000) NOT NULL,  --> should match the MO length
        session_id  UUID NOT NULL,
        script_id   UUID NOT NULL
    );

For each delivered Message acked by the 3rd party gateway:
    - Sndr writes an abbreviated row with just the UUID and the delivery timestamp.

    CREATE TABLE brbl_logs.messages_mt_dlvr (
        id          UUID NOT NULL,
        dlvr_at     TIMESTAMP WITH TIME ZONE NOT NULL
    );

===== 2025-04-29 ====
Finished the insert methods for all four record types.

In tests, inserting two records (brbl_logs.messages_mo and brbl_logs.messages_mo_prcd) appears to take a whopping 112 milliseconds.

delete from brbl_logs.messages_mo;
delete from brbl_logs.messages_mo_prcd;
delete from brbl_logs.messages_mt;
delete from brbl_logs.messages_mt_dlvr ;

select prcd_at - rcvd_at from brbl_logs.messages_mo mos inner join brbl_logs.messages_mo_prcd prcd on mos.id = prcd.id;
select dlvr_at - sent_at from brbl_logs.messages_mt mts inner join brbl_logs.messages_mt_dlvr dlvr on mts.id = dlvr.id;

No doubt creating a brand new Connection object for each isn't helping to make this quick.

===== 2025-04-29 ====

Plugged in a c3p0 connection pool. This helps reduce the insert times especially if I initialize it before running the timed tests.
The init call, by itself, took 117 ms to setup 5 connections.
But subsequent calls to our fetchConnection method return in microseconds.

The batch mode insert functions seem to be a good bit slower (5ms) than the single inserts.
Is this because they are writing more data?
Let's try removing the batch setup...helps a little.
Looking at the times for the insert of the two records we'll do for each and every MO:
    insertMO: b 2025-04-30T14:22:50.188750583Z a 2025-04-30T14:22:50.192991833Z: d PT0.00424125S
        ~> 4.2 ms
    insertProcessedMO: b 2025-04-30T14:22:50.194844917Z a 2025-04-30T14:22:50.195541667Z: d PT0.00069675S
        ~> 0.6 ms
And for MTs...
    insertMT: b 2025-04-30T14:22:50.195807458Z a 2025-04-30T14:22:50.196264708Z: d PT0.00045725S
        ~> 0.4 ms
    insertDeliveredMT: b 2025-04-30T14:22:50.196432250Z a 2025-04-30T14:22:50.197066292Z: d PT0.000634042S
        ~> 0.6 ms
Okay, so not terrible anymore. To be fair, my testing is really not all that meaningful since it is unrealistically optimized (no other overhead from related application code, database running on localhost) and, at the same time, not ideal (no JVM warmup.) Its quite terrible even as seat-of-the-pants micro benchmarks go.


===== 2025-04-30 ====

Cleaned up and pushed the new PersistenceManager class.


===== 2025-05-01 ====

Integrating PersistenceManager with Rcvr...

Note: With the Postgres server running locally we need to use host.docker.internal as the hostname for the database connection pool.

Ok, got the MO write working in Rcvr.


===== 2025-05-06 ====

Processed MO, MT, and Delivered MT writes now working...

SELECT prcd_at - rcvd_at FROM brbl_logs.messages_mo mos INNER JOIN brbl_logs.messages_mo_prcd prcd ON mos.id = prcd.id;
SELECT dlvr_at - sent_at FROM brbl_logs.messages_mt mts INNER JOIN brbl_logs.messages_mt_dlvr dlvr ON mts.id = dlvr.id;

SELECT
    mos.id, mts.id, mts._text
FROM
    brbl_logs.messages_mo_prcd mos
INNER
    join brbl_logs.messages_mt mts
ON
    mos.session_id = mts.session_id;

To calculate the elapsed time to receive, process and respond to an MO:

    SELECT (dlv.dlvr_at - mos.rcvd_at) AS total_response_time
    FROM
        brbl_logs.messages_mo mos
    INNER JOIN
        brbl_logs.messages_mo_prcd prc ON mos.id = prc.id
    INNER JOIN
        brbl_logs.messages_mt mts ON prc.script_id = mts.script_id
    INNER JOIN
        brbl_logs.messages_mt_dlvr dlv ON mts.id = dlv.id
;

For the 5 messages we send from PlatformGateway the results look something like:

total_response_time
---------------------
 00:00:00.274_010
 00:00:00.210_633
 00:00:00.203_084
 00:00:00.197_272
 00:00:00.195_147
(5 rows)

Run a second time, the numbers improve a bit...

 total_response_time
---------------------
 00:00:00.108_010
 00:00:00.067_296
 00:00:00.052_973
 00:00:00.046_465
 00:00:00.044_713

Not neck snapping performance for individual message processing but the concurrency numbers are better.

The inter-message delta for the rcvd time shrinks dramatically proving the value of the database connection pool and threaded http server:

    1)  2025-05-06 09:02:08.063024-04
    2)  2025-05-06 09:02:08.134534-04 --> #2 - #1 = 128_510
    3)  2025-05-06 09:02:08.145519-04 --> #3 - #2 = 010_985
    4)  2025-05-06 09:02:08.154437-04 --> #4 - #3 = 008_918
    5)  2025-05-06 09:02:08.159682-04 --> #5 - #4 = 005_245

We should revisit implementing a RabbitMQ producer channel pool at some point...

Delta between MT delivered and MO processed time:

    SELECT (dlv.dlvr_at - prc.prcd_at) AS total_response_time
    FROM
        brbl_logs.messages_mo_prcd prc
    INNER JOIN
        brbl_logs.messages_mt mts ON prc.script_id = mts.script_id
    INNER JOIN
        brbl_logs.messages_mt_dlvr dlv ON mts.id = dlv.id
    ;

 total_response_time
---------------------
 00:00:00.133810
 00:00:00.136067
 00:00:00.134400
 00:00:00.130744
 00:00:00.129300

     SELECT (dlv.dlvr_at - mts.sent_at) AS mt_queued_to_delivered
     FROM
         brbl_logs.messages_mt mts
     INNER JOIN
         brbl_logs.messages_mt_dlvr dlv
     ON mts.id = dlv.id
     ;

 mt_queued_to_delivered
------------------------
 00:00:00.166_193
 00:00:00.138_192
 00:00:00.136_617
 00:00:00.131_253
 00:00:00.133_792


===== 2025-05-07 ====

Working on the models for Users, Profiles.

See dev.ddl.

How to encode languages and indicate their order of preference?

Just a string of the two character language codes separated by commas? e.g. "ES,EN" or "FR" where list order indicates preference. We wouldn't really need commas given that language code.

The case for using ISO-3 codes for language: 'zh' is the two character code for Chinese but there are two distinct such languages: Cantonese (yue) and Mandarin (cmn).

Changed our existing language support to use the three character codes.

Debating where to place the language and country code. We need the former when starting new conversations. The latter seems less critical but if scripts reference the user's nickname (for general friendliness) then we'd end up making a 1:1 join with the profile table. I guess language seems more like a cross-cutting concern where all the User records for a given user would want to present the same.
On the other hand, if the user is anonymous (no profile) we still need to know what language to use with them. Likewise, to know what legal rules we have to follow we need their country code. So, alas, we need both in the User record.

It's unclear whether we need/benefit from a Session table.


===== 2025-05-12 ====

Got the User/Profile models working. When running the regular docker compose setup with PlatformGateway sending a batch of messages through I realized that the database connections needed read/write access to both the brbl_logs and brbl_users schemas.

Restructured the roles for our two schemas so that we separate privilege definitions from the "users" (really just roles login) that possess them.

===== 2025-05-15 ====

CREATE SCHEMA IF NOT EXISTS brbl_logic AUTHORIZATION brbl_admin ;

CREATE TABLE brbl_logic.nodes (
    id          UUID PRIMARY KEY,
    created_at  TIMESTAMP WITH TIME ZONE NOT NULL,
    text        VARCHAR(255),       --> SMS is limited to 160 chars but other platform have higher limits.
    type        SMALLINT NOT NULL,  --> see ScriptType enum for meaning.
    label       VARCHAR(32)         --> the name given to the node element in a UI
);

CREATE TABLE brbl_logic.edges (
    id              UUID PRIMARY KEY,
    created_at      TIMESTAMP WITH TIME ZONE NOT NULL,
    match_text      VARCHAR(128),       --> the text that must be matched to direct the conversation to the dst node
    response_text   VARCHAR(255),       --> the text emitted when the edge is selected.
    src             UUID NOT NULL,      --> FK to scripts table
    dst             UUID NOT NULL      --> FK to scripts table
    -- CONSTRAINT fk_script_src
    --     FOREIGN KEY(id) REFERENCES brbl_logic.nodes(id),
    -- CONSTRAINT fk_script_dst
    --     FOREIGN KEY(id) REFERENCES brbl_logic.nodes(id)
);

-- Script 1
INSERT INTO brbl_logic.nodes VALUES('89eddcb8-7fe5-4cd1-b18b-78858f0789fb', NOW(),
    'What is your favorite color? 1) red 2) blue 3) flort', 4, 'ColorQuiz') RETURNING *;
-- Script 2
INSERT INTO brbl_logic.nodes VALUES('2ed4ceed-a229-4e82-ab89-668a15835058', NOW(),
    'Oops, that is not one of the options. Try again with one of the listed numbers or say "change topic" to start talking about something else.', 5, 'EvaluateColorAnswer') RETURNING *;
-- Script 3
INSERT INTO brbl_logic.nodes VALUES('f9420f0c-81ca-4f9a-b1d4-7e25fd280399', NOW(),
    'That is all. Bye.', 1, 'EndOfConversation') RETURNING *;

INSERT INTO brbl_logic.edges VALUES('d9d9d89b-3047-4b18-8c97-5fb870fc1ced', NOW(), '1|red', 'Red is the color of life.',
    '89eddcb8-7fe5-4cd1-b18b-78858f0789fb', 'f9420f0c-81ca-4f9a-b1d4-7e25fd280399') RETURNING *;
INSERT INTO brbl_logic.edges VALUES('fee09a2a-5595-43a9-8228-72182789800e', NOW(), '2|blue', 'Blue is my fave, as well.',
    '89eddcb8-7fe5-4cd1-b18b-78858f0789fb', 'f9420f0c-81ca-4f9a-b1d4-7e25fd280399') RETURNING *;
INSERT INTO brbl_logic.edges VALUES('49a0e06a-fff6-4bbc-91f7-fcda4b800cc4', NOW(), '3|flort', 'Flort is for the cool kids.',
    '89eddcb8-7fe5-4cd1-b18b-78858f0789fb', 'f9420f0c-81ca-4f9a-b1d4-7e25fd280399') RETURNING *;

NOTE: gen_random_uuid() is a useful function available in psql.

Next question: how to escape single quotation marks? Likely JDBC already handles this but useful to know when using psql.


===== 2025-05-16 ====

Found an ad for Twilio while reading stackoverflow. Led me down the rabbit hole of SMS providers.
Some leads on low-cost SMS here: https://www.reddit.com/r/rails/comments/175e0fk/which_provider_for_sending_sms_messages_is_the/
Remember to learn more about the differences between GSM-7 and UCS-2 character sets since we'd like to support non-English languages.
Climbing out before it drives me mad...

Work on node-related schema moved to dev.ddl.

The design seems plausible for basic interactions. Will it be open enough to support other things? What are those other things?

Useful queries:
    SELECT id,created_at,type,label,SUBSTR(text,0,40) FROM brbl_logic.nodes;
    SELECT created_at, match_text,response_text, src, dst FROM brbl_logic.edges ORDER BY src;

Keyword mapping:
To find the initial node node for a new session...

CREATE TABLE brbl_logic.keywords (
    id          UUID NOT NULL UNIQUE,
    pattern     VARCHAR(128),   --> the default entry likely has no value so this can't be NOT NULL
    platform    platform,
    script_id   UUID,
    is_default  BOOLEAN DEFAULT FALSE,
    CONSTRAINT fk_script_id
        FOREIGN KEY(id)
            REFERENCES brbl_logic.nodes(id),
    CONSTRAINT unique_pattern_platform
        UNIQUE(pattern, platform)_
);

https://www.postgresql.org/docs/current/sql-createtable.html has some interesting notes about column storage and compression options which might be useful if node text gets large.

===== 2025-05-16 ====

Reading https://stackoverflow.com/questions/54907495/postgresql-recursive-parent-child-query led me to writing the following:

WITH RECURSIVE c AS (
    SELECT <SCRIPT_TABLE_ID>::UUID AS script_id
    UNION ALL
    SELECT e.dst
    FROM brbl_logic.edges AS e
        JOIN c ON (c.script_id = e.src)
)
SELECT
    s.id, s.label, s.text,
    e.match_text, e.response_text, e.dst
FROM
    brbl_logic.nodes s
INNER JOIN
    brbl_logic.edges e ON s.id = e.src
WHERE
    s.id IN (SELECT DISTINCT(c.script_id) FROM c);

The use of the recursive "common table expression" to pull all the data we're looking for recursively starting with the id of the Script, <SCRIPT_TABLE_ID>
 The Stackoverflow post also suggests a stored procedure that does pretty much the same thing --> https://stackoverflow.com/a/54909559/3524850

http://explain.dalibo.com offers an interesting Postgres EXPLAIN plan analyzer. Might be useful for performance tuning.

The "only" trouble with our recursive query (above) is that the terminal node node doesn't get included (because there are no edges that reference it as the src. The code written to turn the ResultSet into a graph can patch up the temporarily missing references but not that one.

Do we want to support terminating scripts? I think so. The query results won't include the final node because it doesn't have any edges.

To address that problem maybe define a single edge that itself has a null dst? This works to include the final node in the result. But this makes me think...

To support cycles in the conversational graph--to allow loops--we need a different solution unless we can do it programmatically with the same data. The problem is that cycles will blow up the (now endless) recursive CTE graph query (taking the database along with it.)


===== 2025-05-23 ====

Still thinking about a way to support cycles in the conversational graph that will still provide queryable

Reading https://www.postgresql.org/docs/current/queries-with.html#QUERIES-WITH-CYCLE presents a possible solution using the built-in CYCLE syntax:

Example from the Postgres documentation:

WITH RECURSIVE search_graph(id, link, data, depth) AS (
    SELECT g.id, g.link, g.data, 1
    FROM graph g
  UNION ALL
    SELECT g.id, g.link, g.data, sg.depth + 1
    FROM graph g, search_graph sg
    WHERE g.id = sg.link
) CYCLE id SET is_cycle USING path
SELECT * FROM search_graph;

Let's try:

WITH RECURSIVE cte AS (
        SELECT '89eddcb8-7fe5-4cd1-b18b-78858f0789fb'::UUID AS script_id
        UNION ALL
        SELECT e.dst
        FROM brbl_logic.edges AS e
            JOIN cte ON (cte.script_id = e.src)
    ) CYCLE script_id SET is_cycle USING path
    SELECT
        s.id, s.created_at, s.text, s.type, s.label,
        e.id, e.created_at, e.match_text, e.response_text, e.src, e.dst
    FROM
        brbl_logic.nodes s
    INNER JOIN
        brbl_logic.edges e ON s.id = e.src
    WHERE
        s.id IN (SELECT DISTINCT(cte.script_id) FROM cte);

This executes without complaint. Let's add a edge that creates a cycle and see if it does what we want...

UPDATE brbl_logic.edges e
    SET dst = '89eddcb8-7fe5-4cd1-b18b-78858f0789fb'
    WHERE e.id = '4bce9e23-2dc4-42d9-aea9-94744a74d005';

which changes the final edge in the graph to point at the first node.
Re-running the recursive, cycle-detecting CTE produces a beautiful bounded result set. Just what we wanted!

It does! Postgres rocks!

===== 2025-05-24 ====

Interesting linear time performance regex lib for Java:

    https://github.com/google/re2j  <--- Only 35k .jar file


Interesting realization about the recursive query and our current data set:
Do a query for any of the node id's and you get all the scripts in the results. Since it's a cycle this is actually correct but, for a moment, I thought the query was wrong (because changing the script_id parameter didn't alter the results.) Doh!

Let's create a node chain that is separate and doesn't cycle:

    --> starting and ending scripts
    INSERT INTO brbl_logic.nodes VALUES('525028ae-0a33-4c80-a22f-868f77bb9531', NOW(),
        'True or false: people are the worst?', 4, 'BadPeople') RETURNING *;
    INSERT INTO brbl_logic.nodes VALUES('cf72ce06-50fc-4bf1-852b-dbdbd9f97f66', NOW(),
        'There is nothing left to say.', 4, 'AnotherEndOfConversation') RETURNING *;

    --> edges connecting them
    INSERT INTO brbl_logic.edges VALUES('2361468c-571d-43e1-a5cc-a5580841253c', NOW(), 'yes|true', 'Sadly, you are correct.',
        '525028ae-0a33-4c80-a22f-868f77bb9531', 'cf72ce06-50fc-4bf1-852b-dbdbd9f97f66') RETURNING *;
    INSERT INTO brbl_logic.edges VALUES('f6c08bf6-a984-4619-97be-3bb2526ba81d', NOW(), 'no|false', 'You are a hopeless optimist.',
        '525028ae-0a33-4c80-a22f-868f77bb9531', 'cf72ce06-50fc-4bf1-852b-dbdbd9f97f66') RETURNING *;
    --> terminating edge (with NULL destination)
    INSERT INTO brbl_logic.edges VALUES('053c7425-dfd9-4c32-b305-a15b50453274', NOW(), 'NOOP', 'NOOP',
        'cf72ce06-50fc-4bf1-852b-dbdbd9f97f66', NULL) RETURNING *;

Verified. The recursive node query only returns elements that are linked.
Asking for the graph starting with the BadPeople element yields:
                 id                   |    label    |                 src                  | match_text |                 dst
--------------------------------------+-------------+--------------------------------------+------------+-------------------------------------
 525028ae-0a33-4c80-a22f-868f77bb9531 | BadPeople   | 525028ae-0a33-4c80-a22f-868f77bb9531 | no|false   | cf72ce06-50fc-4bf1-852b-dbdbd9f97f66
 525028ae-0a33-4c80-a22f-868f77bb9531 | BadPeople   | 525028ae-0a33-4c80-a22f-868f77bb9531 | yes|true   | cf72ce06-50fc-4bf1-852b-dbdbd9f97f66
 cf72ce06-50fc-4bf1-852b-dbdbd9f97f66 | AnotherEn.. | cf72ce06-50fc-4bf1-852b-dbdbd9f97f66 | NOOP       |

 Perfect.


===== 2025-05-26 ====

Got the keywords table/query logic working. This won't be a simple, fast cache using Caffeine since we want/need to support
some amount of regular expressions. Explored embedding the keyword query inside the recursive node/edge query but dropped
it because it would require doing the pattern match in sql (not necessarily bad but possibly unwieldy) and it was hard to determine
from the result set where the graph "started."



===== 2025-05-28 ====

Trying out the combined keyword/node/edge logic in a full running system...

Using PlatformGateway + curl scripts make evaluating this difficult.

Let's bust out the BrblCli hack to see if it helps...

    java -cp target/sndrRcvr-1.0-SNAPSHOT-jar-with-dependencies.jar com.enoughisasgoodasafeast.cli.BrblCli -l

Actually working reasonably well although...
It shows an issue where the penultimate message in the node graph doesn't send the edge response before moving to the
end node. i.e. sending 'Iggy' as our choice of best third Stooge doesn't trigger the "Oops, wrong Stooges!" message.
Instead it just replies with the EchoWithPrefix response "That is all. Bye.: Iggy"

The current data in the scripts and edges table *looks* correct so maybe the Multi.Process logic preceded the node/edge model...

===== 2025-05-29 ====

ALTER ROLE <your_login_role> SET search_path TO a,b,c;

Idea: Leverage the evaluatedScripts list in the Multi.Process.evaluate() method to access the previous Script (the Multi.Present) to reference ResponseLogic elements to process the current input and route the conversation to the next Script.

Implemented a printGraph function for the Script class to better visualize the graph we're producing.
The terminal node node for each branch ends up with three redundant ResponseLogic elements (the 'NOOP' edges.)
This could be a problem with the query or the way we're assembling the graph from the query results.
Otherwise, the structure looks like what we intended.

Which means the behavior we see walking through it via the BrblCli is a problem with the node processing.

Realized that the Multi.Present -> Process combo is supposed to have the former pointing (only) at the latter.
Also that its the latter that contains the references to the choices. So we need to fix the data in brbl_logic.nodes/edges.

===== 2025-05-30 ====

Fixing the data...

TABLE brbl_logic.nodes:
 id         | uuid                     |           | not null |
 created_at | timestamp with time zone |           | not null |
 text       | character varying(255)   |           |          |
 type       | smallint                 |           | not null |
 label      | character varying(32)    |           |

=== Create the new Multi.Process scripts
insert into brbl_logic.nodes values(gen_random_uuid(), now(), 'Error message for ColorQuizProcess', 5, 'ColorQuizProcess') returning id;          --> 95441b9a-3636-4f45-bd0a-ec35e84dd5f7
insert into brbl_logic.nodes values(gen_random_uuid(), now(), 'Error message for StoogeQuiz_Process', 5, 'StoogeQuiz_Process') returning id;          --> 3f467ee3-4874-4ad1-82b1-99be06f575ab
insert into brbl_logic.nodes values(gen_random_uuid(), now(), 'Error message for UtensilQuiz_Process', 5, 'UtensilQuiz_Process') returning id;            --> 1bda7846-31b4-4983-92ec-161eba46c758
insert into brbl_logic.nodes values(gen_random_uuid(), now(), 'Error message for ShapeQuiz_Process', 5, 'ShapeQuiz_Process') returning id;            --> 23f00a69-55b7-4c29-b777-5c278b7088ed
insert into brbl_logic.nodes values(gen_random_uuid(), now(), 'Error message for BadPeople_Process', 5, 'BadPeople_Process') returning id;            --> ae28c0e3-9303-4807-979f-694bc9981dd7
insert into brbl_logic.nodes values(gen_random_uuid(), now(), 'Error message for AnotherEndOfConversation_Process', 5, 'AnotherEndOfConversation_Process') returning id;          --> aec8789f-546f-42c2-b1d5-c80bd10014f0

=== Point the existing edges to the new scripts:
UPDATE brbl_logic.edges set src = '95441b9a-3636-4f45-bd0a-ec35e84dd5f7' where src = '89eddcb8-7fe5-4cd1-b18b-78858f0789fb' ;
UPDATE brbl_logic.edges set src = '3f467ee3-4874-4ad1-82b1-99be06f575ab' where src = '0b2861b6-a16a-4197-910a-158610967dd9' ;
UPDATE brbl_logic.edges set src = '1bda7846-31b4-4983-92ec-161eba46c758' where src = '385a1f99-d844-42e6-9fa3-a0e3a116757d' ;
UPDATE brbl_logic.edges set src = '23f00a69-55b7-4c29-b777-5c278b7088ed' where src = 'b48d36ce-2512-4ee0-a9b9-b743d72e95e9' ;
UPDATE brbl_logic.edges set src = 'ae28c0e3-9303-4807-979f-694bc9981dd7' where src = '525028ae-0a33-4c80-a22f-868f77bb9531' ;
UPDATE brbl_logic.edges set src = 'aec8789f-546f-42c2-b1d5-c80bd10014f0' where src = 'cf72ce06-50fc-4bf1-852b-dbdbd9f97f66' ;

=== Create new edges linking the original Multi.Present scripts (type 4) to the new Process scripts (type 5):
id            | uuid                     |           | not null |
created_at    | timestamp with time zone |           | not null |
match_text    | character varying(128)   |           |          |
response_text | character varying(255)   |           |          |
src           | uuid                     |           | not null |
dst           | uuid                     |           |          |

INSERT INTO brbl_logic.edges VALUES (gen_random_uuid(),now(),'n/a','n/a', '89eddcb8-7fe5-4cd1-b18b-78858f0789fb','95441b9a-3636-4f45-bd0a-ec35e84dd5f7') returning id ;
INSERT INTO brbl_logic.edges VALUES (gen_random_uuid(),now(),'n/a','n/a', '0b2861b6-a16a-4197-910a-158610967dd9','3f467ee3-4874-4ad1-82b1-99be06f575ab') returning id ;
INSERT INTO brbl_logic.edges VALUES (gen_random_uuid(),now(),'n/a','n/a', '385a1f99-d844-42e6-9fa3-a0e3a116757d','1bda7846-31b4-4983-92ec-161eba46c758') returning id ;
INSERT INTO brbl_logic.edges VALUES (gen_random_uuid(),now(),'n/a','n/a', 'b48d36ce-2512-4ee0-a9b9-b743d72e95e9','23f00a69-55b7-4c29-b777-5c278b7088ed') returning id ;
INSERT INTO brbl_logic.edges VALUES (gen_random_uuid(),now(),'n/a','n/a', '525028ae-0a33-4c80-a22f-868f77bb9531','ae28c0e3-9303-4807-979f-694bc9981dd7') returning id ;
INSERT INTO brbl_logic.edges VALUES (gen_random_uuid(),now(),'n/a','n/a', 'cf72ce06-50fc-4bf1-852b-dbdbd9f97f66','aec8789f-546f-42c2-b1d5-c80bd10014f0') returning id ;

New edge ids:
65078637-9862-4682-8bbe-0a52ed29bf6b
ed53f6d3-3e66-4321-9ba7-ad3c36e79eb4
31ec69f2-c618-492a-aaa7-68bb6a4475f5
77ff5c56-ac17-4410-8cf9-439b34e7c587
83408cce-85a9-4824-b622-651c6839f399
4aaf8877-f0dc-4182-94f9-86f05e87de3a

Hmm, what is the "current" node at the end of the chain?


===== 2025-06-01 ====

Scripts are now chaining correctly albeit with some ugly code that I'll have to fix up.

The ScriptType enum currently defines the following types:
*   EchoWithPrefix(1),
    ReverseText(2),
    HelloGoodbye(3),
*   PresentMulti(4),
*   ProcessMulti(5),
    Pivot(6),
    TopicSelection(7);

Of these, only the first, fourth and fifth are being used and can be said to be useful.
#2 and #3 were only for early stage development testing.
#6 and #7 are not implemented.

Let's clear the deck and only consider the *Multi types to be in play.
What additional scenarios do we want/need to support in addition to these two?

A) A notification. Just a one and done message (possibly split into multiple text messages.) This could include instructions on how to start new conversations. No response expected. It chains to a EndOfChat node.
B) Ask a question that invites a free text input and reply with something canned e.g. "Thanks for the feedback" **
C) Same thing but apply sentiment or other textual analysis to the input and, based on the result, select the response sent back. For example, if the response was negative reply with "Uh oh. We're sorry you weren't happy with X..." The selection of next node would be determined by this evaluation, meaning that we require a bounded set of possibilities. In practice, this only requires that there be a default response. The sentiment analysis done by Vader and NLTK both return a value within a bounded range (-1..1 or some variation.)
D) The TopicSelection type is meant to provide an entry point if the user doesn't arrive by keyword (or they continue after completing their original node.) This function could be implemented with either Present/ProcessMulti or the free form option.
E) The Pivot is meant to be used when the user wants to break out of the current node either by using the "change topic" input or by repeatedly failing input validation. "If you want to talk about something else (enter "1" or continue with the current chat (enter "2")" This would chain the 1 response to a TopicSelection node and the 2 response back to the previous node requiring input (found using the Session.evaluated stack.) The implementation of a Pivot could be done simply with a *.Multi pair. The big difference is that Pivot isn't part of the graph. They are injected dynamically to deal with user requested breaks in the flow. Q: How do we reference the Pivot that should be used for a given graph? By Customer?
F) EndOfChat is a terminal node. It is used to avoid overloading null with the same meaning. Null should only indicate an error. An existing  Session where the currentNode is EndOfChat would trigger a call to the same findStartingScript method used when creating a new Session.

** We will have the raw text of the MO recorded in the brbl_logs.message_mo table but it might be useful to have a dedicated "user_poll" table that contains the evaluated response. For example, if the question was "Did you like our cupcakes?" and the MO response was "yup" then the user_poll table might be set with a value from a fixed set (e.g. YES, NO, UNKNOWN) presumably using some kind of sentiment or textual analysis.




===== 2025-06-04 ====

pysentimiento is the name of a sentiment analysis library trained on Spanish. NLTK-Vader is English only.

---

Some proposed rules for interaction between Script, Session and Operator.

Scripts evaluate the Session's current Script and return the Script that should become the new current on the basis of that evaluation.
    -> Session.getCurrentScript()
Scripts may queue MTs
    -> Session.registerOutput(Message)
Scripts may, e.g. Pivots, may access the stack of previously evaluated Scripts.
    -> Session.getEvaluatedScripts()

Operator may retrieve the Session's currentNode.
    -> Session.getCurrentScript()
Operator calls a Script's evaluate method.
Operator may update the Session's currentNode
    -> Session.setCurrentScript(Script)
Operator calls Session.flush().

Defined ScriptContext to constrain the functionality needed by node functions.
Using this class in the various node functions changes help mitigate the problem, I think.

Testing with the brblcli, changes don't appear to have caused any regressions.

Note: discovered an issue where we get into an unusable state if the keyword->node lookup doesn't find anything. Need to setup a default node to address this.


===== 2025-06-05 ====

Watching a video about the Java 25 release...
Ahead of Time Method Profiling is supposed to make programs running in standard JVM (i.e. not GraalVM) warm up much faster.
    (train) -XX:AOTMode ...
    then apply the profiling info...

Project Lilliput has been working on making object headers more compact. Looks easy to try out:
java -XX:+UseCompactObjectHeaders

Cool stuff!



INSERT INTO brbl_logic.nodes VALUES('abfd6e3c-b71d-445d-87fd-89744e66e5d1', now(), 'Thanks for dropping by our bakery!', 9, 'ThanksForVisit') RETURNING id;
INSERT INTO brbl_logic.nodes VALUES('35ab8b42-fbdf-47e1-ac62-bae86c3c7178', now(), 'How did you like our new matcha cupcake?', 7, 'AskCupcakePoll') RETURNING id;
INSERT INTO brbl_logic.nodes VALUES('23b09d2a-18f7-46ac-bff2-3968bd3a4dbe', now(), 'n/a', 8, 'ProcessCupcakePoll') RETURNING id;
INSERT INTO brbl_logic.nodes VALUES('669eea27-2dc1-4dff-bd0f-bc7d029bf714'', now(), 'Thanks for letting us know! We hope to see you again soon <3', 6, 'CyaLater') RETURNING id;
--link thanks to input request
INSERT INTO brbl_logic.edges VALUES ('abe3e848-8d89-4949-b881-0017f10c10cb',now(),'n/a','n/a',
        'abfd6e3c-b71d-445d-87fd-89744e66e5d1','35ab8b42-fbdf-47e1-ac62-bae86c3c7178') returning id ;

--link input request to process response
INSERT INTO brbl_logic.edges VALUES ('c6e395ed-eb0e-4954-bf51-da80242da49a', now(), 'n/a', 'n/a',
        '35ab8b42-fbdf-47e1-ac62-bae86c3c7178','23b09d2a-18f7-46ac-bff2-3968bd3a4dbe') returning id ;

--link process response to endOfChat
INSERT INTO brbl_logic.edges VALUES (gen_random_uuid(),now(),'n/a','n/a',
        '23b09d2a-18f7-46ac-bff2-3968bd3a4dbe','669eea27-2dc1-4dff-bd0f-bc7d029bf714') returning id ;

--finally link to null required to have the node node included in the graph
INSERT INTO brbl_logic.edges VALUES ('5e57ede3-0f45-4fc5-b7cb-80de35c94f71',now(),'n/a','n/a',
        '669eea27-2dc1-4dff-bd0f-bc7d029bf714', null) returning id ;


With these additional rows added to the database, I'm able to test and confirm that the new message types (SendMessage, RequestInput, ProcessInput and EndOfChat) work as expected. Cool cool cool...



===== 2025-06-08 ====

https://vladmihalcea.com/hibernate-with-recursive-query/ has some interesting info about using recursive CTEs with Hibernate.
Hopefully still compatible with the newly released Hibernate 7.

Started a new project/repo: BrblEdit.

To reference the sndrRcvr code (which we should rename to just Brbl as is reflected on GitHub) its simple enough to run:

    mvn install -DskipTests=true

This generates and installs the jar and .pom file in my local ~/.m2 folder.

select * from brbl_logs.messages_mo mos
inner join brbl_logs.messages_mo_prcd prcd on mos.id=prcd.id
where rcvd_at >= '2025-06-09 17:00:00';


===== 2025-06-09 ====
BrblEdit progress...
Got the basic domain models for the record-based models used in our runtime working in Hibernate.
Note, I'm using the direct Hibernate interfaces/classes rather than the Jakarta Persistence API.

===== 2025-06-10 ====

In testing the script graphs produced by BrblEdit I once again realized how brittle the Operator is to exploding when a session attempts to continue talking after the end of the graph is reached. The resulting NPE will put it into a state in which newly arriving messages are not processed. (Rcvr puts them on the queue but Operator won't get called because the BrblConsumer has shutdown:
    operator-1  | java.lang.NullPointerException: Cannot invoke "com.enoughisasgoodasafeast.operator.Node.evaluate(com.enoughisasgoodasafeast.operator.ScriptContext, com.enoughisasgoodasafeast.Message)" because "session.currentNode" is null
    operator-1  |   at com.enoughisasgoodasafeast.operator.Operator.process(Operator.java:143)
    operator-1  |   at com.enoughisasgoodasafeast.operator.Operator.process(Operator.java:112)
    operator-1  |   at com.enoughisasgoodasafeast.OperatorConsumer.handleDelivery(OperatorConsumer.java:41)
    operator-1  |   at com.rabbitmq.client.impl.ConsumerDispatcher$5.run(ConsumerDispatcher.java:149)
    operator-1  |   at com.rabbitmq.client.impl.ConsumerWorkService$WorkPoolRunnable.run(ConsumerWorkService.java:111)
    operator-1  |   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    operator-1  |   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    operator-1  |   at java.base/java.lang.Thread.run(Unknown Source)
    operator-1  | 2025-06-11 09:47:50,000 WARN  [] [pool-1-thread-10] c.e.BrblConsumer - handleShutdownSignal called with consumerTag amq.ctag-uXoJ6yAukegn-3FMeh5NFQ and exception: com.rabbitmq.client.ShutdownSignalException: clean channel shutdown; protocol method: #method<channel.close>(reply-code=200, reply-text=Closed due to exception from Consumer (amq.ctag-uXoJ6yAukegn-3FMeh5NFQ) method handleDelivery for channel AMQChannel(amqp://guest@192.168.1.155:5672/,1), class-id=0, method-id=0)

The same problem exists if the keyword sent doesn't find a match: NPEs and death of the queue consumer.


===== 2025-06-14 ====

While trying to figure out a Hibernate equivalent to our PersistentPostgresManager.getScriptForKeyword() method of retrieval (pulling the graph of nodes and edges in a single query) I started to consider whether we could simplify the script model by merging the node and edge and using the more common single table approach. This is how the Hibernate docs and many relevant blogs assume you're doing it.

We need to support the ability to branch at any point based on the node type (e.g. Multi.Process or Input.Process.)
Initially, I was concerned about overloading the use of the match_text or response_text columns if we only had a single type of object and I wanted to be able to create cycles, reusing nodes across different scripts.

A -----> B -----> C
          \
           * --> D

X -----> Y -----> C

table Node
id       parent_id
======|==========
A     | null
B     |  A
D     |  B
X     |  null
Y     |  X

To find the branches for B we query for rows where parent_id = B
Q: How is C reached from either B or Y?
A: It cannot be reached from both if id is the primary key

The latter concern (reusing nodes/creating cycles) CANNOT be addressed with the merged model while still supporting the former (branch from a node.)
Best case we can copy the properties of C and set them on a separate node.
Is there a way to signal/hint that C and clone of C should be equivalent?
Or a way to separate the mappings from the nodes' other properties?
For example...

table NodeMappings
id    |  parent_id
===================
C     |  B
C     |  Y

table Node
id    | type  | match_text | response_text | label   | etc.
==========================================================
C     |  3    | "foo"      | "Welcome"     | "Intro" | ...

This might work but leads us back to the implementation challenge using Hibernate.

But what if we decided that scripts couldn't be cyclic? That they always have an end point and continuing past the point would trigger some special logic that interpreted the user input (in the absence of a scripted multiple choice element) and routed them to a new script graph.
It seems like we have to solve that problem anyway since we know we won't always be responding to keywords. (A simple implementation would simply provide a default script without attempting to understand the user's context.) As well, we want the user to be able to ask, at any point, "change topic," breaking out of the current script and switching to a different one.

So I guess it comes down to either keeping the Node uncluttered with special case columns that are only used for certain NodeTypes (storing them on the Edge table instead) or simplifying the code for fetching script graphs.




===== 2025-06-16 ====

Let's try a native query with a entity class that combines the Node and Edge types. Maybe it's really not that complicated.
Hmm...the native query used with simple bean that merges the fields of the two types is all but a JDBC query; it doesn't recognize any object references. If we have to use just their id (an UUID instead of a Node, for example,) then we'll have to query again for each object to resolve them. Not what we want.

===== 2025-06-17 ====

Watched an interesting video talking about using a bunch of the newest java features together. One that I hadn't recognized as being useful is sealed classes used with records and the enhanced switch. The idea that now occurs to me is to use them to replace the null case when a script has no next elements. Instead of a value or null define records that represent the two. The record class of the value and the null surrogate would both implement a marker interface. We could also use something similar to enrich the Operator methods that currently return boolean.

We should also take a look at Stream Gatherers that are introduced in JDK 24. (We should also migrate our project though there were changes to the Structured concurrency API that we may have to adapt to.)

...wrote a version of the code that uses the Gatherer API to convert the result set returned by the recursive CTE into a connected graph. Maybe not a great use case; the non-functional code is simpler.

===== 2025-06-29 ====

Mostly working on BrblEdit these days.

Some shared changes need to be done however.
_ Customer table with integration into keywords.
X Adding updatedAt columns to Node and Edge.

We'd been using a composite primary key for the USERS table combining platform_id and platform_code. We knew this was likely overkill given that the platform_id is unique (it's a phone number or WhatsApp number or our own Brbl ID.) And, it creates a problem linking to customer.

The better alternative, I think, is to link Customer to Profile. The latter's primary key references the User's group_id.


===== 2025-07-03 ====

As part of the work to support script authoring (BrblEdit) we need to clean up our organization of keywords and create a means to track scripts.

Currently keywords requires an exact match of the Message.text. It doesn't even trim the input.

If we scope keywords by the short/long code that receives the message the subset could be small enough to use regexes to signal a match.

- Exact match must support case insensitivity, of course
- With or without spaces (if multiword)
- Misspellings
- Common abbreviations
- Common homophones (possibly indistinct from the previous two)

Easy to deal with casing issues (just trim and lower case both the patterns and the input)

    String pattern = "(color|colour|colr).*(quiz|q|kwiz).*";
    Pattern compiledPattern = Pattern.compile(pattern);
    ...
    String mutatedInput = input.trim().toLowerCase();
    return compiledPattern.matcher(mutatedInput).matches();

Other thoughts: matching on homophones of the keyword. Lucene has functionality to support this but dragging all that seems like pretty massive feature creep. Using Lucene via Hibernate Search might be possible but still a big increment and I'm not sure how much flexibility is really needed.


===== 2025-07-04 ====

Realized that I'd really been conflating the keyword and script fetching functionality in my mind. How I'm thinking it should work now:

- the keyword cache population method should pull all the entries in the database.
- the keyword get operation will iterate over the subset of keywords (for the given short code.) Any shortcuts we can take?

The return value is simply the final matching id.
- the corresponding Node would then be pulled from a separate Caffeine cache populated with the graphs provided by PersistenceManager.getScript().

Some thoughts around merging keywords and the new scripts model (see NOTES.md in BrblEdit from 2025-07-01) but since keywords don't make as much sense for non-SMS platforms I think it might be better to keep them separate for now.

We do have to add the notion of short/long codes (aka 10DLC) to keywords.

Adding support for shortcodes and long codes to the keywords table. They allow an additional level of scoping.

    ALTER TABLE brbl_logic.keywords ADD COLUMN short_code VARCHAR(10);
    COMMENT ON COLUMN brbl_logic.keywords.short_code IS
        'The short (5-6 digits) or long (10 digit) code for which the keyword is scoped. Global if none is specified.';
    ALTER TABLE brbl_logic.keywords DROP CONSTRAINT unique_pattern_platform;
    ALTER TABLE brbl_logic.keywords ADD CONSTRAINT unique_platform_shortcode_pattern UNIQUE(platform, short_code, pattern);

===== 2025-07-06 ====

Also need the combined created_at and updated_at columns to the keywords table:

    ALTER TABLE brbl_logic.keywords ADD COLUMN created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW();
    ALTER TABLE brbl_logic.keywords ADD COLUMN updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW();


keyword cache: keyed by a record (KeywordCacheKey) composed of the "to" and text fields of the Message with a UUID value of the Node.

script cache: keyed by the UUID of the starting node,  values: being the Node with all its connections.

The trick here is that, we need the full set of patterns (exact matches or regexes) in hand to evaluate the incoming expressions.
It's the evaluation of the patterns against the incoming message (which we assume to be the keyword) that is cached.  Not sure this will be an actual benefit performance-wise. The value might simply be the UUID of a Node which would then need to be used to pull the connected graph from a separate cache (scriptCache.)

===== 2025-07-08 ====

Ok, for keyword matching evaluation we setup a special, single entry cache. The key is a constant string ("ALL") and the value is a <Pattern,Keyword> map containing the full content of the brbl_logic.keywords table. We evaluate the regexes in the iteration order (whatever it may be) and return the first match, logging misses loudly. The code then uses the returned UUID to pull the connected graph from the scriptCache (also a LoadingCache instance.)

Lightly tested. Seems to be working as designed.

Next up: a means of identifying other types of scripts for specific circumstances per specific customers/short codes. This includes "top level" topic conversations and handlers for when the user asks to "talk about something else."



