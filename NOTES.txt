https://medium.com/helidon/helidon-logging-and-mdc-5de272cf085d

explains, imperfectly, how to use JUL logging with Helidon.
Turns out the configuration file that Helidon expects is logging.properties at the root of the context.
The format is a little weird.

NAMING SCHEMES:

We're going to use the terminology from our SMS/SMPP days.

for queues

    <platform>.<region>.<direction>

    e.g.

    whatsapp.us.mo  - the platform is whatsapp, the region is the United States, the direction is Mobile Originated.

    test.local.mo   - the platform is a developer/test, the region is the scope of the test resources (likely a dev machine), the direction is Mobile Originated.

Current design uses Topics with routingKeys.
I'm thinking that we'd likely have separate endpoints running for each platform gateway but using topics & routingKeys gives us flexibility.

Test Setup:

Run the container image with the RabbitMQ server:

    docker run -it --rm --name rabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:4.0-management

FakePlatformMO (client)
    Reads messages from file, for each
    HTTP POST to
     --> Rcvr (port: 4242)
            --> RabbitMQ client enqueue to "test.mo"

     --> "Operator"
         RabbitMQ client dequeues from "test.mo"
         Processes the message (side effects galore)
         Creates response
         Enqueues response to "test.mt"

     --> Sndr
        RabbitMQ client dequeues from "test.mt"
        HTTP POST to

     --> FakePlatformMT (port: 2424)
        Writes each received message to file.

 Validator compares input file with output file.

Test Program:
FakePlatformMO generates messages with known values.
FakePlatformMT receives messages and validates that it corresponds to a previously sent message.


Helidon has libraries that support tracing (https://helidon.io/docs/v4/se/tracing) for both the web server and web client. Note: OpenTracing (https://opentracing.io/) has been retired in favor of OpenTelemetry (https://opentelemetry.io/docs/languages/js/instrumentation/)

OpenAPI is also supported in case parts of the software needs to be used with AWS Lambda.

Scheduling support is available (https://helidon.io/docs/v4/se/scheduling) which should allow us to avoid shit like EventBridge. Under the covers it uses cron-utils (https://github.com/jmrozanec/cron-utils)


=========== Building an executable jar ===========
Added the following components to the <build/> section of the pom.xml file:
    exec-maven-plugin
    maven-compiler-plugin
    maven-jar-plugin
    maven-assembly-plugin
Now we can run:
    mvn clean compile assembly:single

to yield an artifact that can be run with 'java -jar'
The artifact ends up being around 23MB as of 12/12/2024.

Java Microbenchmark Harness:
    https://www.baeldung.com/java-microbenchmark-harness


Trying to compile the project with native-image yielded errors related to logback.
The GraalVM output was pretty helpful. It suggested adding to the file that tracks usages of reflection.
While researching the problem someone suggested switching to the new configuration system available for logback: logback-tyler
This provides a class generator that provides a fairly easy to edit Configurator implementation that doesn't use XML or reflection.
In addition to making Graal happy the initialization will be faster and more efficient for the lack of XML.

Of course, it couldn't be that simple ;-) There seems to be a bug that causes the generated class to not be marked public. The service-provider mechanism added JDK9 in support of the new package system complained of not having access. Simply editing the class to be public appears to solve the problem. I posted a question to the GitHub discussion board:

    https://github.com/qos-ch/logback-tyler/discussions/5

Needed to add a reachability-metadata.json file to META-INF/native-image to have the .properties files included in the binary.

Some notable/interesting bits from the compiler output:

Top 10 origins of code area:                                Top 10 object types in image heap:
  11.60MB java.base                                            8.48MB byte[] for embedded resources
   2.59MB java.xml                                             5.24MB byte[] for code metadata
   1.22MB svm.jar (Native Image)                               3.31MB byte[] for java.lang.String
 747.48kB logback-core-1.5.12.jar                              2.31MB java.lang.String
 468.30kB amqp-client-5.23.0.jar                               2.20MB java.lang.Class
 326.55kB java.rmi                                           843.80kB byte[] for general heap data
 244.26kB logback-classic-1.5.12.jar                         785.30kB com.oracle.svm.core.hub.DynamicHubCompanion
 207.22kB helidon-http-http2-4.1.4.jar                       554.23kB heap alignment
 203.91kB java.naming                                        513.00kB int[][]
 203.76kB helidon-webclient-api-4.1.4.jar                    491.98kB byte[] for reflection metadata
   1.46MB for 37 more packages                                 5.34MB for 2375 more object types

Recommendations:
 HEAP: Set max heap for improved and more predictable memory usage.
 CPU:  Enable more CPU features with '-march=native' for improved performance.

 Questions: Why are we getting java.xml and java.rmi included?
 The binary size is significant: 50MB.

 We need a way to produce containers that will run locally on my M1
 Can we run a Linux VM locally to run the build? Or even a container?

 NOTE: For a moment I thought the --target option of native-image would let us produce an executable for different platforms.
 This is Java (Write-Once Run Everywhereâ„¢) after. But it turns out it's bullshit. After six years of development this is still on the to do list.
 Given the prevalence of containers which are _only_ supported for Linux its amazing that this hasn't been prioritized.
 As it stands, we'll need to create a bunch of extra container rigging to produce something that we then copy over into another image.

    https://github.com/spotify/dockerfile-maven is a project for building container images with Docker. It's not still in development, however.

=========== STUFF WE WILL WANT ===========

- Software BOM with security information for our dependencies.
- Qodana or something free to perform structured code analysis (we need to take some time to configure this to remove bogus or just unhelpful problems.)
- Container images for arm64 and amd64.

=========== STUFF WE SHOULD LEARN MORE ABOUT ===========
The modules system introduced in JDK 9.
The inner workings of Docker so I can evaluate images, how to extend, etc.

Docker progress:

Finally figured out how to specify/pass arguments when running Burble inside a docker container.
Changed the

docker run -it --rm --name burble -p 2424:2424 -p 4242:4242 burble:0.1 FakeOperator


native-image -jar target/sndrRcvr-1.0-SNAPSHOT-jar-with-dependencies.jar -o burble
    produces a file, burble, that is still fucking massive at 44MB. :-(

The container that builds on eclipse-temurin:23 and includes the binary weighs in at 719MB.

Switching to eclipse-temurin:23-alpine reduces the image to a "mere" 581MB.
Most of the container comes from the installation of
    apk add --no-cache fontconfig ttf-dejavu gnupg ca-certificates p11-kit-trust musl-locales musl-locales-lang binutils tzdata coreutils openssl (63MB)
    OpenJDK23U-jdk_aarch64_alpine-linux_hotspot_23.0.1_11.tar.gz (310MB uncompressed)
The Burble executable is the third-largest item.

We need to add config to the native-image build to specify the musl C lib. This is only applicable to the Linux version, however.
Musl is not a thing for macOS.

Most people apparently use a container to build their linux image. There's an "official" Maven image repo

docker run -it --rm --name mvnc -v "$HOME/.m2":/root/.m2 -v "$(pwd)":/Users/mark/Development/sndrRcvr/src -w /Users/mark/Development/sndrRcvr/src maven:latest mvn package

We want a container
- with graalvm 23
- 3.x Maven
Initially this doesn't need to be alpine based but the musl thing might require it

This repo looks promising: https://github.com/vegardit/docker-graalvm-maven/blob/main/README.md

https://github.com/vegardit/docker-graalvm-maven/blob/main/README.md

=== Example use of the vegardit image to build a native binary for linux ===
docker run --rm -it \
  -v $PWD:/mnt/myproject:rw \       # what's the effect of the :rw part of this?
  -w /mnt/myproject \
  vegardit/graalvm-maven:latest-java23 \
  mvn clean package

Here's how we're using it to

> docker run --rm -it -v "$HOME/.m2/repository":/root/.m2/repository -v "$(pwd)":/mnt/sndrRcvr -w /mnt/sndrRcvr vegardit/graalvm-maven:latest-java23 mvn clean package

Works! Produces a jar file in the target folder of our host OS.

> docker run --rm -it -v "$HOME/.m2/repository":/root/.m2/repository:rw -v "$(pwd)":/mnt/sndrRcvr -w /mnt/sndrRcvr vegardit/graalvm-maven:latest-java23 mvn -Pnative clean package

Works! Produces an ELF executable using the aarch64 instruction set.
NOTE: the unix `file` command points out it's dynamically linked, however:
    target/burble: ELF 64-bit LSB pie executable, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-aarch64.so.1, BuildID[sha1]=99c5d1bf7a89955c2461187540350e00678a5fa1, for GNU/Linux 3.7.0, not stripped

Once the artifact is produced, we can build the docker image that will execute it...

Made a copy of Dockerfile then renamed both.
    Dockerfile.bin - builds an image that uses a native-image generated executable.
    Dockerfile.jvm - builds an image that runs a normal jar file using eclipse-temurin:23-alpine.

Use them like so to build the image:
    docker build -t burble-bin:0.1.0 -f Dockerfile.bin .
    docker build -t burble-jvm:0.1.0 -f Dockerfile.jvm .

Then to run them:
    The binary executable version:
    docker run -it --rm -p 4242:4242 --name burble-bin-rcvr burble-bin:0.1.0 Rcvr
    docker run -it --rm --name burble-bin-fkop burble-bin:0.1.0 FakeOperator
    docker run -it --rm --name burble-bin-sndr burble-bin:0.1.0 Sndr

    The JVM version:
    docker run -it --rm -p 4242:4242 --name burble-jvm-rcvr burble-jvm:0.1.0 Rcvr
    docker run -it --rm --name burble-jvm-fkop burble-jvm:0.1.0 FakeOperator
    docker run -it --rm --name burble-jvm-sndr burble-jvm:0.1.0 Sndr

Where the general form is:
    docker run -it --rm [-p 2424:2424] | [-p 4242:4242] --name burble-jvm burble-jvm:0.1.0 <programName: Rcvr | FakeOperator | Sndr>

The Dockerfiles both default to running FakeOperator.

To run the image without executing the entrypoint/cmd:
    docker run -it --entrypoint /bin/sh burble-bin:0.1.0

Actually it appears that the binary version running on the alpine container fails because, as noted above, the executable is dynamically linked.
It fails with a very terse and somewhat misleading error:
    "exec /opt/app/burble: no such file or directory"
I think this means that it can't find the expected libc rather than the program itself.

I added a block to the configuration section of the native-maven-plugin setup:
<buildArgs>
    <buildArg>--static --libc=musl --enable-sbom</buildArg>
</buildArgs>

Which seems to be signalling the correct switches in the native-image program but the container that produces the linux binary lacks the referenced library.

docker run --rm -it -v "$HOME/.m2/repository":/root/.m2/repository:rw -v "$(pwd)":/mnt/sndrRcvr -w /mnt/sndrRcvr mstewart/graalvm-maven-musl mvn -Pnative clean package

Notable output from native-image:

"HEAP: Set max heap for improved and more predictable memory usage."

Would be nice if they mentioned the switch/param/flag that controls this. Is it the normal JVM flag? Where does it get set?

 "CPU:  Enable more CPU features with '-march=native' for improved performance."

 Added <buildArg>-march=native</buildArg> to the native-maven-plugin config which eliminates the message, so I assume it was the right way to do it.

 1 experimental option(s) unlocked:
 - '-H:IncludeResources': Use a resource-config.json in your META-INF/native-image/<groupID>/<artifactID> directory instead. (origin(s): 'META-INF/native-image/com.rabbitmq/amqp-client/native-image.properties' in 'file:///root/.m2/repository/com/rabbitmq/amqp-client/5.23.0/amqp-client-5.23.0.jar')

This is annoying. The lib is providing the needed data so it can be used to produce the native executable, but they've (I guess) changed their mind how this is to be implemented? Worse, they leave it to me to figure out how this is to be represented in the new file.
I think, for now, we will leave this unaddressed. At least until we're certain of the lib versions we want to use.

::IMPORTANT NOTE::
As of 12/26/2024, building a fully statically linked graalvm native image is *only* supported on linux for x86_64:
    https://github.com/oracle/graal/issues/9490
This is sad. The project seems old enough that it shouldn't still be a problem. There's no cross-compilation either though that is a little more understandable.
We should try building the musl/statically linked binary on one of our x86 machines just to be sure it will work.

NEXT STEPS:
X create a working docker-compose.yml to coordinate testing (leave rabbitmq out initially)
_ create some end-to-end tests
_ extend the rabbitmq image to add the admin user and any other customized bits we need then add it to the docker-compose.yaml.
_ create a management CLI using JLine/Jansi, etc.


I swear, where Docker and GraalVM is concerned, every step is a fucking struggle.

Running just a single container via docker-compose.yaml I seem unable to get an HTTP call to the Rcvr web server.

mark@YA-T7RX9LX2PL sndrRcvr % docker compose up -d

mark@YA-T7RX9LX2PL sndrRcvr % curl --verbose http://localhost:4242/health
* Host localhost:4242 was resolved.
* IPv6: ::1
* IPv4: 127.0.0.1
*   Trying [::1]:4242...
* Connected to localhost (::1) port 4242
> GET /health HTTP/1.1
> Host: localhost:4242
> User-Agent: curl/8.7.1
> Accept: */*
>
* Request completely sent off
* Empty reply from server
* Closing connection
curl: (52) Empty reply from server

===== 2024-12-28 ======
Ran the jvm  version of the containers and no longer get the weird HTTP problem. Were the binaries not working?

I re-ran the build_linux.sh script to make sure the executable is what we want.

mark@YA-T7RX9LX2PL sndrRcvr % file target/burble
target/burble: ELF 64-bit LSB pie executable, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-aarch64.so.1, BuildID[sha1]=0c26671ef34dfacfcaaea9595bc5e32e0417980a, for GNU/Linux 3.7.0, not stripped

OK, check.

The build uses a Debian-based container while the execution container is "eclipse-temurin:23" (I think this uses ubuntu as its base.)
Let's try it with "eclipse-temurin:23-jre" as the base layer (this is definitely an Ubuntu image and might be a bit smaller than the JDK version.)

...and, confirmed, that the problem is only with the binary version. Possibly because of the Debian-Ubuntu schism?

No, even when I change the Dockerfile of the image that runs the program to use the same Debian-based image that built the program AND when running curl from within that container.

$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ cat /proc/version
Linux version 6.10.11-linuxkit (root@buildkitsandbox) (gcc (Alpine 13.2.1_git20240309) 13.2.1 20240309, GNU ld (GNU Binutils) 2.42) #1 SMP Thu Oct  3 10:17:28 UTC 2024

Why is the gcc showing Alpine?

Hmm, the RabbitMQ supplied image that we're using--running Noble Numbat, version 24.04.1 LTS--shows the same gcc info. Maybe this is normal?

$ ldd /opt/app/burble
        linux-vdso.so.1 (0x0000ffffa22a2000)
        libz.so.1 => /lib/aarch64-linux-gnu/libz.so.1 (0x0000ffffa2230000)
        libc.so.6 => /lib/aarch64-linux-gnu/libc.so.6 (0x0000ffff9ea50000)
        /lib/ld-linux-aarch64.so.1 (0x0000ffffa2265000)

===== 2024-12-30 ======

Curious if we get a smaller image if we build the jre instead of the jdk...
With JDK...
burble-jvm                               0.1.0            912143180f41   41 hours ago   801MB
With JRE...
burble-jvm                               0.1.0            7171dc515c27   3 minutes ago   507MB

So, yay. The first actual reduction we've seen. GraalVM binaries are still fucking huge.
Baseline manual test (sending messages through the pipeline) appears to work fine.

What could we gain by putting a JRE on top of something like debian:stable-slim (which is used as the base by vegardit/graalvm-maven, which we use for building linux binaries.)

debian                                   stable-slim      5f21ebd35844   7 days ago       136MB
eclipse-temurin                          23               c2a3aba09776   2 months ago     712MB

How does the Ubuntu base for eclipse-temurin compare to slim?

While investigating this I checked out https://hub.docker.com/_/eclipse-temurin.
Two things of note:
They show how to set up the JDK using a reference to one of their images:

    FROM <base image>
    ENV JAVA_HOME=/opt/java/openjdk
    COPY --from=eclipse-temurin:23 $JAVA_HOME $JAVA_HOME
    ENV PATH="${JAVA_HOME}/bin:${PATH}"

They recommend building a custom JRE using jlink. Is this worth the bother?
There's also an eclipse-temurin:23.0.1_11-jre-alpine image.
See https://github.com/docker-library/repo-info/blob/master/repos/eclipse-temurin/local/23-jre-alpine.md

eclipse-temurin                          23.0.1_11-jre-alpine   623a424ca41d   2 months ago     291MB
Inspecting the layers it appears that the JRE comprises ~161MB of this image.
NOTE: busybox, which combines ~400 commands into a single program, doesn't include curl but *does* include wget so we won't need the former to do basic, intra-container testing.

Adding our code only adds 8MB:
burble-jvm                               0.1.0                  52f06679217c   2 minutes ago   298MB

This is a decent tradeoff.

Another good tip here to avoid having to rebuild the image every time the Maven produced jar changes is to mount the host path onto the container. So given,

    FROM eclipse-temurin:21.0.2_13-jdk
    CMD ["java", "-jar", "/opt/app/japp.jar"]

We can run the container like this:
    docker build -t <tag> .
    docker run -it -v /path/on/host/system/jars:/opt/app <tag>

What syntax do we use for a docker-compose file to do this?

TODO
_ Since building a fully statically linked graalvm native image is *only* supported on linux for x86_64 we should bust out our large memory Macbook.
X Create a merged PlatformGatewayMT and MO generator for testing purposes.

Digging into the testcontainers project and found an image for RabbitMQ that's built on an Alpine base.
As expected it's smaller than the non -alpine version:
rabbitmq                                 4.0-management          14c30a03410f   3 months ago   425MB
rabbitmq                                 4.0-management-alpine   74bf73c53b96   3 months ago   295MB

Testcontainers supports running docker-compose files though the logging output is dramatically different from what we see running those files via docker. See https://codeal.medium.com/how-to-run-docker-compose-with-testcontainers-7d1ba73afeeb (it mentions the use of the volumes section of the compose file that, for things like postgres, can be used to run ddl scripts.


We want to be able to do this for integration testing.
That said, testing things we've noted to be possibly problematic--having different components come up out of order or restarting--would seem to require a per-container level of control.

Side-note: It appears possible to use Testcontainers with Docker alternatives, OrbStack and Colima. See http://rockyourcode.com/testcontainers-with-orbstack/ and http://rockyourcode.com/testcontainers-with-colima. Also https://github.com/testcontainers/testcontainers-java/issues/5034#issuecomment-1036433226

Probably not worth the extra effort unless we have problems with Docker.

===== 2025-01-02 ======

Working on EndToEndMessaging:

The docker setup code informs me:
    'container_name' property set for service 'brkr' but this property is not supported by Testcontainers, consider removing it
But clearly it's more than just a suggestion because it throws an ExceptionInInitializerError that kills the container startup.

This is tracked in https://github.com/testcontainers/testcontainers-java/issues/2472, but they don't seem to be interested in fixing it; a perfectly reasonable PR adding support for it--https://github.com/testcontainers/testcontainers-java/pull/2741--was closed.
So I've commented out the container_name in the docker-compose-jvm.yaml file.

Created an integration test, EndToEndMessaging that uses this file. The @Container annotation is supposed to start the docker process, but it doesn't seem to be doing so. Adding a static block that calls the .start() method after the annotation is a simple workaround even if it's a bit disappointing.

Reworked PlatformGatewayMT to act as both a MO source and the MT destination.

Next up: using specific containers to test ordering dependencies.

Scenarios:
- what should Rcvr do if the broker disappears?
    Currently, when messages are received we actually return the following to each calling request:
        connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)connection is already closed due to connection error; protocol method: #method&lt;connection.close&gt;(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason &#x27;shutdown&#x27;, class-id=0, method-id=0)Complete.

    Obviously we shouldn't do this.
    Does it recover when the broker comes back?

    Restarted the broker container and got the following in the Rcvr log:

        2025-01-05 10:38:44,542 ERROR [] [AMQP Connection 192.168.1.155:5672] c.r.c.impl.ForgivingExceptionHandler - Caught an exception during connection recovery!
        java.io.IOException: null
        	at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:140)
        	at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:136)
        	at com.rabbitmq.client.impl.AMQConnection.start(AMQConnection.java:406)
        	at com.rabbitmq.client.impl.recovery.RecoveryAwareAMQConnectionFactory.newConnection(RecoveryAwareAMQConnectionFactory.java:71)
        	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.recoverConnection(AutorecoveringConnection.java:628)
        	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.beginAutomaticRecovery(AutorecoveringConnection.java:589)
        	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.lambda$addAutomaticRecoveryListener$3(AutorecoveringConnection.java:524)
        	at com.rabbitmq.client.impl.AMQConnection.notifyRecoveryCanBeginListeners(AMQConnection.java:839)
        	at com.rabbitmq.client.impl.AMQConnection.doFinalShutdown(AMQConnection.java:816)
        	at com.rabbitmq.client.impl.AMQConnection$MainLoop.run(AMQConnection.java:700)
        	at java.base/java.lang.Thread.run(Thread.java:1575)
        Caused by: com.rabbitmq.client.ShutdownSignalException: connection error
        	at com.rabbitmq.utility.ValueOrException.getValue(ValueOrException.java:66)
        	at com.rabbitmq.utility.BlockingValueOrException.uninterruptibleGetValue(BlockingValueOrException.java:36)
        	at com.rabbitmq.client.impl.AMQChannel$BlockingRpcContinuation.getReply(AMQChannel.java:552)
        	at com.rabbitmq.client.impl.AMQConnection.start(AMQConnection.java:336)
        	... 8 common frames omitted
        Caused by: java.io.EOFException: null
        	at java.base/java.io.DataInputStream.readUnsignedByte(DataInputStream.java:297)
        	at com.rabbitmq.client.impl.Frame.readFrom(Frame.java:91)
        	at com.rabbitmq.client.impl.SocketFrameHandler.readFrame(SocketFrameHandler.java:199)
        	at com.rabbitmq.client.impl.AMQConnection$MainLoop.run(AMQConnection.java:687)
        	... 1 common frames omitted

However, when we sent more traffic, the Rcvr handled it and was able to send it to the broker. Then I started the FakeOperator and the messages again passed through.

So is there a callback available to know when the broker is available?

===== 2025-01-07 ======
Having yet more fun with Testcontainers. >:-[

To explore the availability scenarios above, I'm programmatically creating containers for each application.

If I try starting a Rcvr container without a RabbitMQ broker running I get a complaint from InternalCommandPortListeningCheck that seems to stem from the call to withExposedPorts() that's used to connect with the Rcvr's web server.

It's clear, however, that to really have our components be testable in this way we need to provide a direct means of configuring ports.
Added an override mechanism to ConfigLoader that reads the properties file and the environment vars, overriding the former with the latter where they overlap.

Need a similar mechanism for PlatformGateway to provide it the effective http port for the Rcvr...
Hacked a fix for this but still running into the InternalCommandPortListeningCheck:
    https://github.com/testcontainers/testcontainers-java/issues/6730

Fucking hell, maybe it's just easier to add bash.
Did that but also needed to rebuild the jar file (which I had been neglecting.) Time to set up a shared mount for the containers so we don't need to rebuild the container everytime we make a change to the Java code (assuming we even remember to do that!)

===== 2025-01-09 ======
ServiceAvailabilityTest.testRcvrReconnect:
I have most of the containers running and passing messages along, up to where Sndr is supposed to call the PlatformGateway's web server to deliver the generated MTs.
Having trouble communicating with the PlatformGateway program that's running directly on the host (not in a container like the rest.)

Let's try...
Run PlatformGateway (its web server using the same sndr.properties configuration, just listening for calls)
Run the Sndr container with bash:

    docker run -it --entrypoint /bin/bash burble-jvm:0.1.0

then use wget to post some data:

    wget --post-data 'wtf dude 2'  http://192.168.1.155:2424/mtReceive
    Connecting to 192.168.1.155:2424 (192.168.1.155:2424)
    saving to 'mtReceive'
    mtReceive            100% |********************************************************************************************|     2  0:00:00 ETA
    'mtReceive' saved

So as long as we got the url right, it should work.
Someone on stackoverflow suggested using 'host.docker.internal' as the host name for the container host. That actually works from
a bash shell inside a running container:
    wget --post-data 'wtf dude 2'  http://host.docker.internal:2424/mtReceive
    Connecting to host.docker.internal:2424 (192.168.65.254:2424)
    saving to 'mtReceive'
    mtReceive            100% |********************************************************************************************|     2  0:00:00 ETA
    'mtReceive' saved

Confirmed that the running program handled it:
    2025-01-09 15:58:12,929 INFO  [] [[0x67cf6f99 0x537fdca4] WebServer socket] c.e.PlatformGateway - Received content: wtf dude 2

When running PlatformGateway, it tells us
    2025-01-09 15:57:27,162 INFO  [] [start @default (/0.0.0.0:2424)] io.helidon.webserver.ServerListener - [0x67cf6f99] http://0.0.0.0:2424 bound for socket '@default'


RANDOM NOTE: https://eclipse.dev/openj9/ OpenJ9 JVM touts faster startup, less memory.

===== 2025-01-18 ======

ServiceAvailabilityTest is still broken due to some innocuous refactoring...

Sndr.init is called (and throws an exception) _after_ the test message are sent and received by the Rcvr:

    2025-01-14 15:54:43,187 INFO  [] [docker-java-stream-788704569] c.e.i.ServiceAvailabilityTest - STDERR: Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method<channel.close>(reply-code=406, reply-text=PRECONDITION_FAILED - inequivalent arg 'durable' for exchange 'test.mt' in vhost '/': received 'true' but current is 'false', class-id=40, method-id=10)

Both the Sndr and FakeOperator can set up the test.mt exchange. Both use the sndr.properties to set the durable property.

That said, even before this the FakeOperator throws almost the same exception for the test.mo queue:

    2025-01-18 13:43:10,542 INFO  [] [docker-java-stream-1817243090] c.e.i.ServiceAvailabilityTest - STDERR: Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method<channel.close>(reply-code=406, reply-text=PRECONDITION_FAILED - inequivalent arg 'durable' for exchange 'test.mo' in vhost '/': received 'true' but current is 'false', class-id=40, method-id=10)

The ordering may simply be an artifact of the logging setup that uses Slf4jLogConsumer to view the container output.

Pared it back to just the rabbit broker and rcvr.
Verified that the messages can be sent to rcvr and  with rabbitmqctl list_queues (from a shell inside the broker container) that they are received.

Next, turned off the Rcvr and turned on the FakeOperator:
    2025-01-19 10:17:04,408 INFO  [] [docker-java-stream-1832978947] c.e.i.ServiceAvailabilityTest - STDOUT: 2025-01-19 15:17:04.406636+00:00 [error] <0.774.0> operation basic.consume caused a channel exception not_found: no queue 'test.mo' in vhost '/'

 Looks like some configuration file errors were causing the trouble.
 Working now.

===== 2025-01-20 ======

The Maven test goal wasn't picking up my integration tests. This is because IntelliJ had auto-imported the older JUnit Test class/annotation instead of the newer Jupiter JUnit 5 version.

Now they run as expected though the Surefire output is lacking the nice formatted .html file. (The regular output are a bunch of text files, one for each test. Really not useful.)

Figured out how to make the surefire plugins (which are so freaking old and pull in so many additional dependencies) generate the readable HTML test report whenever the test goal is run.

Oh, also it appears that running the two container-based tests individually works but running them sequentially via Maven's test goal does not. The PlatformGateway instance isn't shutdown (and port released) before the next instance tries to start.

    2025-01-20 13:50:29,449 ERROR [ff0af9f3-e9de-446f-b242-e4506119c47a] [main] io.helidon.webserver.LoomServer - Failed to start listener: /0.0.0.0:2424
    java.util.concurrent.ExecutionException: java.io.UncheckedIOException: Failed to start server
    ...
    Caused by: java.net.BindException: Address already in use

Hmm, added a stop() method to PlatformGateway that calls through to the contained WebServer's stop() method.
Seems to have resolved the problem.

===== 2025-01-23 ======

On to the next test...testGatewayUnavailable

Added code to support the "restart" of the simulated platform gateway; requires creating a new instance of the web server.

Remembered that the current Sndr really doesn't support retrying, it always acks the message so they won't be delivered ever if the endpoint was missing when the message was received.

While researching the topic I came across:
    https://github.com/joshdevins/rabbitmq-ha-client (NOTE: this is a fairly old project)

 https://www.rabbitmq.com/docs/consumers#acknowledgement-modes covers the basics here with additional documents for the details.

 If a message to a given user is rejected/nacked for gateway availability reasons we want all subsequent messages to be held (not sent) until the first message has been delivered. This consideration is only for individual sessions. Other messages behave according to their sessions.

Side-note: tried integrating spotify's dockerfile-maven-plugin (https://github.com/spotify/dockerfile-maven/blob/master/docs/usage.md) into our pom.xml. Didn't work. Will need to circle around later. It's too simple to write a shell script that does it to spend the time now.


===== 2025-01-24 ======
Wrote rbc.sh to handle rebuilding container after making changes to the non-test code.
TODO
_ restructure Dockerfile-jvm to move the application code to the end; it's not caching layers effectively.
_ Clean up the design of PlatformGateway and HttpMTHandler.

Continuing work on testGatewayUnavailable...

When I added the methods to PlatformGateway to simulate throttling or availability I found that the broker would only requeue/retry the first (failed) message in very rapid succession. This is because the queue consumer declares:
    channel.basicQos(1);
The parameter is named "prefetchCount" which seems like an internally appropriate name but a bit confusing to the developer

===== 2025-01-24 ======

Setting channel.basicQos(3) will change the behavior such that the first three messages are retried/re-queued.
At least anecdotally, they are getting re-processed in the same order as they were put onto the queue.

The scenario we want to handle is, for a given user session with an ordered sequence of events, where the first message is rejected and re-queued the second message is attempted and succeeds. A subsequent delivery attempt for the first message may succeed but the damage is done.

In addition to a some logic to detect send errors due to the structure of a message (rather than a service outage or throttling situation) we need some session logic to prevent out-of-order messaging.

Added a GatewaySimStrategy that simply rejects the first message it sees to approximate the problem.
Calling the com.rabbitmq.client.Channel impl's basicQos() method with a value of 3 we see a final ordering in the current version testGatewayUnavailable:

    Messages received: [27 goodbye, 28 goodbye, 26 goodbye, 29 goodbye, 30 goodbye]

 Here it tries to send a batch of 3 messages. The first is re-queued but the second and third can be sent immediately.

RANDOM NOTE: When we start measuring perf we should make sure to only use System::nanoTime. Apparently the getMillis() is subject to adjustment such that it's possible for it to go backwards!
Also, check out the JMH Java microbenchmark harness: https://github.com/openjdk/jmh and

======== Initial Thoughts about the internals of a real Operator implementation ========
I've been considering whether it would make sense to use Records for this application. Most of the articles I've read don't really explain what domains Records might improve. They wave their hands about Data Transfer Objects which, almost always, feel like a terrible idea. Then I found the following https://blogs.oracle.com/javamagazine/post/records-come-to-java where the author notes:
"...the 'records are nominal tuples' design choice means you should expect that records will work best where you might use tuples in other languages. This includes use cases such as compound map keys or to simulate multi-return from a method. An example compound map key might look like this: record OrderPartition(CurrencyPair pair, Side side) {}"

This has some appeal. Immutable tuples pulled from APIs or similar data sources that can be composed easily and without loads of boilerplate. The compact constructor form also provides a nice place for fail-fast validation.

The Operator, unlike the Editor tools (that will follow), should not need to mutate any of the data it uses. Also Record Patterns (see https://docs.oracle.com/en/java/javase/22/language/record-patterns.html) could prove useful for the processing logic we need to write.

Along these lines and making the Java parts of the system fit well with the functional design of RabbitMQ and (possibly) Phoenix LiveView, consider reading https://www.oreilly.com/library/view/a-functional-approach/9781098109912/ which discusses ways to make Java more functional for the benefits that brings. The author has a blog at https://belief-driven-design.com/looking-at-java-21-switch-pattern-matching-14648/.


===== 2025-01-27 ======
Continuing to think about the session based ordering logic...

We need to add a notion of session groups and sequence order state to our (probably overly simplistic) message model.
Then, I'm thinking, we would reject/re-queue any message from a given session that is not the next expected message.

Start by assuming all the messages are from the same session?

Okay, implemented logic that seems to enforce ordered message delivery by a hardcoded session.
It's hideous looking--we should take a crack at revamping it, possibly with pattern matching--but appears to do what we need.
Let's create some additional variations of the problem to make sure it covers everything we can think of.

===== 2025-01-28 ======
To further test, error handling I need an actual session construct. The tests currently assume only a single session.

Where should this be constructed? In the Rcvr or the Operator?


===== 2025-01-29 ======
RANDOM NOTE: Auth0 has a pretty nice free tier with 25K users, 5 orgs, unlimited Okta & Social connections (not sure what that means), a custom domain with branded domains, and some DoS protection. No credit card. Seems cool.


Initial Domain Modelling for Operator:

Customer---hasMany--->User
|
+---hasMany---> Script

Both Customer and User have the same properties: countryCode, languageCode, brblId, platformId*
A Script is an interface with properties: id, next[] (returns a list of Script ids).
A Session is an interface with properties: id, currentScript, user (User)

An MT is an output from the execution of a Script in the context of a Session.
An MO is an input to the execution of a Script in the context of a Session.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Side-notes:
To address both the "insufficient resolution" and the back-in-time problem with System.currentTimeMillis() I found a class--NanoClock--the provides the wall-clock utility of currentTimeMillis and the better resolution of System.nanoTime() avoiding the impact host adjustments of the clock:
    https://github.com/jenetics/jenetics/blob/master/jenetics/src/main/java/io/jenetics/util/NanoClock.java

For testing purposes if/when we have ML support we could initiate a chat from the MO side and have the system respond. Validation would be weird though.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

===== 2025-01-30 ======
https://www.bandwidth.com/pricing/ shows some reasonable figures e.g. $0.004/message on a U.S. 10DLC.

Random Note of the Day: turso.tech created a fork of SQLite that includes accommodation for vector search--commonly used with RAG-based ML applications. This might be an interesting thing to use instead of spinning up a shared Postgres instance. The folks behind this also founded ScyllaDB. In an interview, Glauber Costa commented that SQLite's creators reserved implementation

===== 2025-02-01 ===
From https://openjdk.org/jeps/499: "Structured concurrency is an approach to concurrent programming that preserves the natural relationship between tasks and subtasks, which leads to more readable, maintainable, and reliable concurrent code. The term 'structured concurrency' was coined by Martin SÃºstrik and popularized by Nathaniel J. Smith. Ideas from other languages, such as Erlang's hierarchical supervisors, inform the design of error handling in structured concurrency."

Neat. Let's try it out. **Still** in preview for the upcoming Java 24 which is annoying but if the API is close to complete...


===== 2025-02-04 ===

https://www.allareacodes.com/canadian_area_codes.htm

Once I started using StructuredConcurrency/virtual thread preview features I had to add

        <configuration>
            <argLine>--enable-preview</argLine>
        </configuration>

to the maven-surefire-plugin. You can almost hear that old code creaking despite the fact that it continues to make new releases...

The built-in test runners were less pedantic. The IDE added the feature flag to the compiler automatically but not surefire.

One of the tests failed with an ExceptionInInitializer. This was enough to cause surefire (ironically) to fail to produce the surefire.html file.

com.enoughisasgoodasafeast.integration.EndToEndMessagingTest complained "Caused by: org.testcontainers.containers.ContainerLaunchException: Local Docker Compose exited abnormally with code 1 whilst running command: compose up -d"

It didn't fail a second time so we'll move on...

===== 2025-02-05 ===

https://www.rabbitmq.com/docs/shovel is interesting for operational purposes. It can move messages between queues.

https://www.rabbitmq.com/docs/publishers#concurrency drops a bomb on multiple threads writing to the same channel:
    "In general, publishing on a shared 'publishing context' (channel in AMQP 0-9-1, connection in STOMP, session in AMQP 1.0 and so on) should be avoided and considered unsafe."
    " Doing so can result in incorrect framing of data frames on the wire. That leads to connection closure."
NOTE: this applies to _publishing_ not consuming. Even so, it makes consider sticking with a single channel...

Looking at https://www.rabbitmq.com/docs/publishers#connection-recovery it appears that the Java client supports automatic recovery of connections and topology (queues, exchanges, bindings, and consumers) so maybe the heartbeat setup isn't something we need to explicitly handle. Perhaps it is enough to handle the cases where we receive a message and the queue is incommunicado.

Confirmed that if a queue comes back after being gone. The existing client/code will send the message quite happily.

Use a local temporary store to continue accepting messages? sqlite?

===== 2025-02-06 =====

Should we change Script to Page and create a new Script that represents a collection of Pages?
Or go the opposite direction and create a Book construct that represents a graph of Scripts?

Got a simple, state machine working with a test that attaches a chain of Scripts.

===== 2025-02-07 =====

Typically, we deal with directed, acyclic graphs but a Customer might want to engage in conversation that might loop around.
We can and probably should drop the acyclic part.
Our reporting metrics will want to be able to show numbers not just for how many users reached a particular node but also how they got there (the sequence of edges.)

There are libraries that provide more general graph functionality. For example, https://jgrapht.org/guide/UserOverview.
Question: do we need more functionality? I don't think so. At least not for the purposes of walking a User through a graph of Scripts.


===== 2025-02-08 =====

Thinking about the more complex logic processing needs, I think the Script record will need some additional data (e.g. the list of choices displayed in the previous script) as well as specialized logic. This could be done as a single object or the data and logic could be a separated into a structured string (as a new field to Script) and a static function that mapped to the Script's type.

If a single object the data would still need to be parsed at initialization time.

===== 2025-02-13 =====

Steve Carey kindly offered to provide a login to his sftp server (scarey.net) where I could put backups. Also, an Ubuntu container that I could use for load test or remote access, etc. What a mensch!
He needs a public key for this purpose.

~> ssh-keygen -t ed25519 -C "Key for scarey's sftp server/other services"
Generating public/private ed25519 key pair.
Enter file in which to save the key (/Users/mark/.ssh/id_ed25519):
Created directory '/Users/mark/.ssh'.
Enter passphrase (empty for no passphrase):
I left the passphrase empty because I've not been the best about remembering such things.

I emailed him the public key.

When I told him about this project and asked him about his thoughts on hosting it from home he sent a link to a thread on reddit that pointed to https://www.servethehome.com/introducing-project-tinyminimicro-home-lab-revolution/

This led me to looking at some other stuff including gitea.com which offers a free-for-open-source self-hosted version of their GitHub-like software.  GitLab, which Steve uses on his home lab, also offers a free version for "personal projects."

We're not quite at this point yet, I think, but useful for when we have a baseline ready.



===== 2025-02-16 =====

Sitting in Logan, waiting to get on a plane to Florida.

Next: replace the List<Script> field, "next" in the Script class with a List<ResponseLogic>

Discovered an interesting wrinkle of sorts about Records with fields that are collections.
I was passing List.of() as a parameter to one of the constructors which resulted in the "next" field being an immutable collection.
In my canonical constructor I had logic that initializes the field with an empty, mutable ArrayList.
For the kind of objects graphs we're building we **have to be able** to add elements to the next list to be able to link objects together in a graph of Scripts.
Even then the order of object creation is difficult.

Created a OperatorTest.


===== 2025-02-19 =====
In Orlando...
The problem with EndToEndMessagingIT were due to changes in the host name not reflected in the rcvr.properties file. I'm not sure of the best way to programmatically deal with this.
Compounding the problem, the integration test container images are baked with the .properties files and jar inside. It would be better if we could use some of the dockerfile machinery to have it read those resources from the file system...

===== 2025-02-20 =====
Flying home.
I'm giving up for now on the integration tests. Neither of them are working now.
Both of the test runner plugins make the dubious choice of not rendering a report if there are failures. Not helpful. Maybe I should fork them and make my own.

At any rate, I added a buffer to the Session that will hold the messages produced in the course of an MOMessage event. This raises some questions about how to avoid concurrently modification of a Session and all it's bits, however.

===== 2025-02-26 =====
Sick for the last five days... ugh.

So, yeah, access to the Session likely needs to synchronized to avoid problems when there are 2+ messages received for the same instance. Further complication: if we use a shared cache (e.g. Redis) the object equality of two deserialized Sessions might be an issue.

This assumes that the callbacks from the rabbit client are

We might use the solution to this to help deal with the lack of thread-safety in the Rabbit channel used to add messages.

https://www.rabbitmq.com/client-libraries/java-api-guide#concurrency

They recommend pooling channels. There's a Spring-based connection pool impl, but I'd rather not bring in all that stuff.

There's a project that uses some of the Apache Commons' pooling framework: https://github.com/sytuacmdyh/amqp-client-pool

===== 2025-03-09 =====

The RabbitQueueConsumer needs to be refactored to remove the MTHandler. This is too hardwired to the FakeOperator and test setups we had before. MTConsumer, likewise, needs to be changed/removed.

===== 2025-03-10 =====
Completed the refactoring for RabbitQueueConsumer and its use by Operator.

Sndr is next...

===== 2025-03-11 =====

As I started work on the Sndr, I realized there wasn't a compelling reason to make MOMessage and MTMessage different classes. These have been merged.

Continue to explore how we want the script logic to work. Pivot and TopicSelection represent common cases we know from our previous experience.

Do we need the 'previous' field in the Script class? We could instead track previously executed Scripts (and navigate back to them) via a list on the Session. Removing previous would make constructing graphs of Scripts easier and less brittle. Detours and deviations from the initial chain of Scripts (because the User wants to talk about something else or asks to go to the top of the conversational tree) could then also be handled dynamically. In any case, we'll want to keep a history of a User's actual path through a conversation (the nodes they visited vs the Script graph that was constructed.)

< User input (short code)
> Present ("welcome"...ask question)
< User input (answer)
> Process (match answer)
> Present (acknowledge and ask another question)
< User input ("change topic")
> Process (no match, detect change topic)
     [optional]
        > Present (ask navigational question) [optional]
        < User input ("start new topic")
        > Process (match answer)
> Present (display new topics and ask for selection)
...


===== 2025-03-16 =====
https://www.rabbitmq.com/client-libraries/java-api-guide#concurrency:
    "When manual acknowledgements are used, it is important to consider what thread does the acknowledgement. If it's different from the thread that received the delivery (e.g. Consumer#handleDelivery delegated delivery handling to a different thread), acknowledging with the multiple parameter set to true is unsafe and will result in double-acknowledgements, and therefore a channel-level protocol exception that closes the channel. Acknowledging a single message at a time can be safe."

===== 2025-03-17 =====
Tachometer is an interesting plugin for Docker. It shows cpu and memory usage of containers in real-time.
Also, https://docs.fluentd.org/container-deployment/docker-logging-driver looks like it might be useful.

Actually..."NOTE: Currently, the Fluentd logging driver doesn't support sub-second precision." Oof! That might be a problem...

===== 2025-03-21 =====
TODO
_ Update OperatorMessageFlowIT to use testcontainers instead of assuming a running instance of RabbitMQ.
X Figure out how to structure the replacement for the generation of the top level topic script chain.
    --> just static functions in a class namespace?
X Update the docker files to point at jar's on the host filesystem.
_ Add build section to our docker-compose.


docker exec -it e016a29bccd69dde8e6a28d3c1dd35d84db8ae2ef7e1e0eeb79d6a114169b19c /bin/bash
wget --post-data=<message text> http://<host>:<port>/<pathInfo>


===== 2025-03-25 =====

With no additional networking config, I can use wget to reach the host machine from inside a bash shell running in the sndr container with either host.docker.internal or 192.168.1.155.
I cannot use localhost, however. Nor the 0.0.0.0 pseudo address used for listeners to attach to all interfaces.

Added a "reachability" test send in Sndr's main. This removes all the noise from the stack trace. Here we find that we're able to reach host.docker.internal but are receiving a 404 for some reason.

This led me to testing via the HttpMTHandler directly. Doing so confirmed where the problem was actually happening.
Looks like we were effectively specifying the pathInfo **twice** when making the POST...

Yes, confirmed. Annoyingly the HttpMTHandler code is littered with comments about its shitty state but it didn't occur to me that it was the problem. Argh....

The running thread is listed in the "Processed message:" log written by sndr-1's OperatorConsumer. We were wondering how many threads the Rabbit driver would use to deliver messages. Here's the munged log listing of counts and distinct thread ids:

   5 [pool-1-thread-10]
   1 [pool-1-thread-3]
   1 [pool-1-thread-4]
   2 [pool-1-thread-5]
   2 [pool-1-thread-6]
   1 [pool-1-thread-7]
   2 [pool-1-thread-8]
   2 [pool-1-thread-9]

NOTE: 1 of the occurrences is for the "handleConsumeOk called with consumerTag" log. The actual total here is 15 because we sent the batch of 5 three times. This is only anecdotal obviously but also informative. Note: the Macbook we're using has 8 cores.
 Also anecdotal, the log indicate the messages were processed in order.


===== 2025-03-26 =====

Noticed that the list_queues command for rabbitmqctl shows four entries. Two are expected (test.mo and test.mt) but for the other pair are, according to my google research, the names used are random strings prefixed by "amq.gen-".

The total system makes four Connections (1 each for Sndr and Rcvr and 2 for Operator.) These are all managed by the two classes, RabbitQueueProducer and RabbitQueueConsumer which always (I think) pass in names for the queue. So where are these extras coming from?

Also, the message counts list_queues displays suggest that the messages sent to each of intended queues--test.mo and test.mt--are also sent to the anonymous queues, as if they were paired with the former.

Hmm, in the logs I'm noticing this:
brkr-1      | 2025-03-25 14:51:36.300810+00:00 [info] <0.652.0> connection 172.18.0.1:63666 -> 172.18.0.2:5672: user 'guest' authenticated and granted access to vhost '/'
operator-1  | 2025-03-25 10:51:36,319 INFO  [] [main] c.e.RabbitQueueConsumer - AMQP.Queue.DeclareOk: queue=amq.gen-7cnH4S2ocnwNzTSHZb1wvw consumerCount=0 messageCount=0
operator-1  | 2025-03-25 10:51:36,321 INFO  [] [main] c.e.RabbitQueueConsumer - AMQP.Queue.BindOk: protocolClassId=50 protocolMethodId=21 protocolMethodName=queue.bind-ok
operator-1  | 2025-03-25 10:51:36,324 WARN  [] [pool-1-thread-3] c.e.OperatorConsumer - handleConsumeOk called with consumerTag amq.ctag-RU77QLZAZiDGLk1UYHB3dQ

Resolved the problem. We weren't using the queueName to create the queue. Doh.


===== 2025-03-28 =====

https://medium.com/@skillcate/sentiment-analysis-using-nltk-vader-98f67f2e6130 talks about using VADER.
https://www.youtube.com/watch?v=szczpgOEdXs includes link to code using BERT and PyTorch for sentiment analysis. Includes params for using Nvidia CUDA support. The model used includes support for Spanish though I suspect it's European Spanish. English support, too, of course.

Note, the downloads of the pretrained models require 600-700mb. This is a lot less than the Llama models (measured in hundreds of gigabytes.)


To handle multiple responses to the same question (think Holy Grail's "blue..no green!")
in the response evaluation logic handle add an error check that considers whether the user input matches one of the responses in the previously presented question. If it does it may be that they're trying to change their answer. In those cases, ask for confirmation. If confirmed, rewind to that question prefixed by a "Did you want to change your answer?". When we get to the stage of turning message history into statistics we'll need to be able to address these response changes.

TODO
_ Update OperatorMessageFlowIT to use testcontainers instead of assuming a running instance of RabbitMQ.
X Add build section to our docker-compose? I *think* this is something for later actually...

My updated test isn't receiving the input message in the named queue. The caller doesn't complain at all but the Rabbit broker log shows:


2025-03-30 11:32:52 2025-03-30 15:32:52.232928+00:00 [error] <0.1665.0> Channel error on connection <0.1656.0> (172.17.0.1:55644 -> 172.17.0.2:5672, vhost: '/', user: 'guest'), channel 1:
2025-03-30 11:32:52 2025-03-30 15:32:52.232928+00:00 [error] <0.1665.0> operation basic.publish caused a channel exception not_found: no exchange 'test.mo' in vhost '/'

Ah, okay. So we broke this when we addressed the machine generated queue name issue. See RabbitQueueFunctions.exchangeForQueueName().
Reviewing their documentation, I realized that the argument passed to channel.basicPublish() is the **exchangeName**, not the queueName. This didn't matter before because these names were the same.

Side note: while working on this, the power to the house was shut off. During that period, running one of the tests, threw "java.net.SocketException: Network is unreachable." My interpretation is that using IP addresses required a functioning router even though they were all effectively local.

Still some troubles here...running each of the test in OperatorMessageFlowIT individually via IDEA works fine but running the test class(also via IDEA) does not. Three of the four tests fail waiting for output to arrive in the InMemoryQueueProducer.

Executing mvn verify (the goal that runs integration tests) fails for three of the tests as well but this time because of a java.lang.UnsupportedClassVersionError.
Adding the same configuration.argLine we use for the surefire plugin to the failsafe config in pom.xml solves this:

    <configuration>
        <argLine>--enable-preview</argLine>
    </configuration>

Now we see the same issue when using IDEA to run all the tests in the class:
    org.awaitility.core.ConditionTimeoutException: Condition with Lambda expression in com.enoughisasgoodasafeast.integration.OperatorMessageFlowIT was not fulfilled within 5 seconds.

Running the OperatorMessageFlowIT class with just a single test (commenting out the @Test annotation for the others) works via IDEA and mvn verify. Something to do with state?

===== 2025-04-01 =====

Debugging...

with two connection setups in the same test method, the first succeeds and the second fails.

Paused the latter after the message was sent to the queue, the output from rabbitmqctl list_queues shows zero entries in test.mo.
So is the problem in the producer rather than the consumer?

It appears that the problem stemmed from the first Rabbit producer and consumer clients still being connected to the broker.
So, at least for the purposes of unit/integration tests, I've added shutdown() methods to the QueueProducer and QueueConsumer interfaces and the Operator class. These call close() on the Channel and Connection instances. Unlike some supported Rabbit types (topics, streams, etc) a queue doesn't deliver messages to multiple consumers. By contrast, having multiple producers is fine.

===== 2025-04-03 =====
Revisited EndToEndMessagingIT and managed to get it working. One caveat, running it repeatedly often fails with this:
    "org.testcontainers.containers.ContainerLaunchException: Local Docker Compose exited abnormally with code 1 whilst running command: compose up -d"
The failure appears to happen after it's created the network and all four containers while it's starting the RabbitMQ container (brkr).
The start operation appears to have succeeded, however. I can shell into it and run rabbitmqctl commands. The failure triggers the teardown of the other containers but the brkr container continues to run. So really not ideal from a test automation/continuous integration perspective.

The Testcontainers team seems to reluctantly support the use of docker compose. So maybe this is the best I can expect. :-{

===== 2025-04-07 =====

While debugging ServiceAvailabilityIT I was trying to examine the burble-jvm image to see why it could find the jar file.

Finally figured out how to run the image without having it immediately exit. Here's how:

    docker run -it --rm --name rcvr --entrypoint "sh" burble-jvm:0.1.0

The jar file *does* exist per the COPY command in Dockerfile.jvm.

    /opt/app # ls -la /opt/app/sndrRcvr-1.0-SNAPSHOT-jar-with-dependencies.jar
    -rw-r--r-- 1 root root 4668198 Apr  5 20:59 /opt/app/sndrRcvr-1.0-SNAPSHOT-jar-with-dependencies.jar

Is it a permission problem? Doesn't seem like it. I can run the entrypoint and cmd from the shell without issue. Provided there's a RabbitMQ container running, all the producers and consumers are able to connect.

Running it manually, the container works as expected, connecting to the broker without complaint.

Ah. The problem was that our dev version of the Dockerfile (Dockerfile.dev.jvm) doesn't copy the jar file into the image. It expects the docker run command will include a volume mapping of /Users/mark/Development/sndrRcvr on the host to /opt/app on the container.


===== 2025-04-10 =====

Time to create some tools for interacting with a running system.

Picocli and jline3 look to be useful and lightweight building blocks for an interactive CLI.
Use an extended version of PlatformGateway.



1) create persona
    > input telephone number
    > input shortcode
    < display newly created persona
2) list personas
    < display all available personas with id number (write these to a serialized file that can be loaded at start up)
3) send message (initially just a send but being able to queue messages and send as a batch will be useful.)
    > display "Which persona will send the message?" then list available personas by id
    > input selected id
    < display selected persona then prompt "Enter the text of the message:"
    > input message text
    < display confirmation dialog "Send message? y/n"
    > input choice (if yes, send message)
    > wait for response (5 seconds) and display it. Otherwise "No response received."
4) exit

Take a param (-l) to signal desire to load any serialized files found in a special directory.



===== 2025-04-16 =====

brblcli is working w/r/t to sending messages and triggering script chains. The hardcoded script for short code "45678" will correctly
track the state for the user.

Lots of work yet to make the CLI easy to use but a good start.

Made some additional changes to the Session:
    - Changed the inputHistory implementation in Session to be fixed size.
    - Added requirement that constructor params be non null and started a unit test for the class.


===== 2025-04-17 =====

Starting on the persistence layer...
Considered SQLite but decided to go with Postgres.

Simplest task is to record receipt of MOs and transmission of MTs.
Ideally we could use the basic records but we'd like to relate them to the Script state.

User  <---:fk:--- Profile

UUID id                           --> String or UUID
Map<Platform, String> platformIds --> punt on this for now?
String countryCode                --> CREATE TYPE country_code AS ENUM ('us', 'ca', 'mx');
List<String> languages)           --> CREATE TYPE language_code AS ENUM ('en', 'es', 'fr', ...);

Installing Postgres...

Used the postgres.app version which is quite permissive with its default pg_hba.conf settings but seems pretty simple.

Learned about server-side prepared statements:
    https://jdbc.postgresql.org/documentation/server-prepare/#server-prepared-statements
Probably a good idea, if we use them, to also set on the connection:
    autosave=conservative
This to avoid having to bounce the server if/when the query plan is changed. There is, however, a warning about the performance impact on long transactions with (auto)save points.

Let's comment it out and wait till we have the code worked out before trying it out.

Created a properly named database:
    brbl_dev

===== 2025-04-17 =====

psql -U mark -d brbl_dev -a -f <ddl-or-dml>.sql

See dev.ddl in src/main/resources/sql.

Note "timestamp with time zone" is, perhaps surprisingly, the right data type for a Java Instant value:
    See https://wiki.postgresql.org/wiki/Don't_Do_This#Don.27t_use_timestamp_.28without_time_zone.29_to_store_UTC_times

Using psql to show all tables
 \dt *.*

Using psql to show all users
 \du

java.time.Instance supports nanos --> java.sql.Timestamp --> Postgres

Gemini suggests using the timestamp9 extension instead of separate fields for the
CREATE EXTENSION timestamp9;

mark@yakko sndrRcvr % brew info postgresql@17
==> postgresql@17: stable 17.2 (bottled) [keg-only]
Object-relational database system
https://www.postgresql.org/
Not installed
From: https://github.com/Homebrew/homebrew-core/blob/HEAD/Formula/p/postgresql@17.rb
License: PostgreSQL
==> Dependencies
Build: docbook âœ˜, docbook-xsl âœ˜, gettext âœ˜, pkgconf âœ˜
Required: icu4c@76 âœ˜, krb5 âœ˜, lz4 âœ”, openssl@3 âœ”, readline âœ˜, zstd âœ”, gettext âœ˜
==> Caveats
This formula has created a default database cluster with:
  initdb --locale=C -E UTF-8 /opt/homebrew/var/postgresql@17

When uninstalling, some dead symlinks are left behind so you may want to run:
  brew cleanup --prune-prefix

postgresql@17 is keg-only, which means it was not symlinked into /opt/homebrew,
because this is an alternate version of another formula.

To start postgresql@17 now and restart at login:
  brew services start postgresql@17
Or, if you don't want/need a background service you can just run:
  LC_ALL="C" /opt/homebrew/opt/postgresql@17/bin/postgres -D /opt/homebrew/var/postgresql@17
==> Analytics
install: 6,954 (30 days), 21,611 (90 days), 36,400 (365 days)
install-on-request: 6,307 (30 days), 19,641 (90 days), 32,337 (365 days)
build-error: 23 (30 days)

After installing the brew package and starting the server I can connect via:
    psql -d postgres

This connects as user 'mark' to the only non-template database available, 'postgres'. Not confusing at all.
I'm used to 'postgres' being the db super user but here it's 'mark'

Creating a table with a column of type 'timestamp with time zone' looks like this:

    postgres=# insert into test_time(received_at_ms) values(NOW());
    INSERT 0 1
    postgres=# select * from test_time ;
            received_at_ms
    -------------------------------
     2025-04-22 09:28:49.010806-04
    (1 row)

This only gets us to the milliseconds level.

Google's Gemini suggests using time9, a Postgres extension, as a way of supporting nanosecond level timestamps. This instead of splitting the milliseconds and nanosecond components into two columns.

The extension isn't part of the regular distro and requires compiling the code here (https://github.com/optiver/timestamp9)

Needed to install cmake which is very dumb about checking it's version requiring me to hack the timestamp9's CMakeLists.txt file.

...and also the c.h file to comment out references to ENABLE_NLS. I tried #undef'ing it at the top of the file as well as setting

    #define ENABLE_NLS 0

in pg_config.h but neither was sufficient. At first blush, it doesn't seem to be something that will impact Brbl's functionality.

mark@yakko build % sudo make install

Password:
[  0%] Built target controlfile
[ 66%] Built target timestamp9
[100%] Built target sqlfile
[100%] Built target sqlupdatescripts
Install the project...
-- Install configuration: "Release"
-- Installing: /opt/homebrew/lib/postgresql@17/timestamp9.so
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.4.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--0.1.0--0.2.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--0.2.0--0.3.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--0.3.0--1.0.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.0.0--1.0.1.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.0.1--1.1.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.1.0--1.2.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.2.0--1.3.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9--1.3.0--1.4.0.sql
-- Installing: /opt/homebrew/share/postgresql@17/extension/timestamp9.control

Restarted the database and connected via psql as superuser (mark) then ran

    CREATE EXTENSION timestamp9;

which spits out:

    ERROR:  could not access file "$libdir/timestamp9": No such file or directory

Ah, the produced library file is named with linux convention with the .so extension.

mark@yakko timestamp9 % file /opt/homebrew/lib/postgresql@17/timestamp9.so
/opt/homebrew/lib/postgresql@17/timestamp9.so: Mach-O 64-bit bundle arm64

So just rename the file? Apparently so...

    postgres=# CREATE EXTENSION timestamp9;
    CREATE EXTENSION

When declaring a column of this type use timestamp9 leaving off the "WITH TIME ZONE" used with the regular TIMESTAMP type.

Hmm... populating a timestamp9 column via JDBC with Timestamp.from(message.receivedAt()) compared to one with using "timestamp with time zone" -->

    timestamp with time zone      |   timestamp9
    ------------------------------|------------------------------------
    2025-04-23 22:03:03.177013-04 | 2025-04-23 22:03:03.177013000 -0400

    It seems like it's just padding the same number by 1000.

Printing the numbers as jdk is running the Message creation code:
    message.receivedAt() ==> 2025-04-24T02:03:03.177013083Z
    message.receivedAt().getEpochSecond()    ==> 177013083
    message.receivedAt().getNano()           ==> 1745460183



See PersistenceManager.instantInvestigation

nanoclock produced instant...
Instant:        2025-04-24T22:08:02.454020250Z --> year-month-day-hours-minutes-seconds-nanoseconds
    --> The last part of this is 9 digits
toEpochMilli:   1745532482454
    --> plug this into https://www.epochconverter.com/ --> number of milliseconds since epoch. Nanos are divided by 1 million so only the leading three digits (of a 9 digit number) are added as milliseconds. As the method name suggests its resolution is only ms.
getEpochSecond: 1745532482    --> Just the number of seconds since the epoch.
getNano:        454020250   --> the nanos that are added
instant converted to timestamp...
as timestamp time: 1745532482454
as timestamp nanos: 454020250
timestamp converted back to instant
Instant:        2025-04-24T22:08:02.454020250Z --> year-month-day-hours-minutes-seconds-nanoseconds
toEpochMilli:   1745532482454
getEpochSecond: 1745532482
getNano:        454020250

Original instant equal to round tripped instant? true

The complete timestamp expressed in nanoseconds would be:
    1745532482454020150 (19 digits)
    9223372036854775807 is the max positive value of a Java long.
Checking and comparing the values while converting from Instant to Timestamp...looks good. Nothing getting dropped.

It seems like the JDBC driver is truncating all but the leading three places of nanoseconds from the java.sql.Timestamp value when it writes the value to the column. This makes sense if the db's timestamp type limits resolution to microseconds.
Also, weirdly, the timezone offset goes from two digits to four. For example, Eastern Daylight Savings Time it becomes -0400 instead of -04 when psql displays the column value.

We wanted to get better than the thousandth of a second resolution of milliseconds (that means 0-999) but regular Instant/Timestamp stored in a Timestamp column in Postgres gets us another three places of resolution that seems to be equivalent to microseconds. Perhaps this is enough? It would eliminate the need to overcome the JDBC and display issues using the timestamp9 data type.

Note: I am thinking/writing this on the train back from NYC and I'm super tired so totally possible I'm getting it all wrong.


===== 2025-04-27 =====

https://www.postgresql.org/docs/current/datatype-datetime.html confirms that the timestamp data type supports microseconds. So our observations were correct.

This is enough for our purposes and removes the need for non-standard type extensions. Moving on...

Thinking about where, in the pipeline, we write the incoming MO...
We want to be able to associate the Message with the Script element that was used to process it.
Likewise we want to associate the outgoing MT with the Script that generated it.
So how about ...

For each incoming Message:
    - Rcvr writes the full Message with its UUID, the timestamp, and the rest. Before or after enqueuing?
            CREATE TABLE brbl_logs.messages_mo (
                id          UUID NOT NULL,
                rcvd_at     TIMESTAMP WITH TIME ZONE NOT NULL,
                _from       VARCHAR(15) NOT NULL,
                _to         VARCHAR(15) NOT NULL,   --> how long are non-SMS (WhatsApp, FB Messenger, etc) identifiers?
                _text       VARCHAR(2000) NOT NULL  --> should match the MT length
            );
    - Operator writes an abbreviated row with just the UUID of the Message, the Session UUID, and the UUID of the handling script.
            CREATE TABLE brbl_logs.messages_mo_prcd (
                id          UUID NOT NULL,
                prcd_at     TIMESTAMP WITH TIME ZONE NOT NULL,
                session_id  UUID NOT NULL,
                script_id   UUID NOT NULL
            );

            To join the two, showing including messages that were received but not processed:
            SELECT
                rcvd.id, prcd.session_id, prcd.script_id, rcvd._text
            FROM
                brbl_logs.messages_mo AS rcvd
            LEFT JOIN
                brbl_logs.messages_mo_prcd AS prcd
            ON rcvd.id = prcd.id;

For each outgoing Message:
    - Operator writes the full Message immediately after enqueuing it.

    CREATE TABLE brbl_logs.messages_mt (
        id          UUID NOT NULL,
        sent_at     TIMESTAMP WITH TIME ZONE NOT NULL,
        _from       VARCHAR(15) NOT NULL,
        _to         VARCHAR(15) NOT NULL,   --> how long are non-SMS (WhatsApp, FB Messenger, etc) identifiers?
        _text       VARCHAR(2000) NOT NULL,  --> should match the MO length
        session_id  UUID NOT NULL,
        script_id   UUID NOT NULL
    );

For each delivered Message acked by the 3rd party gateway:
    - Sndr writes an abbreviated row with just the UUID and the delivery timestamp.

    CREATE TABLE brbl_logs.messages_mt_dlvr (
        id          UUID NOT NULL,
        dlvr_at     TIMESTAMP WITH TIME ZONE NOT NULL
    );

===== 2025-04-29 ====
Finished the insert methods for all four record types.

In tests, inserting two records (brbl_logs.messages_mo and brbl_logs.messages_mo_prcd) appears to take a whopping 112 milliseconds.

delete from brbl_logs.messages_mo;
delete from brbl_logs.messages_mo_prcd;
delete from brbl_logs.messages_mt;
delete from brbl_logs.messages_mt_dlvr ;

select prcd_at - rcvd_at from brbl_logs.messages_mo mos inner join brbl_logs.messages_mo_prcd prcd on mos.id = prcd.id;
select dlvr_at - sent_at from brbl_logs.messages_mt mts inner join brbl_logs.messages_mt_dlvr dlvr on mts.id = dlvr.id;

No doubt creating a brand new Connection object for each isn't helping to make this quick.

===== 2025-04-29 ====

Plugged in a c3p0 connection pool. This helps reduce the insert times especially if I initialize it before running the timed tests.
The init call, by itself, took 117 ms to setup 5 connections.
But subsequent calls to our fetchConnection method return in microseconds.

The batch mode insert functions seem to be a good bit slower (5ms) than the single inserts.
Is this because they are writing more data?
Let's try removing the batch setup...helps a little.
Looking at the times for the insert of the two records we'll do for each and every MO:
    insertMO: b 2025-04-30T14:22:50.188750583Z a 2025-04-30T14:22:50.192991833Z: d PT0.00424125S
        ~> 4.2 ms
    insertProcessedMO: b 2025-04-30T14:22:50.194844917Z a 2025-04-30T14:22:50.195541667Z: d PT0.00069675S
        ~> 0.6 ms
And for MTs...
    insertMT: b 2025-04-30T14:22:50.195807458Z a 2025-04-30T14:22:50.196264708Z: d PT0.00045725S
        ~> 0.4 ms
    insertDeliveredMT: b 2025-04-30T14:22:50.196432250Z a 2025-04-30T14:22:50.197066292Z: d PT0.000634042S
        ~> 0.6 ms
Okay, so not terrible anymore. To be fair, my testing is really not all that meaningful since it is unrealistically optimized (no other overhead from related application code, database running on localhost) and, at the same time, not ideal (no JVM warmup.) Its quite terrible even as seat-of-the-pants micro benchmarks go.


